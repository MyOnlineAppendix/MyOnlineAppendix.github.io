<h3 style="margin:0px">Class: org.apache.hadoop.hdfs.TestInjectionForSimulatedStorage (1 methods) </h3><br><pre style="padding:0"><code class="html"><div id="tags"><button id="5"style="border-width:0;padding:0;background:#282B2E"  onclick="filter('5')" data-toggle="tooltip" title="Executes methods or other tests from the same test unit"><kbd id="tag-5"class="label-info"style="display: inline-block;font-size:7pt" >ExecutionTester&nbsp;(1)</kbd></button>&nbsp;</div></code></pre><center><button onclick="showBlocks()" style="margin:6px">Update filter <span class="glyphicon glyphicon-refresh" aria-hidden="true"></span> </button></center><br>
<pre class="type-5 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testInjection() throws IOException {
  MiniDFSCluster cluster=null;
  String testFile="/replication-test-file";
  Path testPath=new Path(testFile);
  byte buffer[]=new byte[1024];
  for (int i=0; i < buffer.length; i++) {
    buffer[i]='1';
  }
  try {
    Configuration conf=new HdfsConfiguration();
    conf.set(DFSConfigKeys.DFS_REPLICATION_KEY,Integer.toString(numDataNodes));
    conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,checksumSize);
    SimulatedFSDataset.setFactory(conf);
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();
    cluster.waitActive();
    String bpid=cluster.getNamesystem().getBlockPoolId();
    DFSClient dfsClient=new DFSClient(new InetSocketAddress("localhost",cluster.getNameNodePort()),conf);
    DFSTestUtil.createFile(cluster.getFileSystem(),testPath,filesize,filesize,blockSize,(short)numDataNodes,0L);
    waitForBlockReplication(testFile,dfsClient.getNamenode(),numDataNodes,20);
    List<Map<DatanodeStorage,BlockListAsLongs>> blocksList=cluster.getAllBlockReports(bpid);
    cluster.shutdown();
    cluster=null;
    LOG.info("Restarting minicluster");
    conf=new HdfsConfiguration();
    SimulatedFSDataset.setFactory(conf);
    conf.set(DFSConfigKeys.DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_KEY,"0.0f");
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes * 2).format(false).build();
    cluster.waitActive();
    Set<Block> uniqueBlocks=new HashSet<Block>();
    for (    Map<DatanodeStorage,BlockListAsLongs> map : blocksList) {
      for (      BlockListAsLongs blockList : map.values()) {
        for (        Block b : blockList) {
          uniqueBlocks.add(new Block(b));
        }
      }
    }
    LOG.info("Inserting " + uniqueBlocks.size() + " blocks");
    cluster.injectBlocks(0,uniqueBlocks,null);
    dfsClient=new DFSClient(new InetSocketAddress("localhost",cluster.getNameNodePort()),conf);
    waitForBlockReplication(testFile,dfsClient.getNamenode(),numDataNodes,-1);
  }
  finally {
    if (cluster != null) {
      cluster.shutdown();
    }
  }
}

</code></pre>

<script>$(document).ready(function() {
  $('pre code').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});</script>
