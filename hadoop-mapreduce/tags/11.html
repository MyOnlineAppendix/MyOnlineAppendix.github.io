<h3><span class=" glyphicon glyphicon-tag"/>&nbspIterativeVerifier</h3><kbd>Verifies assertions in iterations</kbd><br><br><br><h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.namenode.TestBlockPlacementPolicyRaid </h4><pre class="type-11 type-4 type-7 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies assertions in iterations
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
"></span><br>
/** 
 * Test that the har parity files will be placed at the good locations when we
 * create them.
 */
@Test public void testChooseTargetForHarRaidFile() throws IOException {
  setupCluster();
  try {
    String[] racks={"/rack2","/rack2","/rack2","/rack2","/rack2","/rack2"};
    String[] hosts={"host2.rack2.com","host3.rack2.com","host4.rack2.com","host5.rack2.com","host6.rack2.com","host7.rack2.com"};
    cluster.startDataNodes(conf,6,true,null,racks,hosts,null);
    String harParity=raidrsHarTempPrefix + "/dir/file";
    int numBlocks=11;
    DFSTestUtil.createFile(fs,new Path(harParity),numBlocks,(short)1,0L);
    DFSTestUtil.waitReplication(fs,new Path(harParity),(short)1);
    FileStatus stat=fs.getFileStatus(new Path(harParity));
    BlockLocation[] loc=fs.getFileBlockLocations(stat,0,stat.getLen());
    int rsParityLength=RaidNode.rsParityLength(conf);
    for (int i=0; i < numBlocks - rsParityLength; i++) {
      Set<String> locations=new HashSet<String>();
      for (int j=0; j < rsParityLength; j++) {
        for (int k=0; k < loc[i + j].getNames().length; k++) {
          String name=loc[i + j].getNames()[k];
          LOG.info("Har Raid block location: " + name);
          Assert.assertTrue(locations.add(name));
        }
      }
    }
  }
  finally {
    if (cluster != null) {
      cluster.shutdown();
    }
  }
}

</code></pre>

<br>
<pre class="type-10 type-11 type-4 type-7 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
"></span><br>
/** 
 * Test that the parity files will be placed at the good locations when we
 * create them.
 */
@Test public void testChooseTargetForRaidFile() throws IOException {
  setupCluster();
  try {
    String src="/dir/file";
    String parity=raidrsTempPrefix + src;
    DFSTestUtil.createFile(fs,new Path(src),4,(short)1,0L);
    DFSTestUtil.waitReplication(fs,new Path(src),(short)1);
    refreshPolicy();
    setBlockPlacementPolicy(namesystem,policy);
    String[] racks={"/rack2","/rack2","/rack2","/rack2","/rack2","/rack2"};
    String[] hosts={"host2.rack2.com","host3.rack2.com","host4.rack2.com","host5.rack2.com","host6.rack2.com","host7.rack2.com"};
    cluster.startDataNodes(conf,6,true,null,racks,hosts,null);
    int numBlocks=6;
    DFSTestUtil.createFile(fs,new Path(parity),numBlocks,(short)2,0L);
    DFSTestUtil.waitReplication(fs,new Path(parity),(short)2);
    FileStatus srcStat=fs.getFileStatus(new Path(src));
    BlockLocation[] srcLoc=fs.getFileBlockLocations(srcStat,0,srcStat.getLen());
    FileStatus parityStat=fs.getFileStatus(new Path(parity));
    BlockLocation[] parityLoc=fs.getFileBlockLocations(parityStat,0,parityStat.getLen());
    int parityLen=RaidNode.rsParityLength(conf);
    for (int i=0; i < numBlocks / parityLen; i++) {
      Set<String> locations=new HashSet<String>();
      for (int j=0; j < srcLoc.length; j++) {
        String[] names=srcLoc[j].getNames();
        for (int k=0; k < names.length; k++) {
          LOG.info("Source block location: " + names[k]);
          locations.add(names[k]);
        }
      }
      for (int j=0; j < parityLen; j++) {
        String[] names=parityLoc[j + i * parityLen].getNames();
        for (int k=0; k < names.length; k++) {
          LOG.info("Parity block location: " + names[k]);
          Assert.assertTrue(locations.add(names[k]));
        }
      }
    }
  }
  finally {
    if (cluster != null) {
      cluster.shutdown();
    }
  }
}

</code></pre>

<br>
<pre class="type-10 type-11 type-4 type-7 type-5 type-12 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values related to public fields.">PublicFieldVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects/variable are equal to an expected value 
- Verifies values related to public fields.
- Contains more than 2 JUnit-based stereotypes
"></span><br>
/** 
 * Test BlockPlacementPolicyRaid actually deletes the correct replica.
 * Start 2 datanodes and create 1 source file and its parity file.
 * 1) Start host1, create the parity file with replication 1
 * 2) Start host2, create the source file with replication 2
 * 3) Set repliation of source file to 1
 * Verify that the policy should delete the block with more companion blocks.
 */
@Test public void testDeleteReplica() throws IOException {
  setupCluster();
  try {
    setBlockPlacementPolicy(namesystem,new BlockPlacementPolicyDefault(conf,namesystem,namesystem.clusterMap));
    DatanodeDescriptor datanode1=namesystem.datanodeMap.values().iterator().next();
    String source="/dir/file";
    String parity=xorPrefix + source;
    final Path parityPath=new Path(parity);
    DFSTestUtil.createFile(fs,parityPath,3,(short)1,0L);
    DFSTestUtil.waitReplication(fs,parityPath,(short)1);
    cluster.startDataNodes(conf,1,true,null,rack2,host2,null);
    DatanodeDescriptor datanode2=null;
    for (    DatanodeDescriptor d : namesystem.datanodeMap.values()) {
      if (!d.getName().equals(datanode1.getName())) {
        datanode2=d;
      }
    }
    Assert.assertTrue(datanode2 != null);
    cluster.waitActive();
    final Path sourcePath=new Path(source);
    DFSTestUtil.createFile(fs,sourcePath,5,(short)2,0L);
    DFSTestUtil.waitReplication(fs,sourcePath,(short)2);
    refreshPolicy();
    Assert.assertEquals(parity,policy.getParityFile(source));
    Assert.assertEquals(source,policy.getSourceFile(parity,xorPrefix));
    List<LocatedBlock> sourceBlocks=getBlocks(namesystem,source);
    List<LocatedBlock> parityBlocks=getBlocks(namesystem,parity);
    Assert.assertEquals(5,sourceBlocks.size());
    Assert.assertEquals(3,parityBlocks.size());
    Collection<LocatedBlock> companionBlocks;
    companionBlocks=getCompanionBlocks(namesystem,policy,sourceBlocks.get(0).getBlock());
    verifyCompanionBlocks(companionBlocks,sourceBlocks,parityBlocks,new int[]{0,1},new int[]{0});
    companionBlocks=getCompanionBlocks(namesystem,policy,sourceBlocks.get(1).getBlock());
    verifyCompanionBlocks(companionBlocks,sourceBlocks,parityBlocks,new int[]{0,1},new int[]{0});
    companionBlocks=getCompanionBlocks(namesystem,policy,sourceBlocks.get(2).getBlock());
    verifyCompanionBlocks(companionBlocks,sourceBlocks,parityBlocks,new int[]{2,3},new int[]{1});
    companionBlocks=getCompanionBlocks(namesystem,policy,sourceBlocks.get(3).getBlock());
    verifyCompanionBlocks(companionBlocks,sourceBlocks,parityBlocks,new int[]{2,3},new int[]{1});
    companionBlocks=getCompanionBlocks(namesystem,policy,sourceBlocks.get(4).getBlock());
    verifyCompanionBlocks(companionBlocks,sourceBlocks,parityBlocks,new int[]{4},new int[]{2});
    companionBlocks=getCompanionBlocks(namesystem,policy,parityBlocks.get(0).getBlock());
    verifyCompanionBlocks(companionBlocks,sourceBlocks,parityBlocks,new int[]{0,1},new int[]{0});
    companionBlocks=getCompanionBlocks(namesystem,policy,parityBlocks.get(1).getBlock());
    verifyCompanionBlocks(companionBlocks,sourceBlocks,parityBlocks,new int[]{2,3},new int[]{1});
    companionBlocks=getCompanionBlocks(namesystem,policy,parityBlocks.get(2).getBlock());
    verifyCompanionBlocks(companionBlocks,sourceBlocks,parityBlocks,new int[]{4},new int[]{2});
    refreshPolicy();
    setBlockPlacementPolicy(namesystem,policy);
    fs.setReplication(sourcePath,(short)1);
    DFSTestUtil.waitReplication(fs,sourcePath,(short)1);
    Map<String,Integer> counters=new HashMap<String,Integer>();
    refreshPolicy();
    for (int i=0; i < parityBlocks.size(); i++) {
      companionBlocks=getCompanionBlocks(namesystem,policy,parityBlocks.get(i).getBlock());
      counters=BlockPlacementPolicyRaid.countCompanionBlocks(companionBlocks,false);
      Assert.assertTrue(counters.get(datanode1.getName()) >= 1 && counters.get(datanode1.getName()) <= 2);
      Assert.assertTrue(counters.get(datanode1.getName()) + counters.get(datanode2.getName()) == companionBlocks.size());
      counters=BlockPlacementPolicyRaid.countCompanionBlocks(companionBlocks,true);
      Assert.assertTrue(counters.get(datanode1.getParent().getName()) >= 1 && counters.get(datanode1.getParent().getName()) <= 2);
      Assert.assertTrue(counters.get(datanode1.getParent().getName()) + counters.get(datanode2.getParent().getName()) == companionBlocks.size());
    }
  }
  finally {
    if (cluster != null) {
      cluster.shutdown();
    }
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestCluster </h4><pre class="type-10 type-11 type-13 type-14 type-4 type-7 type-6 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions inside branch conditions">BranchVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies (un)successful execution of the test case by reporting explicitly a failure">UtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies assertions inside branch conditions
- Verifies (un)successful execution of the test case by reporting explicitly a failure
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects are null
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
/** 
 * Test to verify the common properties of tasks.
 * @throws Exception
 */
@Test public void testTaskDetails() throws Exception {
  Configuration conf=new Configuration(cluster.getConf());
  JTProtocol wovenClient=cluster.getJTClient().getProxy();
  FinishTaskControlAction.configureControlActionForJob(conf);
  SleepJob job=new SleepJob();
  job.setConf(conf);
  Job rJob=job.createJob(1,1,100,100,100,100);
  JobClient client=cluster.getJTClient().getClient();
  rJob.submit();
  RunningJob rJob1=client.getJob(org.apache.hadoop.mapred.JobID.downgrade(rJob.getJobID()));
  JobID id=rJob.getJobID();
  JobInfo jInfo=wovenClient.getJobInfo(id);
  while (jInfo.getStatus().getRunState() != JobStatus.RUNNING) {
    Thread.sleep(1000);
    jInfo=wovenClient.getJobInfo(id);
  }
  LOG.info("Waiting till job starts running one map");
  TaskInfo[] myTaskInfos=wovenClient.getTaskInfo(id);
  boolean isOneTaskStored=false;
  String sometaskpid=null;
  org.apache.hadoop.mapreduce.TaskAttemptID sometaskId=null;
  TTClient myCli=null;
  for (  TaskInfo info : myTaskInfos) {
    if (!info.isSetupOrCleanup()) {
      String[] taskTrackers=info.getTaskTrackers();
      for (      String taskTracker : taskTrackers) {
        TTInfo ttInfo=wovenClient.getTTInfo(taskTracker);
        TTClient ttCli=cluster.getTTClient(ttInfo.getStatus().getHost());
        TaskID taskId=info.getTaskID();
        TTTaskInfo ttTaskInfo=ttCli.getProxy().getTask(taskId);
        Assert.assertNotNull(ttTaskInfo);
        Assert.assertNotNull(ttTaskInfo.getConf());
        Assert.assertNotNull(ttTaskInfo.getUser());
        Assert.assertTrue(ttTaskInfo.getTaskStatus().getProgress() >= 0.0);
        Assert.assertTrue(ttTaskInfo.getTaskStatus().getProgress() <= 1.0);
        String pid=ttTaskInfo.getPid();
        int i=1;
        while (pid.isEmpty()) {
          Thread.sleep(1000);
          LOG.info("Waiting for task to report its pid back");
          ttTaskInfo=ttCli.getProxy().getTask(taskId);
          pid=ttTaskInfo.getPid();
          if (i == 40) {
            Assert.fail("The task pid not reported for 40 seconds.");
          }
          i++;
        }
        if (!isOneTaskStored) {
          sometaskpid=pid;
          sometaskId=ttTaskInfo.getTaskStatus().getTaskID();
          myCli=ttCli;
          isOneTaskStored=true;
        }
        LOG.info("verified task progress to be between 0 and 1");
        State state=ttTaskInfo.getTaskStatus().getRunState();
        if (ttTaskInfo.getTaskStatus().getProgress() < 1.0 && ttTaskInfo.getTaskStatus().getProgress() > 0.0) {
          Assert.assertEquals(TaskStatus.State.RUNNING,state);
          LOG.info("verified run state as " + state);
        }
        FinishTaskControlAction action=new FinishTaskControlAction(org.apache.hadoop.mapred.TaskID.downgrade(info.getTaskID()));
        ttCli.getProxy().sendAction(action);
      }
    }
  }
  rJob.killJob();
  int i=1;
  while (!rJob.isComplete()) {
    Thread.sleep(1000);
    if (i == 40) {
      Assert.fail("The job not completed within 40 seconds after killing it.");
    }
    i++;
  }
  TTTaskInfo myTaskInfo=myCli.getProxy().getTask(sometaskId.getTaskID());
  i=0;
  while (myTaskInfo != null && !myTaskInfo.getPid().isEmpty()) {
    LOG.info("sleeping till task is retired from TT memory");
    Thread.sleep(1000);
    myTaskInfo=myCli.getProxy().getTask(sometaskId.getTaskID());
    if (i == 40) {
      Assert.fail("Task not retired from TT memory within 40 seconds of job completeing");
    }
    i++;
  }
  Assert.assertFalse(myCli.getProxy().isProcessTreeAlive(sometaskpid));
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestConcatenatedCompressedInput </h4><pre class="type-10 type-11 type-13 type-4 type-7 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions inside branch conditions">BranchVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies assertions inside branch conditions
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
/** 
 * Test using the raw Inflater codec for reading gzip files.
 */
@Test public void testPrototypeInflaterGzip() throws IOException {
  CompressionCodec gzip=new GzipCodec();
  localFs.delete(workDir,true);
  System.out.println(COLOR_BR_BLUE + "testPrototypeInflaterGzip() using " + "non-native/Java Inflater and manual gzip header/trailer parsing"+ COLOR_NORMAL);
  final String fn="concat" + gzip.getDefaultExtension();
  Path fnLocal=new Path(System.getProperty("test.concat.data","/tmp"),fn);
  Path fnHDFS=new Path(workDir,fn);
  localFs.copyFromLocalFile(fnLocal,fnHDFS);
  final FileInputStream in=new FileInputStream(fnLocal.toString());
  assertEquals("concat bytes available",148,in.available());
  byte[] compressedBuf=new byte[256];
  int numBytesRead=in.read(compressedBuf,0,10);
  assertEquals("header bytes read",10,numBytesRead);
  assertEquals("1st byte",0x1f,compressedBuf[0] & 0xff);
  assertEquals("2nd byte",0x8b,compressedBuf[1] & 0xff);
  assertEquals("3rd byte (compression method)",8,compressedBuf[2] & 0xff);
  byte flags=(byte)(compressedBuf[3] & 0xff);
  if ((flags & 0x04) != 0) {
    numBytesRead=in.read(compressedBuf,0,2);
    assertEquals("XLEN bytes read",2,numBytesRead);
    int xlen=((compressedBuf[1] << 8) | compressedBuf[0]) & 0xffff;
    in.skip(xlen);
  }
  if ((flags & 0x08) != 0) {
    while ((numBytesRead=in.read()) != 0) {
      assertFalse("unexpected end-of-file while reading filename",numBytesRead == -1);
    }
  }
  if ((flags & 0x10) != 0) {
    while ((numBytesRead=in.read()) != 0) {
      assertFalse("unexpected end-of-file while reading comment",numBytesRead == -1);
    }
  }
  if ((flags & 0xe0) != 0) {
    assertTrue("reserved bits are set??",(flags & 0xe0) == 0);
  }
  if ((flags & 0x02) != 0) {
    numBytesRead=in.read(compressedBuf,0,2);
    assertEquals("CRC16 bytes read",2,numBytesRead);
    int crc16=((compressedBuf[1] << 8) | compressedBuf[0]) & 0xffff;
  }
  numBytesRead=in.read(compressedBuf);
  byte[] uncompressedBuf=new byte[256];
  Inflater inflater=new Inflater(true);
  inflater.setInput(compressedBuf,0,numBytesRead);
  try {
    int numBytesUncompressed=inflater.inflate(uncompressedBuf);
    String outString=new String(uncompressedBuf,0,numBytesUncompressed,"UTF-8");
    System.out.println("uncompressed data of first gzip member = [" + outString + "]");
  }
 catch (  java.util.zip.DataFormatException ex) {
    throw new IOException(ex.getMessage());
  }
  in.close();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestControlledJob </h4><pre class="type-11 type-13 type-14 type-4 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions inside branch conditions">BranchVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies (un)successful execution of the test case by reporting explicitly a failure">UtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies assertions in iterations
- Verifies assertions inside branch conditions
- Verifies (un)successful execution of the test case by reporting explicitly a failure
- Verifies values of objects/variables related to AUT calls
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testControlledJob() throws Exception {
  Configuration conf=new Configuration(cluster.getConf());
  JTProtocol wovenClient=cluster.getJTClient().getProxy();
  FinishTaskControlAction.configureControlActionForJob(conf);
  SleepJob job=new SleepJob();
  job.setConf(conf);
  Job slpJob=job.createJob(1,0,100,100,100,100);
  slpJob.submit();
  JobClient client=cluster.getJTClient().getClient();
  RunningJob rJob=client.getJob(org.apache.hadoop.mapred.JobID.downgrade(slpJob.getJobID()));
  JobID id=rJob.getID();
  JobInfo jInfo=wovenClient.getJobInfo(id);
  while (jInfo.getStatus().getRunState() != JobStatus.RUNNING) {
    Thread.sleep(1000);
    jInfo=wovenClient.getJobInfo(id);
  }
  LOG.info("Waiting till job starts running one map");
  jInfo=wovenClient.getJobInfo(id);
  Assert.assertEquals(jInfo.runningMaps(),1);
  LOG.info("waiting for another cycle to " + "check if the maps dont finish off");
  Thread.sleep(1000);
  jInfo=wovenClient.getJobInfo(id);
  Assert.assertEquals(jInfo.runningMaps(),1);
  TaskInfo[] taskInfos=wovenClient.getTaskInfo(id);
  for (  TaskInfo info : taskInfos) {
    LOG.info("constructing control action to signal task to finish");
    FinishTaskControlAction action=new FinishTaskControlAction(TaskID.downgrade(info.getTaskID()));
    for (    TTClient cli : cluster.getTTClients()) {
      cli.getProxy().sendAction(action);
    }
  }
  jInfo=wovenClient.getJobInfo(id);
  int i=1;
  if (jInfo != null) {
    while (!jInfo.getStatus().isJobComplete()) {
      Thread.sleep(1000);
      jInfo=wovenClient.getJobInfo(id);
      if (jInfo == null) {
        break;
      }
      if (i > 40) {
        Assert.fail("Controlled Job with ID : " + jInfo.getID() + " has not completed in 40 seconds after signalling.");
      }
      i++;
    }
  }
  LOG.info("Job sucessfully completed after signalling!!!!");
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestDistributedCacheModifiedFile </h4><pre class="type-10 type-11 type-13 type-14 type-4 type-7 type-6 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions inside branch conditions">BranchVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies (un)successful execution of the test case by reporting explicitly a failure">UtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies assertions inside branch conditions
- Verifies (un)successful execution of the test case by reporting explicitly a failure
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects are null
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testDistributedCache() throws Exception {
  Configuration conf=new Configuration(cluster.getConf());
  JTProtocol wovenClient=cluster.getJTClient().getProxy();
  int count=0;
  boolean continueLoop=true;
  int countLoop=0;
  int taskTrackerCounter=0;
  ArrayList<String> taskTrackerCollection=new ArrayList<String>();
  boolean taskTrackerFound=false;
  do {
    SleepJob job=new SleepJob();
    job.setConf(conf);
    Job slpJob=job.createJob(5,1,1000,1000,100,100);
    String input="This will be the content of\n" + "distributed cache\n";
    DataOutputStream file=UtilsForTests.createTmpFileDFS(dfs,URIPATH,permission,input);
    DistributedCache.createSymlink(conf);
    URI uri=URI.create(uriPath);
    DistributedCache.addCacheFile(uri,conf);
    JobConf jconf=new JobConf(conf);
    FinishTaskControlAction.configureControlActionForJob(conf);
    slpJob.submit();
    RunningJob rJob=cluster.getJTClient().getClient().getJob(org.apache.hadoop.mapred.JobID.downgrade(slpJob.getJobID()));
    countLoop++;
    TTClient tClient=null;
    JobInfo jInfo=wovenClient.getJobInfo(rJob.getID());
    LOG.info("jInfo is :" + jInfo);
    Assert.assertNotNull("jobInfo is null",jInfo);
    count=0;
    while (jInfo.getStatus().getRunState() != JobStatus.RUNNING) {
      UtilsForTests.waitFor(10000);
      count++;
      jInfo=wovenClient.getJobInfo(rJob.getID());
      if (count > 10) {
        Assert.fail("job has not reached running state for more than" + "100 seconds. Failing at this point");
      }
    }
    LOG.info("job id is :" + rJob.getID().toString());
    TaskInfo[] taskInfos=cluster.getJTClient().getProxy().getTaskInfo(rJob.getID());
    boolean distCacheFileIsFound;
    for (    TaskInfo taskInfo : taskInfos) {
      distCacheFileIsFound=false;
      String[] taskTrackers=taskInfo.getTaskTrackers();
      for (      String taskTracker : taskTrackers) {
        taskTracker=UtilsForTests.getFQDNofTT(taskTracker);
        LOG.info("taskTracker is :" + taskTracker);
        taskTrackerFound=false;
        if (countLoop > 1) {
          if (taskTracker != null) {
            continueLoop=taskTrackerCollection.contains(taskTracker);
          }
          if (continueLoop) {
            taskTrackerFound=true;
          }
        }
        if (taskTracker != null)         taskTrackerCollection.add(taskTracker);
        if (countLoop > 1) {
          continueLoop=false;
        }
        tClient=cluster.getTTClient(taskTracker);
        if (tClient == null) {
          continue;
        }
        String[] localDirs=tClient.getMapredLocalDirs();
        int distributedFileCount=0;
        for (        String localDir : localDirs) {
          localDir=localDir + Path.SEPARATOR + TaskTracker.getPublicDistributedCacheDir();
          LOG.info("localDir is : " + localDir);
          FileStatus[] fileStatuses=tClient.listStatus(localDir,true,true);
          for (          FileStatus fileStatus : fileStatuses) {
            Path path=fileStatus.getPath();
            LOG.info("path is :" + path.toString());
            distCacheFileIsFound=(path.toString()).endsWith(distributedFileName);
            if (distCacheFileIsFound) {
              LOG.info("PATH found is :" + path.toString());
              distributedFileCount++;
              String filename=path.getName();
              FsPermission fsPerm=fileStatus.getPermission();
              Assert.assertTrue("File Permission is not 777",fsPerm.equals(new FsPermission("777")));
            }
          }
        }
        LOG.debug("The distributed FileCount is :" + distributedFileCount);
        LOG.debug("The taskTrackerFound is :" + taskTrackerFound);
        if (distributedFileCount != 2 && taskTrackerFound) {
          Assert.fail("The distributed cache file has to be two. " + "But found was " + distributedFileCount);
        }
 else         if (distributedFileCount > 1 && !taskTrackerFound) {
          Assert.fail("The distributed cache file cannot more than one." + " But found was " + distributedFileCount);
        }
 else         if (distributedFileCount < 1)         Assert.fail("The distributed cache file is less than one. " + "But found was " + distributedFileCount);
        if (!distCacheFileIsFound) {
          Assert.assertEquals("The distributed cache file does not exist",distCacheFileIsFound,false);
        }
      }
    }
    for (    TaskInfo taskInfoRemaining : taskInfos) {
      FinishTaskControlAction action=new FinishTaskControlAction(TaskID.downgrade(taskInfoRemaining.getTaskID()));
      Collection<TTClient> tts=cluster.getTTClients();
      for (      TTClient cli : tts) {
        cli.getProxy().sendAction(action);
      }
    }
    rJob.killJob();
    Thread.sleep(3000);
    TaskInfo[] myTaskInfos=wovenClient.getTaskInfo(rJob.getID());
    if (myTaskInfos != null) {
      for (      TaskInfo info : myTaskInfos) {
        if (info.isSetupOrCleanup()) {
          String[] taskTrackers=info.getTaskTrackers();
          for (          String taskTracker : taskTrackers) {
            taskTracker=UtilsForTests.getFQDNofTT(taskTracker);
            LOG.info("taskTracker is :" + taskTracker);
            if (taskTracker != null)             taskTrackerCollection.add(taskTracker);
          }
        }
      }
    }
    while (jInfo != null && !jInfo.getStatus().isJobComplete()) {
      Thread.sleep(10000);
      jInfo=wovenClient.getJobInfo(rJob.getID());
    }
  }
 while (continueLoop);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestDistributedCachePrivateFile </h4><pre class="type-10 type-11 type-13 type-14 type-4 type-7 type-6 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions inside branch conditions">BranchVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies (un)successful execution of the test case by reporting explicitly a failure">UtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies assertions inside branch conditions
- Verifies (un)successful execution of the test case by reporting explicitly a failure
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects are null
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testDistributedCache() throws Exception {
  Configuration conf=new Configuration(cluster.getConf());
  JTProtocol wovenClient=cluster.getJTClient().getProxy();
  int count=0;
  SleepJob job=new SleepJob();
  job.setConf(conf);
  Job slpJob=job.createJob(5,1,1000,1000,100,100);
  DistributedCache.createSymlink(conf);
  URI uri=URI.create(uriPath);
  DistributedCache.addCacheFile(uri,conf);
  JobConf jconf=new JobConf(conf);
  FinishTaskControlAction.configureControlActionForJob(conf);
  slpJob.submit();
  RunningJob rJob=cluster.getJTClient().getClient().getJob(org.apache.hadoop.mapred.JobID.downgrade(slpJob.getJobID()));
  JobStatus[] jobStatus=client.getAllJobs();
  String userName=jobStatus[0].getUsername();
  TTClient tClient=null;
  JobInfo jInfo=wovenClient.getJobInfo(rJob.getID());
  LOG.info("jInfo is :" + jInfo);
  Assert.assertNotNull("jobInfo is null",jInfo);
  count=0;
  while (jInfo.getStatus().getRunState() != JobStatus.RUNNING) {
    UtilsForTests.waitFor(10000);
    count++;
    jInfo=wovenClient.getJobInfo(rJob.getID());
    if (count > 10) {
      Assert.fail("job has not reached running state for more than" + "100 seconds. Failing at this point");
    }
  }
  LOG.info("job id is :" + rJob.getID().toString());
  TaskInfo[] taskInfos=cluster.getJTClient().getProxy().getTaskInfo(rJob.getID());
  boolean distCacheFileIsFound;
  for (  TaskInfo taskInfo : taskInfos) {
    distCacheFileIsFound=false;
    String[] taskTrackers=taskInfo.getTaskTrackers();
    for (    String taskTracker : taskTrackers) {
      taskTracker=UtilsForTests.getFQDNofTT(taskTracker);
      tClient=cluster.getTTClient(taskTracker);
      String[] localDirs=tClient.getMapredLocalDirs();
      int distributedFileCount=0;
      String localDirOnly=null;
      boolean FileNotPresentForThisDirectoryPath=false;
      for (      String localDir : localDirs) {
        FileNotPresentForThisDirectoryPath=false;
        localDirOnly=localDir;
        localDirOnly=localDir + Path.SEPARATOR + TaskTracker.SUBDIR+ Path.SEPARATOR+ userName;
        localDir=localDir + Path.SEPARATOR + TaskTracker.getPrivateDistributedCacheDir(userName);
        FileStatus fileStatusMapredLocalDirUserName=null;
        try {
          fileStatusMapredLocalDirUserName=tClient.getFileStatus(localDirOnly,true);
        }
 catch (        Exception e) {
          LOG.info("LocalDirOnly :" + localDirOnly + " not found");
          FileNotPresentForThisDirectoryPath=true;
        }
        if (FileNotPresentForThisDirectoryPath)         continue;
        Path pathMapredLocalDirUserName=fileStatusMapredLocalDirUserName.getPath();
        FsPermission fsPermMapredLocalDirUserName=fileStatusMapredLocalDirUserName.getPermission();
        Assert.assertTrue("Directory Permission is not 700",fsPermMapredLocalDirUserName.equals(new FsPermission("700")));
        FileStatus[] fileStatuses=tClient.listStatus(localDir,true,true);
        for (        FileStatus fileStatus : fileStatuses) {
          Path path=fileStatus.getPath();
          LOG.info("path is :" + path.toString());
          distCacheFileIsFound=(path.toString()).endsWith(distributedFileName);
          if (distCacheFileIsFound) {
            LOG.info("PATH found is :" + path.toString());
            distributedFileCount++;
            String filename=path.getName();
            FsPermission fsPerm=fileStatus.getPermission();
            Assert.assertTrue("File Permission is not 777",fsPerm.equals(new FsPermission("777")));
          }
        }
      }
      LOG.info("Distributed File count is :" + distributedFileCount);
      if (distributedFileCount > 1) {
        Assert.fail("The distributed cache file is more than one");
      }
 else       if (distributedFileCount < 1)       Assert.fail("The distributed cache file is less than one");
      if (!distCacheFileIsFound) {
        Assert.assertEquals("The distributed cache file does not exist",distCacheFileIsFound,false);
      }
    }
    for (    TaskInfo taskInfoRemaining : taskInfos) {
      FinishTaskControlAction action=new FinishTaskControlAction(TaskID.downgrade(taskInfoRemaining.getTaskID()));
      Collection<TTClient> tts=cluster.getTTClients();
      for (      TTClient cli : tts) {
        cli.getProxy().sendAction(action);
      }
    }
    rJob.killJob();
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestDistributedCacheUnModifiedFile </h4><pre class="type-10 type-11 type-13 type-14 type-4 type-7 type-6 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions inside branch conditions">BranchVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies (un)successful execution of the test case by reporting explicitly a failure">UtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies assertions inside branch conditions
- Verifies (un)successful execution of the test case by reporting explicitly a failure
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects are null
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testDistributedCache() throws Exception {
  Configuration conf=new Configuration(cluster.getConf());
  JTProtocol wovenClient=cluster.getJTClient().getProxy();
  int count=0;
  boolean continueLoop=true;
  int countLoop=0;
  int taskTrackerCounter=0;
  ArrayList<String> taskTrackerCollection=new ArrayList<String>();
  do {
    SleepJob job=new SleepJob();
    job.setConf(conf);
    Job slpJob=job.createJob(5,1,1000,1000,100,100);
    DistributedCache.createSymlink(conf);
    URI uri=URI.create(uriPath);
    DistributedCache.addCacheFile(uri,conf);
    JobConf jconf=new JobConf(conf);
    FinishTaskControlAction.configureControlActionForJob(conf);
    slpJob.submit();
    RunningJob rJob=cluster.getJTClient().getClient().getJob(org.apache.hadoop.mapred.JobID.downgrade(slpJob.getJobID()));
    countLoop++;
    TTClient tClient=null;
    JobInfo jInfo=wovenClient.getJobInfo(rJob.getID());
    LOG.info("jInfo is :" + jInfo);
    Assert.assertNotNull("jobInfo is null",jInfo);
    count=0;
    while (jInfo.getStatus().getRunState() != JobStatus.RUNNING) {
      UtilsForTests.waitFor(10000);
      count++;
      jInfo=wovenClient.getJobInfo(rJob.getID());
      if (count > 10) {
        Assert.fail("job has not reached running state for more than" + "100 seconds. Failing at this point");
      }
    }
    LOG.info("job id is :" + rJob.getID().toString());
    TaskInfo[] taskInfos=cluster.getJTClient().getProxy().getTaskInfo(rJob.getID());
    boolean distCacheFileIsFound;
    for (    TaskInfo taskInfo : taskInfos) {
      distCacheFileIsFound=false;
      String[] taskTrackers=taskInfo.getTaskTrackers();
      for (      String taskTracker : taskTrackers) {
        taskTracker=UtilsForTests.getFQDNofTT(taskTracker);
        LOG.info("taskTracker is :" + taskTracker);
        if (countLoop > 1) {
          if (taskTracker != null) {
            continueLoop=taskTrackerCollection.contains(taskTracker);
          }
          if (!continueLoop) {
            break;
          }
        }
        if (taskTracker != null)         taskTrackerCollection.add(taskTracker);
        if (countLoop > 2) {
          continueLoop=false;
        }
        tClient=cluster.getTTClient(taskTracker);
        if (tClient == null) {
          continue;
        }
        String[] localDirs=tClient.getMapredLocalDirs();
        int distributedFileCount=0;
        for (        String localDir : localDirs) {
          localDir=localDir + Path.SEPARATOR + TaskTracker.getPublicDistributedCacheDir();
          LOG.info("localDir is : " + localDir);
          FileStatus[] fileStatuses=tClient.listStatus(localDir,true,true);
          for (          FileStatus fileStatus : fileStatuses) {
            Path path=fileStatus.getPath();
            LOG.info("path is :" + path.toString());
            distCacheFileIsFound=(path.toString()).endsWith(distributedFileName);
            if (distCacheFileIsFound) {
              LOG.info("PATH found is :" + path.toString());
              distributedFileCount++;
              String filename=path.getName();
              FsPermission fsPerm=fileStatus.getPermission();
              Assert.assertTrue("File Permission is not 777",fsPerm.equals(new FsPermission("777")));
            }
          }
        }
        if (distributedFileCount > 1) {
          Assert.fail("The distributed cache file is more than one");
        }
 else         if (distributedFileCount < 1)         Assert.fail("The distributed cache file is less than one");
        if (!distCacheFileIsFound) {
          Assert.assertEquals("The distributed cache file does not exist",distCacheFileIsFound,false);
        }
      }
    }
    for (    TaskInfo taskInfoRemaining : taskInfos) {
      FinishTaskControlAction action=new FinishTaskControlAction(TaskID.downgrade(taskInfoRemaining.getTaskID()));
      Collection<TTClient> tts=cluster.getTTClients();
      for (      TTClient cli : tts) {
        cli.getProxy().sendAction(action);
      }
    }
    rJob.killJob();
  }
 while (continueLoop);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestMapRed </h4><pre class="type-11 type-4 type-7 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies assertions in iterations
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
"></span><br>
@Test public void testNullKeys() throws Exception {
  JobConf conf=new JobConf(TestMapRed.class);
  FileSystem fs=FileSystem.getLocal(conf);
  HashSet<String> values=new HashSet<String>();
  String m="AAAAAAAAAAAAAA";
  for (int i=1; i < 11; ++i) {
    values.add(m);
    m=m.replace((char)('A' + i - 1),(char)('A' + i));
  }
  Path testdir=new Path(System.getProperty("test.build.data","/tmp")).makeQualified(fs);
  fs.delete(testdir,true);
  Path inFile=new Path(testdir,"nullin/blah");
  SequenceFile.Writer w=SequenceFile.createWriter(fs,conf,inFile,NullWritable.class,Text.class,SequenceFile.CompressionType.NONE);
  Text t=new Text();
  for (  String s : values) {
    t.set(s);
    w.append(NullWritable.get(),t);
  }
  w.close();
  FileInputFormat.setInputPaths(conf,inFile);
  FileOutputFormat.setOutputPath(conf,new Path(testdir,"nullout"));
  conf.setMapperClass(NullMapper.class);
  conf.setReducerClass(IdentityReducer.class);
  conf.setOutputKeyClass(NullWritable.class);
  conf.setOutputValueClass(Text.class);
  conf.setInputFormat(SequenceFileInputFormat.class);
  conf.setOutputFormat(SequenceFileOutputFormat.class);
  conf.setNumReduceTasks(1);
  JobClient.runJob(conf);
  SequenceFile.Reader r=new SequenceFile.Reader(fs,new Path(testdir,"nullout/part-00000"),conf);
  m="AAAAAAAAAAAAAA";
  for (int i=1; r.next(NullWritable.get(),t); ++i) {
    assertTrue("Unexpected value: " + t,values.remove(t.toString()));
    m=m.replace((char)('A' + i - 1),(char)('A' + i));
  }
  assertTrue("Missing values: " + values.toString(),values.isEmpty());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestRefreshOfQueues </h4><pre class="type-10 type-11 type-4 type-5 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies values of objects/variables related to AUT calls
- Verifies whether objects/variable are equal to an expected value 
"></span><br>
/** 
 * @throws Throwable
 */
@Test public void testRefreshOfQueuesSanity() throws Throwable {
  JobQueueInfo[] queues=TestQueueManagerRefresh.getSimpleQueueHierarchy();
  Properties[] props=new Properties[3];
  for (int i=0; i < props.length; i++) {
    props[i]=queues[i].getProperties();
    props[i].setProperty(CapacitySchedulerConf.CAPACITY_PROPERTY,String.valueOf(i + 10));
    props[i].setProperty(CapacitySchedulerConf.MAX_CAPACITY_PROPERTY,String.valueOf(i + 15));
    props[i].setProperty(CapacitySchedulerConf.SUPPORTS_PRIORITY_PROPERTY,String.valueOf(false));
    props[i].setProperty(CapacitySchedulerConf.MAXIMUM_INITIALIZED_JOBS_PER_USER_PROPERTY,String.valueOf(i + 11));
    props[i].setProperty(CapacitySchedulerConf.MINIMUM_USER_LIMIT_PERCENT_PROPERTY,String.valueOf(i + 16));
  }
  QueueManagerTestUtils.writeQueueConfigurationFile(queueConfigFile.getAbsolutePath(),new JobQueueInfo[]{queues[0]});
  setupAndStartSchedulerFramework(0,0,0);
  Map<String,AbstractQueue> allQueues=getAllQueues(scheduler);
  for (int i=0; i < queues.length; i++) {
    String qName=queues[i].getQueueName();
    LOG.info("Queue name : " + qName);
    QueueSchedulingContext qsc=allQueues.get(qName).getQueueSchedulingContext();
    LOG.info("Context for queue " + qName + " is : "+ qsc);
    assertEquals(i + 10,qsc.getCapacityPercent(),0);
    assertEquals(i + 15,qsc.getMaxCapacityPercent(),0);
    assertEquals(Boolean.valueOf(false),Boolean.valueOf(qsc.supportsPriorities()));
    assertEquals(i + 16,qsc.getUlMin());
  }
  for (int i=0; i < props.length; i++) {
    props[i]=queues[i].getProperties();
    props[i].setProperty(CapacitySchedulerConf.CAPACITY_PROPERTY,String.valueOf(i + 20));
    props[i].setProperty(CapacitySchedulerConf.MAX_CAPACITY_PROPERTY,String.valueOf(i + 25));
    props[i].setProperty(CapacitySchedulerConf.SUPPORTS_PRIORITY_PROPERTY,String.valueOf(false));
    props[i].setProperty(CapacitySchedulerConf.MAXIMUM_INITIALIZED_JOBS_PER_USER_PROPERTY,String.valueOf(i + 5));
    props[i].setProperty(CapacitySchedulerConf.MINIMUM_USER_LIMIT_PERCENT_PROPERTY,String.valueOf(i + 10));
  }
  QueueManagerTestUtils.writeQueueConfigurationFile(queueConfigFile.getAbsolutePath(),new JobQueueInfo[]{queues[0]});
  refreshQueues(taskTrackerManager.getQueueManager(),null,scheduler);
  allQueues=getAllQueues(scheduler);
  for (int i=0; i < queues.length; i++) {
    String qName=queues[i].getQueueName();
    LOG.info("Queue name : " + qName);
    QueueSchedulingContext qsc=allQueues.get(qName).getQueueSchedulingContext();
    assertEquals(qName,qsc.getQueueName());
    LOG.info("Context for queue " + qName + " is : "+ qsc);
    assertEquals(i + 20,qsc.getCapacityPercent(),0);
    assertEquals(i + 25,qsc.getMaxCapacityPercent(),0);
    assertEquals(Boolean.valueOf(false),Boolean.valueOf(qsc.supportsPriorities()));
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestSimulatorEventQueue </h4><pre class="type-11 type-13 type-4 type-7 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions inside branch conditions">BranchVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies assertions in iterations
- Verifies assertions inside branch conditions
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
"></span><br>
@Test public void testKeepOrder(){
  SimulatorEventQueue queue=new SimulatorEventQueue();
  SimulatorEventListener listener=new TestListener();
  List<SimulatorEvent> listEvent=new ArrayList<SimulatorEvent>();
  int count=0;
  for (int i=0; i < random.nextInt(100); i++) {
    listEvent.clear();
    for (int j=0; j < random.nextInt(5); j++) {
      listEvent.add(new TestEventWithCount(listener,random.nextInt(10),count++));
    }
    queue.addAll(listEvent);
  }
  TestEventWithCount next;
  TestEventWithCount last=null;
  while ((next=(TestEventWithCount)queue.get()) != null) {
    if (last != null && last.getTimeStamp() == next.getTimeStamp()) {
      Assert.assertTrue(last.getCount() < next.getCount());
    }
    last=next;
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestTaskKilling </h4><pre class="type-11 type-13 type-14 type-4 type-7 type-6 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions inside branch conditions">BranchVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies (un)successful execution of the test case by reporting explicitly a failure">UtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies assertions in iterations
- Verifies assertions inside branch conditions
- Verifies (un)successful execution of the test case by reporting explicitly a failure
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects are null
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testAllTaskAttemptKill() throws Exception {
  Configuration conf=new Configuration(cluster.getConf());
  JobStatus[] jobStatus=null;
  SleepJob job=new SleepJob();
  job.setConf(conf);
  Job slpJob=job.createJob(3,1,40000,1000,100,100);
  JobConf jconf=new JobConf(conf);
  slpJob.submit();
  RunningJob rJob=cluster.getJTClient().getClient().getJob(org.apache.hadoop.mapred.JobID.downgrade(slpJob.getJobID()));
  int MAX_MAP_TASK_ATTEMPTS=Integer.parseInt(jconf.get(MRJobConfig.MAP_MAX_ATTEMPTS));
  LOG.info("MAX_MAP_TASK_ATTEMPTS is : " + MAX_MAP_TASK_ATTEMPTS);
  Assert.assertTrue(MAX_MAP_TASK_ATTEMPTS > 0);
  TTClient tClient=null;
  TTClient[] ttClients=null;
  JobInfo jInfo=remoteJTClient.getJobInfo(rJob.getID());
  Assert.assertNotNull(jInfo.getStatus().getRunState());
  while (jInfo.getStatus().getRunState() != JobStatus.RUNNING) {
    try {
      Thread.sleep(10000);
    }
 catch (    InterruptedException e) {
    }
    ;
    jInfo=remoteJTClient.getJobInfo(rJob.getID());
  }
  JobID jobidStore=rJob.getID();
  jobidStore=JobID.downgrade(jobidStore);
  LOG.info("job id is :" + jobidStore.toString());
  TaskInfo[] taskInfos=null;
  boolean runningCount=false;
  int count=0;
  do {
    taskInfos=cluster.getJTClient().getProxy().getTaskInfo(rJob.getID());
    runningCount=false;
    for (    TaskInfo taskInfo : taskInfos) {
      TaskStatus[] taskStatuses=taskInfo.getTaskStatus();
      if (taskStatuses.length > 0) {
        LOG.info("taskStatuses[0].getRunState() is :" + taskStatuses[0].getRunState());
        if (taskStatuses[0].getRunState() == TaskStatus.State.RUNNING) {
          runningCount=true;
          break;
        }
 else {
          LOG.info("Sleeping 5 seconds");
          Thread.sleep(5000);
        }
      }
    }
    count++;
    if (count > 10) {
      Assert.fail("Since the sleep count has reached beyond a point" + "failing at this point");
    }
  }
 while (!runningCount);
  String taskIdKilled=null;
  for (int i=0; i < MAX_MAP_TASK_ATTEMPTS; i++) {
    taskInfos=cluster.getJTClient().getProxy().getTaskInfo(rJob.getID());
    for (    TaskInfo taskInfo : taskInfos) {
      TaskAttemptID taskAttemptID;
      if (!taskInfo.isSetupOrCleanup()) {
        TaskID taskid=TaskID.downgrade(taskInfo.getTaskID());
        LOG.info("taskid is :" + taskid);
        if (i == 0) {
          taskIdKilled=taskid.toString();
          taskAttemptID=new TaskAttemptID(taskid,i);
          LOG.info("taskAttemptid going to be killed is : " + taskAttemptID);
          (new JobClient.NetworkedJob(jInfo.getStatus(),jobClient.cluster)).killTask(taskAttemptID,true);
          checkTaskCompletionEvent(taskAttemptID,jInfo);
          break;
        }
 else {
          if (taskIdKilled.equals(taskid.toString())) {
            taskAttemptID=new TaskAttemptID(taskid,i);
            LOG.info("taskAttemptid going to be killed is : " + taskAttemptID);
            (new JobClient.NetworkedJob(jInfo.getStatus(),jobClient.cluster)).killTask(taskAttemptID,true);
            checkTaskCompletionEvent(taskAttemptID,jInfo);
            break;
          }
        }
      }
    }
  }
  while (jInfo != null && !jInfo.getStatus().isJobComplete()) {
    Thread.sleep(10000);
    jInfo=remoteJTClient.getJobInfo(rJob.getID());
  }
  jobStatus=jobClient.getAllJobs();
  JobStatus jobStatusFound=null;
  for (  JobStatus jobStatusTmp : jobStatus) {
    if (JobID.downgrade(jobStatusTmp.getJobID()).equals(jobidStore)) {
      jobStatusFound=jobStatusTmp;
      LOG.info("jobStatus found is :" + jobStatusFound.getJobId().toString());
    }
  }
  Assert.assertEquals("The job should have failed at this stage",JobStatus.FAILED,jobStatusFound.getRunState());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestTaskOwner </h4><pre class="type-10 type-11 type-13 type-4 type-5 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions inside branch conditions">BranchVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies assertions inside branch conditions
- Verifies values of objects/variables related to AUT calls
- Verifies whether objects/variable are equal to an expected value 
"></span><br>
@Test public void testProcessPermission() throws Exception {
  Configuration conf=cluster.getJTClient().getConf();
  Job job=new Job(conf,"user name check");
  job.setJarByClass(UserNamePermission.class);
  job.setMapperClass(UserNamePermission.UserNameMapper.class);
  job.setCombinerClass(UserNamePermission.UserNameReducer.class);
  job.setMapOutputKeyClass(Text.class);
  job.setMapOutputValueClass(Text.class);
  job.setReducerClass(UserNamePermission.UserNameReducer.class);
  job.setNumReduceTasks(1);
  FileInputFormat.addInputPath(job,inDir);
  FileOutputFormat.setOutputPath(job,outDir);
  job.waitForCompletion(true);
  FileSystem fs=outDir.getFileSystem(conf);
  Path[] fileList=FileUtil.stat2Paths(fs.listStatus(outDir,new Utils.OutputFileUtils.OutputFilesFilter()));
  for (int i=0; i < fileList.length; ++i) {
    LOG.info("File list[" + i + "]"+ ": "+ fileList[i]);
    BufferedReader file=new BufferedReader(new InputStreamReader(fs.open(fileList[i])));
    String line=file.readLine();
    while (line != null) {
      StringTokenizer token=new StringTokenizer(line);
      if (token.hasMoreTokens()) {
        LOG.info("First token " + token.nextToken());
        String userName=token.nextToken();
        LOG.info("Next token " + userName);
        Assert.assertEquals("The user name did not match permission violation ",userName,System.getProperty("user.name").toString());
        break;
      }
    }
    file.close();
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestTaskTrackerMemoryManager </h4><pre class="type-10 type-11 type-7 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies boolean conditions
"></span><br>
/** 
 * Test for verifying that tasks causing cumulative usage to go beyond TT's
 * limit get killed even though they all are under individual limits. Memory
 * management for tasks with disabled task-limits also traverses the same
 * code-path, so we don't need a separate testTaskLimitsDisabled.
 * @throws Exception
 */
@Test public void testTasksCumulativelyExceedingTTLimits() throws Exception {
  if (!isProcfsBasedTreeAvailable()) {
    return;
  }
  long PER_TASK_LIMIT=100 * 1024L;
  JobConf fConf=new JobConf();
  fConf.setLong(MRConfig.MAPMEMORY_MB,1L);
  fConf.setLong(MRConfig.REDUCEMEMORY_MB,1L);
  long TASK_TRACKER_LIMIT=2 * 1024 * 1024L;
  fConf.setInt(TTConfig.TT_MEMORY_MANAGER_MONITORING_INTERVAL,100);
  startCluster(fConf);
  Pattern taskOverLimitPattern=Pattern.compile(String.format(taskOverLimitPatternString,String.valueOf(PER_TASK_LIMIT)));
  Pattern trackerOverLimitPattern=Pattern.compile("Killing one of the least progress tasks - .*, as " + "the cumulative memory usage of all the tasks on the TaskTracker" + " exceeds virtual memory limit " + TASK_TRACKER_LIMIT + ".");
  Matcher mat=null;
  JobConf conf=new JobConf(miniMRCluster.createJobConf());
  conf.setMemoryForMapTask(PER_TASK_LIMIT);
  conf.setMemoryForReduceTask(PER_TASK_LIMIT);
  JobClient jClient=new JobClient(conf);
  SleepJob sleepJob=new SleepJob();
  sleepJob.setConf(conf);
  Job job=sleepJob.createJob(1,1,5000,1,1000,1);
  job.submit();
  boolean TTOverFlowMsgPresent=false;
  while (true) {
    List<TaskReport> allTaskReports=new ArrayList<TaskReport>();
    allTaskReports.addAll(Arrays.asList(jClient.getSetupTaskReports(JobID.downgrade(job.getJobID()))));
    allTaskReports.addAll(Arrays.asList(jClient.getMapTaskReports(JobID.downgrade(job.getJobID()))));
    for (    TaskReport tr : allTaskReports) {
      String[] diag=tr.getDiagnostics();
      for (      String str : diag) {
        mat=taskOverLimitPattern.matcher(str);
        assertFalse(mat.find());
        mat=trackerOverLimitPattern.matcher(str);
        if (mat.find()) {
          TTOverFlowMsgPresent=true;
        }
      }
    }
    if (TTOverFlowMsgPresent) {
      break;
    }
    try {
      Thread.sleep(1000);
    }
 catch (    InterruptedException e) {
    }
  }
  job.killJob();
}

</code></pre>

<br>
<pre class="type-11 type-4 type-7 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies assertions in iterations
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
"></span><br>
/** 
 * Test for verifying that tasks causing cumulative usage of physical memory
 * to go beyond TT's limit get killed.
 * @throws Exception
 */
@Test public void testTasksCumulativelyExceedingTTPhysicalLimits() throws Exception {
  if (!isProcfsBasedTreeAvailable()) {
    return;
  }
  JobConf fConf=new JobConf();
  fConf.setInt(TTConfig.TT_MEMORY_MANAGER_MONITORING_INTERVAL,100);
  LinuxResourceCalculatorPlugin memoryCalculatorPlugin=new LinuxResourceCalculatorPlugin();
  long totalPhysicalMemory=memoryCalculatorPlugin.getPhysicalMemorySize();
  long reservedPhysicalMemory=totalPhysicalMemory / (1024 * 1024) + 1;
  fConf.setLong(TTConfig.TT_RESERVED_PHYSCIALMEMORY_MB,reservedPhysicalMemory);
  long maxRssMemoryAllowedForAllTasks=totalPhysicalMemory - reservedPhysicalMemory * 1024 * 1024L;
  Pattern physicalMemoryOverLimitPattern=Pattern.compile("Killing one of the memory-consuming tasks - .*" + ", as the cumulative RSS memory usage of all the tasks on " + "the TaskTracker exceeds physical memory limit " + maxRssMemoryAllowedForAllTasks + ".");
  startCluster(fConf);
  Matcher mat=null;
  JobConf conf=new JobConf(miniMRCluster.createJobConf());
  conf.setLong(MRJobConfig.MAP_MEMORY_PHYSICAL_MB,2 * 1024L);
  conf.setLong(MRJobConfig.REDUCE_MEMORY_PHYSICAL_MB,2 * 1024L);
  JobClient jClient=new JobClient(conf);
  SleepJob sleepJob=new SleepJob();
  sleepJob.setConf(conf);
  Job job=sleepJob.createJob(1,1,100000,1,100000,1);
  job.submit();
  boolean TTOverFlowMsgPresent=false;
  while (true) {
    List<TaskReport> allTaskReports=new ArrayList<TaskReport>();
    allTaskReports.addAll(Arrays.asList(jClient.getSetupTaskReports(JobID.downgrade(job.getJobID()))));
    allTaskReports.addAll(Arrays.asList(jClient.getMapTaskReports(JobID.downgrade(job.getJobID()))));
    for (    TaskReport tr : allTaskReports) {
      String[] diag=tr.getDiagnostics();
      for (      String str : diag) {
        mat=physicalMemoryOverLimitPattern.matcher(str);
        if (mat.find()) {
          TTOverFlowMsgPresent=true;
        }
      }
    }
    if (TTOverFlowMsgPresent) {
      break;
    }
    assertFalse("Job should not finish successfully",job.isSuccessful());
    try {
      Thread.sleep(1000);
    }
 catch (    InterruptedException e) {
    }
  }
  job.killJob();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestTextInputFormat </h4><pre class="type-10 type-11 type-13 type-4 type-7 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions inside branch conditions">BranchVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies assertions inside branch conditions
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testFormat() throws Exception {
  JobConf job=new JobConf(defaultConf);
  Path file=new Path(workDir,"test.txt");
  Reporter reporter=Reporter.NULL;
  int seed=new Random().nextInt();
  LOG.info("seed = " + seed);
  Random random=new Random(seed);
  localFs.delete(workDir,true);
  FileInputFormat.setInputPaths(job,workDir);
  for (int length=0; length < MAX_LENGTH; length+=random.nextInt(MAX_LENGTH / 10) + 1) {
    LOG.debug("creating; entries = " + length);
    Writer writer=new OutputStreamWriter(localFs.create(file));
    try {
      for (int i=0; i < length; i++) {
        writer.write(Integer.toString(i));
        writer.write("\n");
      }
    }
  finally {
      writer.close();
    }
    TextInputFormat format=new TextInputFormat();
    format.configure(job);
    LongWritable key=new LongWritable();
    Text value=new Text();
    for (int i=0; i < 3; i++) {
      int numSplits=random.nextInt(MAX_LENGTH / 20) + 1;
      LOG.debug("splitting: requesting = " + numSplits);
      InputSplit[] splits=format.getSplits(job,numSplits);
      LOG.debug("splitting: got =        " + splits.length);
      if (length == 0) {
        assertEquals("Files of length 0 are not returned from FileInputFormat.getSplits().",1,splits.length);
        assertEquals("Empty file length == 0",0,splits[0].getLength());
      }
      BitSet bits=new BitSet(length);
      for (int j=0; j < splits.length; j++) {
        LOG.debug("split[" + j + "]= "+ splits[j]);
        RecordReader<LongWritable,Text> reader=format.getRecordReader(splits[j],job,reporter);
        try {
          int count=0;
          while (reader.next(key,value)) {
            int v=Integer.parseInt(value.toString());
            LOG.debug("read " + v);
            if (bits.get(v)) {
              LOG.warn("conflict with " + v + " in split "+ j+ " at position "+ reader.getPos());
            }
            assertFalse("Key in multiple partitions.",bits.get(v));
            bits.set(v);
            count++;
          }
          LOG.debug("splits[" + j + "]="+ splits[j]+ " count="+ count);
        }
  finally {
          reader.close();
        }
      }
      assertEquals("Some keys in no partition.",length,bits.cardinality());
    }
  }
}

</code></pre>

<br>
<pre class="type-10 type-11 type-4 type-5 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies values of objects/variables related to AUT calls
- Verifies whether objects/variable are equal to an expected value 
"></span><br>
/** 
 * Test readLine for various kinds of line termination sequneces.
 * Varies buffer size to stress test.  Also check that returned
 * value matches the string length.
 * @throws Exception
 */
@Test public void testNewLines() throws Exception {
  final String STR="a\nbb\n\nccc\rdddd\r\r\r\n\r\neeeee";
  final int STRLENBYTES=STR.getBytes().length;
  Text out=new Text();
  for (int bufsz=1; bufsz < STRLENBYTES + 1; ++bufsz) {
    LineReader in=makeStream(STR,bufsz);
    int c=0;
    c+=in.readLine(out);
    assertEquals("line1 length, bufsz:" + bufsz,1,out.getLength());
    c+=in.readLine(out);
    assertEquals("line2 length, bufsz:" + bufsz,2,out.getLength());
    c+=in.readLine(out);
    assertEquals("line3 length, bufsz:" + bufsz,0,out.getLength());
    c+=in.readLine(out);
    assertEquals("line4 length, bufsz:" + bufsz,3,out.getLength());
    c+=in.readLine(out);
    assertEquals("line5 length, bufsz:" + bufsz,4,out.getLength());
    c+=in.readLine(out);
    assertEquals("line6 length, bufsz:" + bufsz,0,out.getLength());
    c+=in.readLine(out);
    assertEquals("line7 length, bufsz:" + bufsz,0,out.getLength());
    c+=in.readLine(out);
    assertEquals("line8 length, bufsz:" + bufsz,0,out.getLength());
    c+=in.readLine(out);
    assertEquals("line9 length, bufsz:" + bufsz,5,out.getLength());
    assertEquals("end of file, bufsz: " + bufsz,0,in.readLine(out));
    assertEquals("total bytes, bufsz: " + bufsz,c,STRLENBYTES);
  }
}

</code></pre>

<br>
<pre class="type-10 type-11 type-4 type-5 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies values of objects/variables related to AUT calls
- Verifies whether objects/variable are equal to an expected value 
"></span><br>
/** 
 * Test readLine for correct interpretation of maxLineLength
 * (returned string should be clipped at maxLineLength, and the
 * remaining bytes on the same line should be thrown out).
 * Also check that returned value matches the string length.
 * Varies buffer size to stress test.
 * @throws Exception
 */
@Test public void testMaxLineLength() throws Exception {
  final String STR="a\nbb\n\nccc\rdddd\r\neeeee";
  final int STRLENBYTES=STR.getBytes().length;
  Text out=new Text();
  for (int bufsz=1; bufsz < STRLENBYTES + 1; ++bufsz) {
    LineReader in=makeStream(STR,bufsz);
    int c=0;
    c+=in.readLine(out,1);
    assertEquals("line1 length, bufsz: " + bufsz,1,out.getLength());
    c+=in.readLine(out,1);
    assertEquals("line2 length, bufsz: " + bufsz,1,out.getLength());
    c+=in.readLine(out,1);
    assertEquals("line3 length, bufsz: " + bufsz,0,out.getLength());
    c+=in.readLine(out,3);
    assertEquals("line4 length, bufsz: " + bufsz,3,out.getLength());
    c+=in.readLine(out,10);
    assertEquals("line5 length, bufsz: " + bufsz,4,out.getLength());
    c+=in.readLine(out,8);
    assertEquals("line5 length, bufsz: " + bufsz,5,out.getLength());
    assertEquals("end of file, bufsz: " + bufsz,0,in.readLine(out));
    assertEquals("total bytes, bufsz: " + bufsz,c,STRLENBYTES);
  }
}

</code></pre>

<br>
<pre class="type-10 type-11 type-4 type-7 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testSplitableCodecs() throws IOException {
  JobConf conf=new JobConf(defaultConf);
  int seed=new Random().nextInt();
  CompressionCodec codec=null;
  try {
    codec=(CompressionCodec)ReflectionUtils.newInstance(conf.getClassByName("org.apache.hadoop.io.compress.BZip2Codec"),conf);
  }
 catch (  ClassNotFoundException cnfe) {
    throw new IOException("Illegal codec!");
  }
  Path file=new Path(workDir,"test" + codec.getDefaultExtension());
  Reporter reporter=Reporter.NULL;
  LOG.info("seed = " + seed);
  Random random=new Random(seed);
  FileSystem localFs=FileSystem.getLocal(conf);
  localFs.delete(workDir,true);
  FileInputFormat.setInputPaths(conf,workDir);
  final int MAX_LENGTH=500000;
  for (int length=MAX_LENGTH / 2; length < MAX_LENGTH; length+=random.nextInt(MAX_LENGTH / 4) + 1) {
    LOG.info("creating; entries = " + length);
    Writer writer=new OutputStreamWriter(codec.createOutputStream(localFs.create(file)));
    try {
      for (int i=0; i < length; i++) {
        writer.write(Integer.toString(i));
        writer.write("\n");
      }
    }
  finally {
      writer.close();
    }
    TextInputFormat format=new TextInputFormat();
    format.configure(conf);
    LongWritable key=new LongWritable();
    Text value=new Text();
    for (int i=0; i < 3; i++) {
      int numSplits=random.nextInt(MAX_LENGTH / 2000) + 1;
      LOG.info("splitting: requesting = " + numSplits);
      InputSplit[] splits=format.getSplits(conf,numSplits);
      LOG.info("splitting: got =        " + splits.length);
      BitSet bits=new BitSet(length);
      for (int j=0; j < splits.length; j++) {
        LOG.debug("split[" + j + "]= "+ splits[j]);
        RecordReader<LongWritable,Text> reader=format.getRecordReader(splits[j],conf,reporter);
        try {
          int counter=0;
          while (reader.next(key,value)) {
            int v=Integer.parseInt(value.toString());
            LOG.debug("read " + v);
            if (bits.get(v)) {
              LOG.warn("conflict with " + v + " in split "+ j+ " at position "+ reader.getPos());
            }
            assertFalse("Key in multiple partitions.",bits.get(v));
            bits.set(v);
            counter++;
          }
          if (counter > 0) {
            LOG.info("splits[" + j + "]="+ splits[j]+ " count="+ counter);
          }
 else {
            LOG.debug("splits[" + j + "]="+ splits[j]+ " count="+ counter);
          }
        }
  finally {
          reader.close();
        }
      }
      assertEquals("Some keys in no partition.",length,bits.cardinality());
    }
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.gridmix.TestSleepJob </h4><pre class="type-10 type-11 type-4 type-5 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies values of objects/variables related to AUT calls
- Verifies whether objects/variable are equal to an expected value 
"></span><br>
@Test public void testMapTasksOnlySleepJobs() throws Exception {
  Configuration conf=new Configuration();
  conf.setBoolean(SleepJob.SLEEPJOB_MAPTASK_ONLY,true);
  DebugJobProducer jobProducer=new DebugJobProducer(5,conf);
  JobConf jconf=GridmixTestUtils.mrCluster.createJobConf(new JobConf(conf));
  UserGroupInformation ugi=UserGroupInformation.getLoginUser();
  JobStory story;
  int seq=1;
  while ((story=jobProducer.getNextJob()) != null) {
    GridmixJob gridmixJob=JobCreator.SLEEPJOB.createGridmixJob(jconf,0,story,new Path("ignored"),ugi,seq++);
    gridmixJob.buildSplits(null);
    Job job=gridmixJob.call();
    assertEquals(0,job.getNumReduceTasks());
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapreduce.TestCounters </h4><pre class="type-10 type-11 type-4 type-5 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies values of objects/variables related to AUT calls
- Verifies whether objects/variable are equal to an expected value 
"></span><br>
/** 
 * Verify counter value works
 */
@Test public void testCounterValue(){
  final int NUMBER_TESTS=100;
  final int NUMBER_INC=10;
  final Random rand=new Random();
  for (int i=0; i < NUMBER_TESTS; i++) {
    long initValue=rand.nextInt();
    long expectedValue=initValue;
    Counter counter=new Counter("foo","bar",expectedValue);
    assertEquals("Counter value is not initialized correctly",expectedValue,counter.getValue());
    for (int j=0; j < NUMBER_INC; j++) {
      int incValue=rand.nextInt();
      counter.increment(incValue);
      expectedValue+=incValue;
      assertEquals("Counter value is not incremented correctly",expectedValue,counter.getValue());
    }
    expectedValue=rand.nextInt();
    counter.setValue(expectedValue);
    assertEquals("Counter value is not set correctly",expectedValue,counter.getValue());
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapreduce.lib.input.TestMRKeyValueTextInputFormat </h4><pre class="type-10 type-11 type-4 type-7 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testFormat() throws Exception {
  Job job=Job.getInstance(new Configuration(defaultConf));
  Path file=new Path(workDir,"test.txt");
  int seed=new Random().nextInt();
  LOG.info("seed = " + seed);
  Random random=new Random(seed);
  localFs.delete(workDir,true);
  FileInputFormat.setInputPaths(job,workDir);
  final int MAX_LENGTH=10000;
  for (int length=0; length < MAX_LENGTH; length+=random.nextInt(MAX_LENGTH / 10) + 1) {
    LOG.debug("creating; entries = " + length);
    Writer writer=new OutputStreamWriter(localFs.create(file));
    try {
      for (int i=0; i < length; i++) {
        writer.write(Integer.toString(i * 2));
        writer.write("\t");
        writer.write(Integer.toString(i));
        writer.write("\n");
      }
    }
  finally {
      writer.close();
    }
    KeyValueTextInputFormat format=new KeyValueTextInputFormat();
    for (int i=0; i < 3; i++) {
      int numSplits=random.nextInt(MAX_LENGTH / 20) + 1;
      LOG.debug("splitting: requesting = " + numSplits);
      List<InputSplit> splits=format.getSplits(job);
      LOG.debug("splitting: got =        " + splits.size());
      BitSet bits=new BitSet(length);
      for (int j=0; j < splits.size(); j++) {
        LOG.debug("split[" + j + "]= "+ splits.get(j));
        TaskAttemptContext context=MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());
        RecordReader<Text,Text> reader=format.createRecordReader(splits.get(j),context);
        Class<?> clazz=reader.getClass();
        assertEquals("reader class is KeyValueLineRecordReader.",KeyValueLineRecordReader.class,clazz);
        MapContext<Text,Text,Text,Text> mcontext=new MapContextImpl<Text,Text,Text,Text>(job.getConfiguration(),context.getTaskAttemptID(),reader,null,null,MapReduceTestUtil.createDummyReporter(),splits.get(j));
        reader.initialize(splits.get(j),mcontext);
        Text key=null;
        Text value=null;
        try {
          int count=0;
          while (reader.nextKeyValue()) {
            key=reader.getCurrentKey();
            clazz=key.getClass();
            assertEquals("Key class is Text.",Text.class,clazz);
            value=reader.getCurrentValue();
            clazz=value.getClass();
            assertEquals("Value class is Text.",Text.class,clazz);
            final int k=Integer.parseInt(key.toString());
            final int v=Integer.parseInt(value.toString());
            assertEquals("Bad key",0,k % 2);
            assertEquals("Mismatched key/value",k / 2,v);
            LOG.debug("read " + v);
            assertFalse("Key in multiple partitions.",bits.get(v));
            bits.set(v);
            count++;
          }
          LOG.debug("splits[" + j + "]="+ splits.get(j)+ " count="+ count);
        }
  finally {
          reader.close();
        }
      }
      assertEquals("Some keys in no partition.",length,bits.cardinality());
    }
  }
}

</code></pre>

<br>
<pre class="type-10 type-11 type-4 type-7 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testSplitableCodecs() throws Exception {
  final Job job=Job.getInstance(defaultConf);
  final Configuration conf=job.getConfiguration();
  CompressionCodec codec=null;
  try {
    codec=(CompressionCodec)ReflectionUtils.newInstance(conf.getClassByName("org.apache.hadoop.io.compress.BZip2Codec"),conf);
  }
 catch (  ClassNotFoundException cnfe) {
    throw new IOException("Illegal codec!");
  }
  Path file=new Path(workDir,"test" + codec.getDefaultExtension());
  int seed=new Random().nextInt();
  LOG.info("seed = " + seed);
  Random random=new Random(seed);
  localFs.delete(workDir,true);
  FileInputFormat.setInputPaths(job,workDir);
  final int MAX_LENGTH=500000;
  FileInputFormat.setMaxInputSplitSize(job,MAX_LENGTH / 20);
  for (int length=0; length < MAX_LENGTH; length+=random.nextInt(MAX_LENGTH / 4) + 1) {
    LOG.info("creating; entries = " + length);
    Writer writer=new OutputStreamWriter(codec.createOutputStream(localFs.create(file)));
    try {
      for (int i=0; i < length; i++) {
        writer.write(Integer.toString(i * 2));
        writer.write("\t");
        writer.write(Integer.toString(i));
        writer.write("\n");
      }
    }
  finally {
      writer.close();
    }
    KeyValueTextInputFormat format=new KeyValueTextInputFormat();
    assertTrue("KVTIF claims not splittable",format.isSplitable(job,file));
    for (int i=0; i < 3; i++) {
      int numSplits=random.nextInt(MAX_LENGTH / 2000) + 1;
      LOG.info("splitting: requesting = " + numSplits);
      List<InputSplit> splits=format.getSplits(job);
      LOG.info("splitting: got =        " + splits.size());
      BitSet bits=new BitSet(length);
      for (int j=0; j < splits.size(); j++) {
        LOG.debug("split[" + j + "]= "+ splits.get(j));
        TaskAttemptContext context=MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());
        RecordReader<Text,Text> reader=format.createRecordReader(splits.get(j),context);
        Class<?> clazz=reader.getClass();
        MapContext<Text,Text,Text,Text> mcontext=new MapContextImpl<Text,Text,Text,Text>(job.getConfiguration(),context.getTaskAttemptID(),reader,null,null,MapReduceTestUtil.createDummyReporter(),splits.get(j));
        reader.initialize(splits.get(j),mcontext);
        Text key=null;
        Text value=null;
        try {
          int count=0;
          while (reader.nextKeyValue()) {
            key=reader.getCurrentKey();
            value=reader.getCurrentValue();
            final int k=Integer.parseInt(key.toString());
            final int v=Integer.parseInt(value.toString());
            assertEquals("Bad key",0,k % 2);
            assertEquals("Mismatched key/value",k / 2,v);
            LOG.debug("read " + k + ","+ v);
            assertFalse(k + "," + v+ " in multiple partitions.",bits.get(v));
            bits.set(v);
            count++;
          }
          if (count > 0) {
            LOG.info("splits[" + j + "]="+ splits.get(j)+ " count="+ count);
          }
 else {
            LOG.debug("splits[" + j + "]="+ splits.get(j)+ " count="+ count);
          }
        }
  finally {
          reader.close();
        }
      }
      assertEquals("Some keys in no partition.",length,bits.cardinality());
    }
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapreduce.lib.partition.TestInputSampler </h4><pre class="type-10 type-11 type-5 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies whether objects/variable are equal to an expected value 
"></span><br>
/** 
 * Verify IntervalSampler contract, that samples are taken at regular
 * intervals from the given splits.
 */
@Test @SuppressWarnings("unchecked") public void testIntervalSampler() throws Exception {
  final int TOT_SPLITS=16;
  final int PER_SPLIT_SAMPLE=4;
  final int NUM_SAMPLES=TOT_SPLITS * PER_SPLIT_SAMPLE;
  final double FREQ=1.0 / TOT_SPLITS;
  InputSampler.Sampler<IntWritable,NullWritable> sampler=new InputSampler.IntervalSampler<IntWritable,NullWritable>(FREQ,NUM_SAMPLES);
  int inits[]=new int[TOT_SPLITS];
  for (int i=0; i < TOT_SPLITS; ++i) {
    inits[i]=i;
  }
  Job ignored=Job.getInstance();
  Object[] samples=sampler.getSample(new TestInputSamplerIF(NUM_SAMPLES,TOT_SPLITS,inits),ignored);
  assertEquals(NUM_SAMPLES,samples.length);
  Arrays.sort(samples,new IntWritable.Comparator());
  for (int i=0; i < NUM_SAMPLES; ++i) {
    assertEquals(i,((IntWritable)samples[i]).get());
  }
}

</code></pre>

<br>
<pre class="type-10 type-11 type-5 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies whether objects/variable are equal to an expected value 
"></span><br>
/** 
 * Verify SplitSampler contract, that an equal number of records are taken
 * from the first splits.
 */
@Test @SuppressWarnings("unchecked") public void testSplitSampler() throws Exception {
  final int TOT_SPLITS=15;
  final int NUM_SPLITS=5;
  final int STEP_SAMPLE=5;
  final int NUM_SAMPLES=NUM_SPLITS * STEP_SAMPLE;
  InputSampler.Sampler<IntWritable,NullWritable> sampler=new InputSampler.SplitSampler<IntWritable,NullWritable>(NUM_SAMPLES,NUM_SPLITS);
  int inits[]=new int[TOT_SPLITS];
  for (int i=0; i < TOT_SPLITS; ++i) {
    inits[i]=i * STEP_SAMPLE;
  }
  Job ignored=Job.getInstance();
  Object[] samples=sampler.getSample(new TestInputSamplerIF(100000,TOT_SPLITS,inits),ignored);
  assertEquals(NUM_SAMPLES,samples.length);
  Arrays.sort(samples,new IntWritable.Comparator());
  for (int i=0; i < NUM_SAMPLES; ++i) {
    assertEquals(i,((IntWritable)samples[i]).get());
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.raid.TestBlockFixerDistConcurrency </h4><pre class="type-10 type-11 type-4 type-7 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
/** 
 * tests that the distributed block fixer obeys
 * the limit on how many files to fix simultaneously
 */
@Test public void testMaxPendingFiles() throws Exception {
  LOG.info("Test testMaxPendingFiles started.");
  long blockSize=8192L;
  int stripeLength=3;
  mySetup(stripeLength,-1);
  Path file1=new Path("/user/dhruba/raidtest/file1");
  Path file2=new Path("/user/dhruba/raidtest/file2");
  Path destPath=new Path("/destraid/user/dhruba/raidtest");
  long crc1=TestRaidDfs.createTestFilePartialLastBlock(fileSys,file1,1,20,blockSize);
  long crc2=TestRaidDfs.createTestFilePartialLastBlock(fileSys,file2,1,20,blockSize);
  long file1Len=fileSys.getFileStatus(file1).getLen();
  long file2Len=fileSys.getFileStatus(file2).getLen();
  LOG.info("Test testMaxPendingFiles created test files");
  Configuration localConf=new Configuration(conf);
  localConf.set(RaidNode.RAID_LOCATION_KEY,"/destraid");
  localConf.setInt("raid.blockfix.interval",1000);
  localConf.set("raid.blockfix.classname","org.apache.hadoop.raid.DistBlockFixer");
  localConf.setLong("raid.blockfix.filespertask",2L);
  localConf.setLong("raid.blockfix.maxpendingfiles",1L);
  try {
    cnode=RaidNode.createRaidNode(null,localConf);
    TestRaidDfs.waitForFileRaided(LOG,fileSys,file1,destPath);
    TestRaidDfs.waitForFileRaided(LOG,fileSys,file2,destPath);
    cnode.stop();
    cnode.join();
    FileStatus file1Stat=fileSys.getFileStatus(file1);
    FileStatus file2Stat=fileSys.getFileStatus(file2);
    DistributedFileSystem dfs=(DistributedFileSystem)fileSys;
    LocatedBlocks file1Loc=RaidDFSUtil.getBlockLocations(dfs,file1.toUri().getPath(),0,file1Stat.getLen());
    LocatedBlocks file2Loc=RaidDFSUtil.getBlockLocations(dfs,file2.toUri().getPath(),0,file2Stat.getLen());
    String[] corruptFiles=RaidDFSUtil.getCorruptFiles(dfs);
    assertEquals("no corrupt files expected",0,corruptFiles.length);
    assertEquals("filesFixed() should return 0 before fixing files",0,cnode.blockFixer.filesFixed());
    int[] corruptBlockIdxs=new int[]{0,4,6};
    for (    int idx : corruptBlockIdxs)     corruptBlock(file1Loc.get(idx).getBlock());
    reportCorruptBlocks(dfs,file1,corruptBlockIdxs,blockSize);
    corruptFiles=RaidDFSUtil.getCorruptFiles(dfs);
    cnode=RaidNode.createRaidNode(null,localConf);
    DistBlockFixer blockFixer=(DistBlockFixer)cnode.blockFixer;
    long start=System.currentTimeMillis();
    while (blockFixer.jobsRunning() < 1 && System.currentTimeMillis() - start < 240000) {
      LOG.info("Test testBlockFix waiting for fixing job 1 to start");
      Thread.sleep(10);
    }
    assertEquals("job not running",1,blockFixer.jobsRunning());
    for (    int idx : corruptBlockIdxs)     corruptBlock(file2Loc.get(idx).getBlock());
    reportCorruptBlocks(dfs,file2,corruptBlockIdxs,blockSize);
    corruptFiles=RaidDFSUtil.getCorruptFiles(dfs);
    while (blockFixer.filesFixed() < 2 && System.currentTimeMillis() - start < 240000) {
      assertTrue("too many jobs running",blockFixer.jobsRunning() <= 1);
      Thread.sleep(10);
    }
    assertEquals("files not fixed",2,blockFixer.filesFixed());
    dfs=getDFS(conf,dfs);
    try {
      Thread.sleep(5 * 1000);
    }
 catch (    InterruptedException ignore) {
    }
    assertTrue("file not fixed",TestRaidDfs.validateFile(dfs,file1,file1Len,crc1));
    assertTrue("file not fixed",TestRaidDfs.validateFile(dfs,file2,file2Len,crc2));
  }
 catch (  Exception e) {
    LOG.info("Test testMaxPendingFiles exception " + e + StringUtils.stringifyException(e));
    throw e;
  }
 finally {
    myTearDown();
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.security.TestMapredGroupMappingServiceRefresh </h4><pre class="type-10 type-11 type-4 type-7 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testGroupMappingRefresh() throws Exception {
  MRAdmin admin=new MRAdmin(config);
  String[] args=new String[]{"-refreshUserToGroupsMappings"};
  Groups groups=Groups.getUserToGroupsMappingService(config);
  String user=UserGroupInformation.getLoginUser().getShortUserName();
  System.out.println("first attempt:");
  List<String> g1=groups.getGroups(user);
  String[] str_groups=new String[g1.size()];
  g1.toArray(str_groups);
  System.out.println(Arrays.toString(str_groups));
  System.out.println("second attempt, should be same:");
  List<String> g2=groups.getGroups(user);
  g2.toArray(str_groups);
  System.out.println(Arrays.toString(str_groups));
  for (int i=0; i < g2.size(); i++) {
    assertEquals("Should be same group ",g1.get(i),g2.get(i));
  }
  admin.run(args);
  System.out.println("third attempt(after refresh command), should be different:");
  List<String> g3=groups.getGroups(user);
  g3.toArray(str_groups);
  System.out.println(Arrays.toString(str_groups));
  for (int i=0; i < g3.size(); i++) {
    assertFalse("Should be different group ",g1.get(i).equals(g3.get(i)));
  }
  System.out.println("");
  Thread.sleep(groupRefreshTimeoutSec * 1100);
  System.out.println("fourth attempt(after timeout), should be different:");
  List<String> g4=groups.getGroups(user);
  g4.toArray(str_groups);
  System.out.println(Arrays.toString(str_groups));
  for (int i=0; i < g4.size(); i++) {
    assertFalse("Should be different group ",g3.get(i).equals(g4.get(i)));
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.streaming.TestAutoInputFormat </h4><pre class="type-11 type-13 type-4 type-7 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions inside branch conditions">BranchVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies assertions in iterations
- Verifies assertions inside branch conditions
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@SuppressWarnings({"unchecked","deprecation"}) @Test public void testFormat() throws IOException {
  JobConf job=new JobConf(conf);
  FileSystem fs=FileSystem.getLocal(conf);
  Path dir=new Path(System.getProperty("test.build.data",".") + "/mapred");
  Path txtFile=new Path(dir,"auto.txt");
  Path seqFile=new Path(dir,"auto.seq");
  fs.delete(dir,true);
  FileInputFormat.setInputPaths(job,dir);
  Writer txtWriter=new OutputStreamWriter(fs.create(txtFile));
  try {
    for (int i=0; i < LINES_COUNT; i++) {
      txtWriter.write("" + (10 * i));
      txtWriter.write("\n");
    }
  }
  finally {
    txtWriter.close();
  }
  SequenceFile.Writer seqWriter=SequenceFile.createWriter(fs,conf,seqFile,IntWritable.class,LongWritable.class);
  try {
    for (int i=0; i < RECORDS_COUNT; i++) {
      IntWritable key=new IntWritable(11 * i);
      LongWritable value=new LongWritable(12 * i);
      seqWriter.append(key,value);
    }
  }
  finally {
    seqWriter.close();
  }
  AutoInputFormat format=new AutoInputFormat();
  InputSplit[] splits=format.getSplits(job,SPLITS_COUNT);
  for (  InputSplit split : splits) {
    RecordReader reader=format.getRecordReader(split,job,Reporter.NULL);
    Object key=reader.createKey();
    Object value=reader.createValue();
    try {
      while (reader.next(key,value)) {
        if (key instanceof LongWritable) {
          assertEquals("Wrong value class.",Text.class,value.getClass());
          assertTrue("Invalid value",Integer.parseInt(((Text)value).toString()) % 10 == 0);
        }
 else {
          assertEquals("Wrong key class.",IntWritable.class,key.getClass());
          assertEquals("Wrong value class.",LongWritable.class,value.getClass());
          assertTrue("Invalid key.",((IntWritable)key).get() % 11 == 0);
          assertTrue("Invalid value.",((LongWritable)value).get() % 12 == 0);
        }
      }
    }
  finally {
      reader.close();
    }
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.streaming.TestDumpTypedBytes </h4><pre class="type-10 type-11 type-4 type-7 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testDumping() throws Exception {
  Configuration conf=new Configuration();
  MiniDFSCluster cluster=new MiniDFSCluster(conf,2,true,null);
  FileSystem fs=cluster.getFileSystem();
  PrintStream psBackup=System.out;
  ByteArrayOutputStream out=new ByteArrayOutputStream();
  PrintStream psOut=new PrintStream(out);
  System.setOut(psOut);
  DumpTypedBytes dumptb=new DumpTypedBytes(conf);
  try {
    Path root=new Path("/typedbytestest");
    assertTrue(fs.mkdirs(root));
    assertTrue(fs.exists(root));
    OutputStreamWriter writer=new OutputStreamWriter(fs.create(new Path(root,"test.txt")));
    try {
      for (int i=0; i < 100; i++) {
        writer.write("" + (10 * i) + "\n");
      }
    }
  finally {
      writer.close();
    }
    String[] args=new String[1];
    args[0]="/typedbytestest";
    int ret=dumptb.run(args);
    assertEquals("Return value != 0.",0,ret);
    ByteArrayInputStream in=new ByteArrayInputStream(out.toByteArray());
    TypedBytesInput tbinput=new TypedBytesInput(new DataInputStream(in));
    int counter=0;
    Object key=tbinput.read();
    while (key != null) {
      assertEquals(Long.class,key.getClass());
      Object value=tbinput.read();
      assertEquals(String.class,value.getClass());
      assertTrue("Invalid output.",Integer.parseInt(value.toString()) % 10 == 0);
      counter++;
      key=tbinput.read();
    }
    assertEquals("Wrong number of outputs.",100,counter);
  }
  finally {
    try {
      fs.close();
    }
 catch (    Exception e) {
    }
    System.setOut(psBackup);
    cluster.shutdown();
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.streaming.TestLoadTypedBytes </h4><pre class="type-11 type-4 type-7 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies assertions in iterations
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testLoading() throws Exception {
  Configuration conf=new Configuration();
  MiniDFSCluster cluster=new MiniDFSCluster(conf,2,true,null);
  FileSystem fs=cluster.getFileSystem();
  ByteArrayOutputStream out=new ByteArrayOutputStream();
  TypedBytesOutput tboutput=new TypedBytesOutput(new DataOutputStream(out));
  for (int i=0; i < 100; i++) {
    tboutput.write(new Long(i));
    tboutput.write("" + (10 * i));
  }
  InputStream isBackup=System.in;
  ByteArrayInputStream in=new ByteArrayInputStream(out.toByteArray());
  System.setIn(in);
  LoadTypedBytes loadtb=new LoadTypedBytes(conf);
  try {
    Path root=new Path("/typedbytestest");
    assertTrue(fs.mkdirs(root));
    assertTrue(fs.exists(root));
    String[] args=new String[1];
    args[0]="/typedbytestest/test.seq";
    int ret=loadtb.run(args);
    assertEquals("Return value != 0.",0,ret);
    Path file=new Path(root,"test.seq");
    assertTrue(fs.exists(file));
    SequenceFile.Reader reader=new SequenceFile.Reader(fs,file,conf);
    int counter=0;
    TypedBytesWritable key=new TypedBytesWritable();
    TypedBytesWritable value=new TypedBytesWritable();
    while (reader.next(key,value)) {
      assertEquals(Long.class,key.getValue().getClass());
      assertEquals(String.class,value.getValue().getClass());
      assertTrue("Invalid record.",Integer.parseInt(value.toString()) % 10 == 0);
      counter++;
    }
    assertEquals("Wrong number of records.",100,counter);
  }
  finally {
    try {
      fs.close();
    }
 catch (    Exception e) {
    }
    System.setIn(isBackup);
    cluster.shutdown();
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.tools.rumen.TestHistograms </h4><pre class="type-10 type-11 type-13 type-14 type-4 type-7 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions inside branch conditions">BranchVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies (un)successful execution of the test case by reporting explicitly a failure">UtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies assertions inside branch conditions
- Verifies (un)successful execution of the test case by reporting explicitly a failure
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Contains more than 2 JUnit-based stereotypes
"></span><br>
/** 
 * @throws IOExceptionThere should be files in the directory named by
 * ${test.build.data}/rumen/histogram-test .
 * There will be pairs of files, inputXxx.json and goldXxx.json .
 * We read the input file as a HistogramRawTestData in json. Then we
 * create a Histogram using the data field, and then a
 * LoggedDiscreteCDF using the percentiles and scale field. Finally,
 * we read the corresponding goldXxx.json as a LoggedDiscreteCDF and
 * deepCompare them.
 */
@Test public void testHistograms() throws IOException {
  final Configuration conf=new Configuration();
  final FileSystem lfs=FileSystem.getLocal(conf);
  final Path rootInputDir=new Path(System.getProperty("test.tools.input.dir","")).makeQualified(lfs);
  final Path rootInputFile=new Path(rootInputDir,"rumen/histogram-tests");
  FileStatus[] tests=lfs.listStatus(rootInputFile);
  for (int i=0; i < tests.length; ++i) {
    Path filePath=tests[i].getPath();
    String fileName=filePath.getName();
    if (fileName.startsWith("input")) {
      String testName=fileName.substring("input".length());
      Path goldFilePath=new Path(rootInputFile,"gold" + testName);
      assertTrue("Gold file dies not exist",lfs.exists(goldFilePath));
      LoggedDiscreteCDF newResult=histogramFileToCDF(filePath,lfs);
      System.out.println("Testing a Histogram for " + fileName);
      FSDataInputStream goldStream=lfs.open(goldFilePath);
      JsonObjectMapperParser<LoggedDiscreteCDF> parser=new JsonObjectMapperParser<LoggedDiscreteCDF>(goldStream,LoggedDiscreteCDF.class);
      try {
        LoggedDiscreteCDF dcdf=parser.getNext();
        dcdf.deepCompare(newResult,new TreePath(null,"<root>"));
      }
 catch (      DeepInequalityException e) {
        fail(e.path.toString());
      }
 finally {
        parser.close();
      }
    }
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.typedbytes.TestIO </h4><pre class="type-10 type-11 type-4 type-5 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies values of objects/variables related to AUT calls
- Verifies whether objects/variable are equal to an expected value 
"></span><br>
@Test public void testWritableIO() throws IOException {
  Writable[] vectorValues=new Writable[]{new Text("test1"),new Text("test2"),new Text("test3")};
  ArrayWritable vector=new ArrayWritable(Text.class,vectorValues);
  MapWritable map=new MapWritable();
  map.put(new Text("one"),new VIntWritable(1));
  map.put(new Text("two"),new VLongWritable(2));
  Writable[] writables=new Writable[]{new BytesWritable(new byte[]{1,2,3,4}),new ByteWritable((byte)123),new BooleanWritable(true),new VIntWritable(12345),new VLongWritable(123456789L),new FloatWritable((float)1.2),new DoubleWritable(1.234),new Text("random string")};
  TypedBytesWritable tbw=new TypedBytesWritable();
  tbw.setValue("typed bytes text");
  RecRecord1 r1=new RecRecord1();
  r1.setBoolVal(true);
  r1.setByteVal((byte)0x66);
  r1.setFloatVal(3.145F);
  r1.setDoubleVal(1.5234);
  r1.setIntVal(-4567);
  r1.setLongVal(-2367L);
  r1.setStringVal("random text");
  r1.setBufferVal(new Buffer());
  r1.setVectorVal(new ArrayList<String>());
  r1.setMapVal(new TreeMap<String,String>());
  RecRecord0 r0=new RecRecord0();
  r0.setStringVal("other random text");
  r1.setRecordVal(r0);
  FileOutputStream ostream=new FileOutputStream(tmpfile);
  DataOutputStream dostream=new DataOutputStream(ostream);
  TypedBytesWritableOutput out=new TypedBytesWritableOutput(dostream);
  for (  Writable w : writables) {
    out.write(w);
  }
  out.write(tbw);
  out.write(vector);
  out.write(map);
  out.write(r1);
  dostream.close();
  ostream.close();
  FileInputStream istream=new FileInputStream(tmpfile);
  DataInputStream distream=new DataInputStream(istream);
  TypedBytesWritableInput in=new TypedBytesWritableInput(distream);
  for (  Writable w : writables) {
    assertEquals(w,in.read());
  }
  assertEquals(tbw.getValue().toString(),in.read().toString());
  assertEquals(ArrayWritable.class,in.readType());
  ArrayWritable aw=in.readArray();
  Writable[] writables1=vector.get(), writables2=aw.get();
  assertEquals(writables1.length,writables2.length);
  for (int i=0; i < writables1.length; i++) {
    assertEquals(((Text)writables1[i]).toString(),((TypedBytesWritable)writables2[i]).getValue());
  }
  assertEquals(MapWritable.class,in.readType());
  MapWritable mw=in.readMap();
  assertEquals(map.entrySet(),mw.entrySet());
  assertEquals(Type.LIST,TypedBytesInput.get(distream).readType());
  assertEquals(r1.getBoolVal(),TypedBytesInput.get(distream).read());
  assertEquals(r1.getByteVal(),TypedBytesInput.get(distream).read());
  assertEquals(r1.getIntVal(),TypedBytesInput.get(distream).read());
  assertEquals(r1.getLongVal(),TypedBytesInput.get(distream).read());
  assertEquals(r1.getFloatVal(),TypedBytesInput.get(distream).read());
  assertEquals(r1.getDoubleVal(),TypedBytesInput.get(distream).read());
  assertEquals(r1.getStringVal(),TypedBytesInput.get(distream).read());
  Object prevObj=null, obj=TypedBytesInput.get(distream).read();
  while (obj != null) {
    prevObj=obj;
    obj=TypedBytesInput.get(distream).read();
  }
  List recList=(List)prevObj;
  assertEquals(r0.getStringVal(),recList.get(0));
  distream.close();
  istream.close();
}

</code></pre>

<br>
<script>$(document).ready(function() {
  $('pre code').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});</script>
