<h3><span class=" glyphicon glyphicon-tag"/>&nbspNullVerifier</h3><kbd>Verifies whether objects are null</kbd><br><br><br><h4 style="margin:0px">Class: org.apache.hadoop.conf.TestJobConf </h4><pre class="type-4 type-7 type-6 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects are null
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testProfileParamsDefaults(){
  JobConf configuration=new JobConf();
  Assert.assertNull(configuration.get(MRJobConfig.TASK_PROFILE_PARAMS));
  String result=configuration.getProfileParams();
  Assert.assertNotNull(result);
  Assert.assertTrue(result.contains("file=%s"));
  Assert.assertTrue(result.startsWith("-agentlib:hprof"));
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestCluster </h4><pre class="type-4 type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to AUT calls
- Verifies whether objects are null
"></span><br>
@Test public void testProcessInfo() throws Exception {
  LOG.info("Process info of JobTracker is : " + cluster.getJTClient().getProcessInfo());
  Assert.assertNotNull(cluster.getJTClient().getProcessInfo());
  Collection<TTClient> tts=cluster.getTTClients();
  for (  TTClient tt : tts) {
    LOG.info("Process info of TaskTracker is : " + tt.getProcessInfo());
    Assert.assertNotNull(tt.getProcessInfo());
  }
}

</code></pre>

<br>
<pre class="type-10 type-11 type-13 type-14 type-4 type-7 type-6 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions inside branch conditions">BranchVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies (un)successful execution of the test case by reporting explicitly a failure">UtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies assertions inside branch conditions
- Verifies (un)successful execution of the test case by reporting explicitly a failure
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects are null
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
/** 
 * Test to verify the common properties of tasks.
 * @throws Exception
 */
@Test public void testTaskDetails() throws Exception {
  Configuration conf=new Configuration(cluster.getConf());
  JTProtocol wovenClient=cluster.getJTClient().getProxy();
  FinishTaskControlAction.configureControlActionForJob(conf);
  SleepJob job=new SleepJob();
  job.setConf(conf);
  Job rJob=job.createJob(1,1,100,100,100,100);
  JobClient client=cluster.getJTClient().getClient();
  rJob.submit();
  RunningJob rJob1=client.getJob(org.apache.hadoop.mapred.JobID.downgrade(rJob.getJobID()));
  JobID id=rJob.getJobID();
  JobInfo jInfo=wovenClient.getJobInfo(id);
  while (jInfo.getStatus().getRunState() != JobStatus.RUNNING) {
    Thread.sleep(1000);
    jInfo=wovenClient.getJobInfo(id);
  }
  LOG.info("Waiting till job starts running one map");
  TaskInfo[] myTaskInfos=wovenClient.getTaskInfo(id);
  boolean isOneTaskStored=false;
  String sometaskpid=null;
  org.apache.hadoop.mapreduce.TaskAttemptID sometaskId=null;
  TTClient myCli=null;
  for (  TaskInfo info : myTaskInfos) {
    if (!info.isSetupOrCleanup()) {
      String[] taskTrackers=info.getTaskTrackers();
      for (      String taskTracker : taskTrackers) {
        TTInfo ttInfo=wovenClient.getTTInfo(taskTracker);
        TTClient ttCli=cluster.getTTClient(ttInfo.getStatus().getHost());
        TaskID taskId=info.getTaskID();
        TTTaskInfo ttTaskInfo=ttCli.getProxy().getTask(taskId);
        Assert.assertNotNull(ttTaskInfo);
        Assert.assertNotNull(ttTaskInfo.getConf());
        Assert.assertNotNull(ttTaskInfo.getUser());
        Assert.assertTrue(ttTaskInfo.getTaskStatus().getProgress() >= 0.0);
        Assert.assertTrue(ttTaskInfo.getTaskStatus().getProgress() <= 1.0);
        String pid=ttTaskInfo.getPid();
        int i=1;
        while (pid.isEmpty()) {
          Thread.sleep(1000);
          LOG.info("Waiting for task to report its pid back");
          ttTaskInfo=ttCli.getProxy().getTask(taskId);
          pid=ttTaskInfo.getPid();
          if (i == 40) {
            Assert.fail("The task pid not reported for 40 seconds.");
          }
          i++;
        }
        if (!isOneTaskStored) {
          sometaskpid=pid;
          sometaskId=ttTaskInfo.getTaskStatus().getTaskID();
          myCli=ttCli;
          isOneTaskStored=true;
        }
        LOG.info("verified task progress to be between 0 and 1");
        State state=ttTaskInfo.getTaskStatus().getRunState();
        if (ttTaskInfo.getTaskStatus().getProgress() < 1.0 && ttTaskInfo.getTaskStatus().getProgress() > 0.0) {
          Assert.assertEquals(TaskStatus.State.RUNNING,state);
          LOG.info("verified run state as " + state);
        }
        FinishTaskControlAction action=new FinishTaskControlAction(org.apache.hadoop.mapred.TaskID.downgrade(info.getTaskID()));
        ttCli.getProxy().sendAction(action);
      }
    }
  }
  rJob.killJob();
  int i=1;
  while (!rJob.isComplete()) {
    Thread.sleep(1000);
    if (i == 40) {
      Assert.fail("The job not completed within 40 seconds after killing it.");
    }
    i++;
  }
  TTTaskInfo myTaskInfo=myCli.getProxy().getTask(sometaskId.getTaskID());
  i=0;
  while (myTaskInfo != null && !myTaskInfo.getPid().isEmpty()) {
    LOG.info("sleeping till task is retired from TT memory");
    Thread.sleep(1000);
    myTaskInfo=myCli.getProxy().getTask(sometaskId.getTaskID());
    if (i == 40) {
      Assert.fail("Task not retired from TT memory within 40 seconds of job completeing");
    }
    i++;
  }
  Assert.assertFalse(myCli.getProxy().isProcessTreeAlive(sometaskpid));
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestDistributedCacheModifiedFile </h4><pre class="type-10 type-11 type-13 type-14 type-4 type-7 type-6 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions inside branch conditions">BranchVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies (un)successful execution of the test case by reporting explicitly a failure">UtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies assertions inside branch conditions
- Verifies (un)successful execution of the test case by reporting explicitly a failure
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects are null
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testDistributedCache() throws Exception {
  Configuration conf=new Configuration(cluster.getConf());
  JTProtocol wovenClient=cluster.getJTClient().getProxy();
  int count=0;
  boolean continueLoop=true;
  int countLoop=0;
  int taskTrackerCounter=0;
  ArrayList<String> taskTrackerCollection=new ArrayList<String>();
  boolean taskTrackerFound=false;
  do {
    SleepJob job=new SleepJob();
    job.setConf(conf);
    Job slpJob=job.createJob(5,1,1000,1000,100,100);
    String input="This will be the content of\n" + "distributed cache\n";
    DataOutputStream file=UtilsForTests.createTmpFileDFS(dfs,URIPATH,permission,input);
    DistributedCache.createSymlink(conf);
    URI uri=URI.create(uriPath);
    DistributedCache.addCacheFile(uri,conf);
    JobConf jconf=new JobConf(conf);
    FinishTaskControlAction.configureControlActionForJob(conf);
    slpJob.submit();
    RunningJob rJob=cluster.getJTClient().getClient().getJob(org.apache.hadoop.mapred.JobID.downgrade(slpJob.getJobID()));
    countLoop++;
    TTClient tClient=null;
    JobInfo jInfo=wovenClient.getJobInfo(rJob.getID());
    LOG.info("jInfo is :" + jInfo);
    Assert.assertNotNull("jobInfo is null",jInfo);
    count=0;
    while (jInfo.getStatus().getRunState() != JobStatus.RUNNING) {
      UtilsForTests.waitFor(10000);
      count++;
      jInfo=wovenClient.getJobInfo(rJob.getID());
      if (count > 10) {
        Assert.fail("job has not reached running state for more than" + "100 seconds. Failing at this point");
      }
    }
    LOG.info("job id is :" + rJob.getID().toString());
    TaskInfo[] taskInfos=cluster.getJTClient().getProxy().getTaskInfo(rJob.getID());
    boolean distCacheFileIsFound;
    for (    TaskInfo taskInfo : taskInfos) {
      distCacheFileIsFound=false;
      String[] taskTrackers=taskInfo.getTaskTrackers();
      for (      String taskTracker : taskTrackers) {
        taskTracker=UtilsForTests.getFQDNofTT(taskTracker);
        LOG.info("taskTracker is :" + taskTracker);
        taskTrackerFound=false;
        if (countLoop > 1) {
          if (taskTracker != null) {
            continueLoop=taskTrackerCollection.contains(taskTracker);
          }
          if (continueLoop) {
            taskTrackerFound=true;
          }
        }
        if (taskTracker != null)         taskTrackerCollection.add(taskTracker);
        if (countLoop > 1) {
          continueLoop=false;
        }
        tClient=cluster.getTTClient(taskTracker);
        if (tClient == null) {
          continue;
        }
        String[] localDirs=tClient.getMapredLocalDirs();
        int distributedFileCount=0;
        for (        String localDir : localDirs) {
          localDir=localDir + Path.SEPARATOR + TaskTracker.getPublicDistributedCacheDir();
          LOG.info("localDir is : " + localDir);
          FileStatus[] fileStatuses=tClient.listStatus(localDir,true,true);
          for (          FileStatus fileStatus : fileStatuses) {
            Path path=fileStatus.getPath();
            LOG.info("path is :" + path.toString());
            distCacheFileIsFound=(path.toString()).endsWith(distributedFileName);
            if (distCacheFileIsFound) {
              LOG.info("PATH found is :" + path.toString());
              distributedFileCount++;
              String filename=path.getName();
              FsPermission fsPerm=fileStatus.getPermission();
              Assert.assertTrue("File Permission is not 777",fsPerm.equals(new FsPermission("777")));
            }
          }
        }
        LOG.debug("The distributed FileCount is :" + distributedFileCount);
        LOG.debug("The taskTrackerFound is :" + taskTrackerFound);
        if (distributedFileCount != 2 && taskTrackerFound) {
          Assert.fail("The distributed cache file has to be two. " + "But found was " + distributedFileCount);
        }
 else         if (distributedFileCount > 1 && !taskTrackerFound) {
          Assert.fail("The distributed cache file cannot more than one." + " But found was " + distributedFileCount);
        }
 else         if (distributedFileCount < 1)         Assert.fail("The distributed cache file is less than one. " + "But found was " + distributedFileCount);
        if (!distCacheFileIsFound) {
          Assert.assertEquals("The distributed cache file does not exist",distCacheFileIsFound,false);
        }
      }
    }
    for (    TaskInfo taskInfoRemaining : taskInfos) {
      FinishTaskControlAction action=new FinishTaskControlAction(TaskID.downgrade(taskInfoRemaining.getTaskID()));
      Collection<TTClient> tts=cluster.getTTClients();
      for (      TTClient cli : tts) {
        cli.getProxy().sendAction(action);
      }
    }
    rJob.killJob();
    Thread.sleep(3000);
    TaskInfo[] myTaskInfos=wovenClient.getTaskInfo(rJob.getID());
    if (myTaskInfos != null) {
      for (      TaskInfo info : myTaskInfos) {
        if (info.isSetupOrCleanup()) {
          String[] taskTrackers=info.getTaskTrackers();
          for (          String taskTracker : taskTrackers) {
            taskTracker=UtilsForTests.getFQDNofTT(taskTracker);
            LOG.info("taskTracker is :" + taskTracker);
            if (taskTracker != null)             taskTrackerCollection.add(taskTracker);
          }
        }
      }
    }
    while (jInfo != null && !jInfo.getStatus().isJobComplete()) {
      Thread.sleep(10000);
      jInfo=wovenClient.getJobInfo(rJob.getID());
    }
  }
 while (continueLoop);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestDistributedCachePrivateFile </h4><pre class="type-10 type-11 type-13 type-14 type-4 type-7 type-6 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions inside branch conditions">BranchVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies (un)successful execution of the test case by reporting explicitly a failure">UtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies assertions inside branch conditions
- Verifies (un)successful execution of the test case by reporting explicitly a failure
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects are null
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testDistributedCache() throws Exception {
  Configuration conf=new Configuration(cluster.getConf());
  JTProtocol wovenClient=cluster.getJTClient().getProxy();
  int count=0;
  SleepJob job=new SleepJob();
  job.setConf(conf);
  Job slpJob=job.createJob(5,1,1000,1000,100,100);
  DistributedCache.createSymlink(conf);
  URI uri=URI.create(uriPath);
  DistributedCache.addCacheFile(uri,conf);
  JobConf jconf=new JobConf(conf);
  FinishTaskControlAction.configureControlActionForJob(conf);
  slpJob.submit();
  RunningJob rJob=cluster.getJTClient().getClient().getJob(org.apache.hadoop.mapred.JobID.downgrade(slpJob.getJobID()));
  JobStatus[] jobStatus=client.getAllJobs();
  String userName=jobStatus[0].getUsername();
  TTClient tClient=null;
  JobInfo jInfo=wovenClient.getJobInfo(rJob.getID());
  LOG.info("jInfo is :" + jInfo);
  Assert.assertNotNull("jobInfo is null",jInfo);
  count=0;
  while (jInfo.getStatus().getRunState() != JobStatus.RUNNING) {
    UtilsForTests.waitFor(10000);
    count++;
    jInfo=wovenClient.getJobInfo(rJob.getID());
    if (count > 10) {
      Assert.fail("job has not reached running state for more than" + "100 seconds. Failing at this point");
    }
  }
  LOG.info("job id is :" + rJob.getID().toString());
  TaskInfo[] taskInfos=cluster.getJTClient().getProxy().getTaskInfo(rJob.getID());
  boolean distCacheFileIsFound;
  for (  TaskInfo taskInfo : taskInfos) {
    distCacheFileIsFound=false;
    String[] taskTrackers=taskInfo.getTaskTrackers();
    for (    String taskTracker : taskTrackers) {
      taskTracker=UtilsForTests.getFQDNofTT(taskTracker);
      tClient=cluster.getTTClient(taskTracker);
      String[] localDirs=tClient.getMapredLocalDirs();
      int distributedFileCount=0;
      String localDirOnly=null;
      boolean FileNotPresentForThisDirectoryPath=false;
      for (      String localDir : localDirs) {
        FileNotPresentForThisDirectoryPath=false;
        localDirOnly=localDir;
        localDirOnly=localDir + Path.SEPARATOR + TaskTracker.SUBDIR+ Path.SEPARATOR+ userName;
        localDir=localDir + Path.SEPARATOR + TaskTracker.getPrivateDistributedCacheDir(userName);
        FileStatus fileStatusMapredLocalDirUserName=null;
        try {
          fileStatusMapredLocalDirUserName=tClient.getFileStatus(localDirOnly,true);
        }
 catch (        Exception e) {
          LOG.info("LocalDirOnly :" + localDirOnly + " not found");
          FileNotPresentForThisDirectoryPath=true;
        }
        if (FileNotPresentForThisDirectoryPath)         continue;
        Path pathMapredLocalDirUserName=fileStatusMapredLocalDirUserName.getPath();
        FsPermission fsPermMapredLocalDirUserName=fileStatusMapredLocalDirUserName.getPermission();
        Assert.assertTrue("Directory Permission is not 700",fsPermMapredLocalDirUserName.equals(new FsPermission("700")));
        FileStatus[] fileStatuses=tClient.listStatus(localDir,true,true);
        for (        FileStatus fileStatus : fileStatuses) {
          Path path=fileStatus.getPath();
          LOG.info("path is :" + path.toString());
          distCacheFileIsFound=(path.toString()).endsWith(distributedFileName);
          if (distCacheFileIsFound) {
            LOG.info("PATH found is :" + path.toString());
            distributedFileCount++;
            String filename=path.getName();
            FsPermission fsPerm=fileStatus.getPermission();
            Assert.assertTrue("File Permission is not 777",fsPerm.equals(new FsPermission("777")));
          }
        }
      }
      LOG.info("Distributed File count is :" + distributedFileCount);
      if (distributedFileCount > 1) {
        Assert.fail("The distributed cache file is more than one");
      }
 else       if (distributedFileCount < 1)       Assert.fail("The distributed cache file is less than one");
      if (!distCacheFileIsFound) {
        Assert.assertEquals("The distributed cache file does not exist",distCacheFileIsFound,false);
      }
    }
    for (    TaskInfo taskInfoRemaining : taskInfos) {
      FinishTaskControlAction action=new FinishTaskControlAction(TaskID.downgrade(taskInfoRemaining.getTaskID()));
      Collection<TTClient> tts=cluster.getTTClients();
      for (      TTClient cli : tts) {
        cli.getProxy().sendAction(action);
      }
    }
    rJob.killJob();
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestDistributedCacheUnModifiedFile </h4><pre class="type-10 type-11 type-13 type-14 type-4 type-7 type-6 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions inside branch conditions">BranchVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies (un)successful execution of the test case by reporting explicitly a failure">UtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies assertions in iterations
- Verifies assertions inside branch conditions
- Verifies (un)successful execution of the test case by reporting explicitly a failure
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects are null
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testDistributedCache() throws Exception {
  Configuration conf=new Configuration(cluster.getConf());
  JTProtocol wovenClient=cluster.getJTClient().getProxy();
  int count=0;
  boolean continueLoop=true;
  int countLoop=0;
  int taskTrackerCounter=0;
  ArrayList<String> taskTrackerCollection=new ArrayList<String>();
  do {
    SleepJob job=new SleepJob();
    job.setConf(conf);
    Job slpJob=job.createJob(5,1,1000,1000,100,100);
    DistributedCache.createSymlink(conf);
    URI uri=URI.create(uriPath);
    DistributedCache.addCacheFile(uri,conf);
    JobConf jconf=new JobConf(conf);
    FinishTaskControlAction.configureControlActionForJob(conf);
    slpJob.submit();
    RunningJob rJob=cluster.getJTClient().getClient().getJob(org.apache.hadoop.mapred.JobID.downgrade(slpJob.getJobID()));
    countLoop++;
    TTClient tClient=null;
    JobInfo jInfo=wovenClient.getJobInfo(rJob.getID());
    LOG.info("jInfo is :" + jInfo);
    Assert.assertNotNull("jobInfo is null",jInfo);
    count=0;
    while (jInfo.getStatus().getRunState() != JobStatus.RUNNING) {
      UtilsForTests.waitFor(10000);
      count++;
      jInfo=wovenClient.getJobInfo(rJob.getID());
      if (count > 10) {
        Assert.fail("job has not reached running state for more than" + "100 seconds. Failing at this point");
      }
    }
    LOG.info("job id is :" + rJob.getID().toString());
    TaskInfo[] taskInfos=cluster.getJTClient().getProxy().getTaskInfo(rJob.getID());
    boolean distCacheFileIsFound;
    for (    TaskInfo taskInfo : taskInfos) {
      distCacheFileIsFound=false;
      String[] taskTrackers=taskInfo.getTaskTrackers();
      for (      String taskTracker : taskTrackers) {
        taskTracker=UtilsForTests.getFQDNofTT(taskTracker);
        LOG.info("taskTracker is :" + taskTracker);
        if (countLoop > 1) {
          if (taskTracker != null) {
            continueLoop=taskTrackerCollection.contains(taskTracker);
          }
          if (!continueLoop) {
            break;
          }
        }
        if (taskTracker != null)         taskTrackerCollection.add(taskTracker);
        if (countLoop > 2) {
          continueLoop=false;
        }
        tClient=cluster.getTTClient(taskTracker);
        if (tClient == null) {
          continue;
        }
        String[] localDirs=tClient.getMapredLocalDirs();
        int distributedFileCount=0;
        for (        String localDir : localDirs) {
          localDir=localDir + Path.SEPARATOR + TaskTracker.getPublicDistributedCacheDir();
          LOG.info("localDir is : " + localDir);
          FileStatus[] fileStatuses=tClient.listStatus(localDir,true,true);
          for (          FileStatus fileStatus : fileStatuses) {
            Path path=fileStatus.getPath();
            LOG.info("path is :" + path.toString());
            distCacheFileIsFound=(path.toString()).endsWith(distributedFileName);
            if (distCacheFileIsFound) {
              LOG.info("PATH found is :" + path.toString());
              distributedFileCount++;
              String filename=path.getName();
              FsPermission fsPerm=fileStatus.getPermission();
              Assert.assertTrue("File Permission is not 777",fsPerm.equals(new FsPermission("777")));
            }
          }
        }
        if (distributedFileCount > 1) {
          Assert.fail("The distributed cache file is more than one");
        }
 else         if (distributedFileCount < 1)         Assert.fail("The distributed cache file is less than one");
        if (!distCacheFileIsFound) {
          Assert.assertEquals("The distributed cache file does not exist",distCacheFileIsFound,false);
        }
      }
    }
    for (    TaskInfo taskInfoRemaining : taskInfos) {
      FinishTaskControlAction action=new FinishTaskControlAction(TaskID.downgrade(taskInfoRemaining.getTaskID()));
      Collection<TTClient> tts=cluster.getTTClients();
      for (      TTClient cli : tts) {
        cli.getProxy().sendAction(action);
      }
    }
    rJob.killJob();
  }
 while (continueLoop);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestFileOwner </h4><pre class="type-13 type-4 type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions inside branch conditions">BranchVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies assertions inside branch conditions
- Verifies values of objects/variables related to AUT calls
- Verifies whether objects are null
"></span><br>
@Test public void testFilePermission() throws Exception {
  wovenClient=cluster.getJTClient().getProxy();
  Configuration conf=new Configuration(cluster.getConf());
  FinishTaskControlAction.configureControlActionForJob(conf);
  SleepJob job=new SleepJob();
  job.setConf(conf);
  Job slpJob=job.createJob(1,0,100,100,100,100);
  JobConf jconf=new JobConf(conf);
  slpJob.submit();
  RunningJob rJob=cluster.getJTClient().getClient().getJob(org.apache.hadoop.mapred.JobID.downgrade(slpJob.getJobID()));
  taskController=conf.get(TTConfig.TT_TASK_CONTROLLER);
  JobInfo info=wovenClient.getJobInfo(rJob.getID());
  Assert.assertNotNull("JobInfo is null",info);
  JobID id=rJob.getID();
  while (info.runningMaps() != 1) {
    Thread.sleep(1000);
    info=wovenClient.getJobInfo(id);
  }
  TaskInfo[] myTaskInfos=wovenClient.getTaskInfo(id);
  for (  TaskInfo tInfo : myTaskInfos) {
    if (!tInfo.isSetupOrCleanup()) {
      String[] taskTrackers=tInfo.getTaskTrackers();
      for (      String taskTracker : taskTrackers) {
        TTInfo ttInfo=wovenClient.getTTInfo(taskTracker);
        TTClient ttCli=cluster.getTTClient(ttInfo.getStatus().getHost());
        Assert.assertNotNull("TTClient instance is null",ttCli);
        TTTaskInfo ttTaskInfo=ttCli.getProxy().getTask(tInfo.getTaskID());
        Assert.assertNotNull("TTTaskInfo is null",ttTaskInfo);
        while (ttTaskInfo.getTaskStatus().getRunState() != TaskStatus.State.RUNNING) {
          Thread.sleep(100);
          ttTaskInfo=ttCli.getProxy().getTask(tInfo.getTaskID());
        }
        testPermissionWithTaskController(ttCli,conf,info);
        FinishTaskControlAction action=new FinishTaskControlAction(TaskID.downgrade(tInfo.getTaskID()));
        for (        TTClient cli : cluster.getTTClients()) {
          cli.getProxy().sendAction(action);
        }
      }
    }
  }
  JobInfo jInfo=wovenClient.getJobInfo(id);
  jInfo=cluster.getJTClient().getProxy().getJobInfo(id);
  while (!jInfo.getStatus().isJobComplete()) {
    Thread.sleep(100);
    jInfo=cluster.getJTClient().getProxy().getJobInfo(id);
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestJobQueueClient </h4><pre class="type-4 type-6 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to AUT calls
- Verifies whether objects are null
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testGetQueue() throws Exception {
  deleteQueuesConfigFile();
  Document doc=createDocument();
  createSimpleDocumentWithAcls(doc);
  writeToFile(doc,QUEUES_CONFIG_FILE_PATH);
  JobConf jobConf=new JobConf();
  String namenode="file:///";
  miniMRCluster=new MiniMRCluster(0,namenode,3,null,null,jobConf);
  JobClient jc=new JobClient(miniMRCluster.createJobConf());
  QueueInfo queueInfo=jc.getQueueInfo("q1");
  assertEquals("q1",queueInfo.getQueueName());
  queueInfo=jc.getQueueInfo("queue");
  assertNull(queueInfo);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestJobTrackerPlugins </h4><pre class="type-10 type-4 type-7 type-16 type-6 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether two objects/variables are the same">IdentityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether two objects/variables are the same
- Verifies whether objects are null
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void test() throws Exception {
  JobConf conf=new JobConf();
  conf.set(JTConfig.JT_IPC_ADDRESS,"localhost:0");
  conf.set(JTConfig.JT_HTTP_ADDRESS,"0.0.0.0:0");
  conf.setClass(JTConfig.JT_PLUGINS,FakeServicePlugin.class,ServicePlugin.class);
  assertNull("Plugin not created",FakeServicePlugin.getInstance());
  JobTracker jobTracker=JobTracker.startTracker(conf);
  assertNotNull("Plugin created",FakeServicePlugin.getInstance());
  assertSame("Service is jobTracker",FakeServicePlugin.getInstance().getService(),jobTracker);
  assertFalse("Plugin not stopped",FakeServicePlugin.getInstance().isStopped());
  jobTracker.close();
  assertTrue("Plugin stopped",FakeServicePlugin.getInstance().isStopped());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestNetworkedJob </h4><pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
@SuppressWarnings("deprecation") @Test public void testGetNullCounters() throws Exception {
  Job mockJob=mock(Job.class);
  RunningJob underTest=new JobClient.NetworkedJob(mockJob);
  when(mockJob.getCounters()).thenReturn(null);
  assertNull(underTest.getCounters());
  verify(mockJob).getCounters();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestQueueManager </h4><pre class="type-4 type-6 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to AUT calls
- Verifies whether objects are null
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testDefault() throws Exception {
  deleteQueuesConfigFile();
  QueueManager qm=new QueueManager();
  Queue root=qm.getRoot();
  assertEquals(root.getChildren().size(),1);
  assertEquals(root.getChildren().iterator().next().getName(),"default");
  assertNull(root.getChildren().iterator().next().getChildren());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestQueueManagerRefresh </h4><pre class="type-10 type-4 type-6 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies values of objects/variables related to AUT calls
- Verifies whether objects are null
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
/** 
 * Test to verify that the refresh of scheduler properties passes smoothly.
 * @throws Exception
 */
@Test public void testRefreshOfSchedulerProperties() throws Exception {
  JobQueueInfo[] queues=getSimpleQueueHierarchy();
  for (  JobQueueInfo jqi : queues) {
    Properties props=new Properties();
    props.setProperty("testing.property","testing.value." + jqi.getQueueName());
    jqi.setProperties(props);
  }
  writeQueueConfigurationFile(QUEUES_CONFIG_FILE_PATH,new JobQueueInfo[]{queues[0]});
  QueueManager qManager=new QueueManager();
  MyTaskScheduler myScheduler=new MyTaskScheduler();
  qManager.refreshQueues(null,myScheduler.new MyQueueRefresher());
  Map<String,Properties> schedProps=myScheduler.getSchedulerProperties();
  for (  JobQueueInfo jqi : queues) {
    String expectedVal="testing.value." + jqi.getQueueName();
    Properties qProperties=schedProps.get(jqi.getQueueName());
    assertNotNull("Properties should not be null for the SchedulerQueue " + jqi.getQueueName(),qProperties);
    String observedVal=qProperties.getProperty("testing.property");
    assertEquals("Properties for the SchedulerQueue " + jqi.getQueueName() + " are not reloaded properly!",expectedVal,observedVal);
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestRefreshOfQueues </h4><pre class="type-4 type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to AUT calls
- Verifies whether objects are null
"></span><br>
/** 
 * @throws Throwable
 */
@Test public void testRefreshUserLimits() throws Throwable {
  JobQueueInfo[] queues=TestQueueManagerRefresh.getSimpleQueueHierarchy();
  queues[0].getProperties().setProperty(CapacitySchedulerConf.CAPACITY_PROPERTY,String.valueOf(100));
  queues[1].getProperties().setProperty(CapacitySchedulerConf.CAPACITY_PROPERTY,String.valueOf(50));
  queues[2].getProperties().setProperty(CapacitySchedulerConf.CAPACITY_PROPERTY,String.valueOf(50));
  queues[2].getProperties().setProperty(CapacitySchedulerConf.MINIMUM_USER_LIMIT_PERCENT_PROPERTY,String.valueOf(100));
  QueueManagerTestUtils.writeQueueConfigurationFile(queueConfigFile.getAbsolutePath(),new JobQueueInfo[]{queues[0]});
  setupAndStartSchedulerFramework(1,2,2);
  FakeJobInProgress job1=taskTrackerManager.submitJobAndInit(JobStatus.PREP,2,2,queues[2].getQueueName(),"user1");
  FakeJobInProgress job2=taskTrackerManager.submitJobAndInit(JobStatus.PREP,2,2,queues[2].getQueueName(),"user2");
  Map<String,String> expectedStrings=new HashMap<String,String>();
  expectedStrings.put(MAP,"attempt_test_0001_m_000001_0 on tt1");
  expectedStrings.put(REDUCE,"attempt_test_0001_r_000001_0 on tt1");
  checkMultipleTaskAssignment(taskTrackerManager,scheduler,"tt1",expectedStrings);
  expectedStrings.clear();
  expectedStrings.put(MAP,"attempt_test_0001_m_000002_0 on tt1");
  expectedStrings.put(REDUCE,"attempt_test_0001_r_000002_0 on tt1");
  checkMultipleTaskAssignment(taskTrackerManager,scheduler,"tt1",expectedStrings);
  assertNull(scheduler.assignTasks(taskTrackerManager.getTaskTracker("tt1")));
  taskTrackerManager.killJob(job1.getJobID());
  taskTrackerManager.killJob(job2.getJobID());
  queues[2].getProperties().setProperty(CapacitySchedulerConf.MINIMUM_USER_LIMIT_PERCENT_PROPERTY,String.valueOf(50));
  QueueManagerTestUtils.writeQueueConfigurationFile(queueConfigFile.getAbsolutePath(),new JobQueueInfo[]{queues[0]});
  refreshQueues(taskTrackerManager.getQueueManager(),null,scheduler);
  job1=taskTrackerManager.submitJobAndInit(JobStatus.PREP,2,2,queues[1].getQueueName(),"user1");
  job2=taskTrackerManager.submitJobAndInit(JobStatus.PREP,2,2,queues[2].getQueueName(),"user2");
  expectedStrings.clear();
  expectedStrings.put(MAP,"attempt_test_0003_m_000001_0 on tt1");
  expectedStrings.put(REDUCE,"attempt_test_0003_r_000001_0 on tt1");
  checkMultipleTaskAssignment(taskTrackerManager,scheduler,"tt1",expectedStrings);
  expectedStrings.clear();
  expectedStrings.put(MAP,"attempt_test_0004_m_000001_0 on tt1");
  expectedStrings.put(REDUCE,"attempt_test_0004_r_000001_0 on tt1");
  checkMultipleTaskAssignment(taskTrackerManager,scheduler,"tt1",expectedStrings);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestTaskKilling </h4><pre class="type-11 type-13 type-14 type-4 type-7 type-6 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions in iterations">IterativeVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions inside branch conditions">BranchVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies (un)successful execution of the test case by reporting explicitly a failure">UtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies assertions in iterations
- Verifies assertions inside branch conditions
- Verifies (un)successful execution of the test case by reporting explicitly a failure
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects are null
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testAllTaskAttemptKill() throws Exception {
  Configuration conf=new Configuration(cluster.getConf());
  JobStatus[] jobStatus=null;
  SleepJob job=new SleepJob();
  job.setConf(conf);
  Job slpJob=job.createJob(3,1,40000,1000,100,100);
  JobConf jconf=new JobConf(conf);
  slpJob.submit();
  RunningJob rJob=cluster.getJTClient().getClient().getJob(org.apache.hadoop.mapred.JobID.downgrade(slpJob.getJobID()));
  int MAX_MAP_TASK_ATTEMPTS=Integer.parseInt(jconf.get(MRJobConfig.MAP_MAX_ATTEMPTS));
  LOG.info("MAX_MAP_TASK_ATTEMPTS is : " + MAX_MAP_TASK_ATTEMPTS);
  Assert.assertTrue(MAX_MAP_TASK_ATTEMPTS > 0);
  TTClient tClient=null;
  TTClient[] ttClients=null;
  JobInfo jInfo=remoteJTClient.getJobInfo(rJob.getID());
  Assert.assertNotNull(jInfo.getStatus().getRunState());
  while (jInfo.getStatus().getRunState() != JobStatus.RUNNING) {
    try {
      Thread.sleep(10000);
    }
 catch (    InterruptedException e) {
    }
    ;
    jInfo=remoteJTClient.getJobInfo(rJob.getID());
  }
  JobID jobidStore=rJob.getID();
  jobidStore=JobID.downgrade(jobidStore);
  LOG.info("job id is :" + jobidStore.toString());
  TaskInfo[] taskInfos=null;
  boolean runningCount=false;
  int count=0;
  do {
    taskInfos=cluster.getJTClient().getProxy().getTaskInfo(rJob.getID());
    runningCount=false;
    for (    TaskInfo taskInfo : taskInfos) {
      TaskStatus[] taskStatuses=taskInfo.getTaskStatus();
      if (taskStatuses.length > 0) {
        LOG.info("taskStatuses[0].getRunState() is :" + taskStatuses[0].getRunState());
        if (taskStatuses[0].getRunState() == TaskStatus.State.RUNNING) {
          runningCount=true;
          break;
        }
 else {
          LOG.info("Sleeping 5 seconds");
          Thread.sleep(5000);
        }
      }
    }
    count++;
    if (count > 10) {
      Assert.fail("Since the sleep count has reached beyond a point" + "failing at this point");
    }
  }
 while (!runningCount);
  String taskIdKilled=null;
  for (int i=0; i < MAX_MAP_TASK_ATTEMPTS; i++) {
    taskInfos=cluster.getJTClient().getProxy().getTaskInfo(rJob.getID());
    for (    TaskInfo taskInfo : taskInfos) {
      TaskAttemptID taskAttemptID;
      if (!taskInfo.isSetupOrCleanup()) {
        TaskID taskid=TaskID.downgrade(taskInfo.getTaskID());
        LOG.info("taskid is :" + taskid);
        if (i == 0) {
          taskIdKilled=taskid.toString();
          taskAttemptID=new TaskAttemptID(taskid,i);
          LOG.info("taskAttemptid going to be killed is : " + taskAttemptID);
          (new JobClient.NetworkedJob(jInfo.getStatus(),jobClient.cluster)).killTask(taskAttemptID,true);
          checkTaskCompletionEvent(taskAttemptID,jInfo);
          break;
        }
 else {
          if (taskIdKilled.equals(taskid.toString())) {
            taskAttemptID=new TaskAttemptID(taskid,i);
            LOG.info("taskAttemptid going to be killed is : " + taskAttemptID);
            (new JobClient.NetworkedJob(jInfo.getStatus(),jobClient.cluster)).killTask(taskAttemptID,true);
            checkTaskCompletionEvent(taskAttemptID,jInfo);
            break;
          }
        }
      }
    }
  }
  while (jInfo != null && !jInfo.getStatus().isJobComplete()) {
    Thread.sleep(10000);
    jInfo=remoteJTClient.getJobInfo(rJob.getID());
  }
  jobStatus=jobClient.getAllJobs();
  JobStatus jobStatusFound=null;
  for (  JobStatus jobStatusTmp : jobStatus) {
    if (JobID.downgrade(jobStatusTmp.getJobID()).equals(jobidStore)) {
      jobStatusFound=jobStatusTmp;
      LOG.info("jobStatus found is :" + jobStatusFound.getJobId().toString());
    }
  }
  Assert.assertEquals("The job should have failed at this stage",JobStatus.FAILED,jobStatusFound.getRunState());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestTaskLauncher </h4><pre class="type-10 type-4 type-6 type-5 type-12 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values related to public fields.">PublicFieldVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies values of objects/variables related to AUT calls
- Verifies whether objects are null
- Verifies whether objects/variable are equal to an expected value 
- Verifies values related to public fields.
- Contains more than 2 JUnit-based stereotypes
"></span><br>
/** 
 * Tests the case "task waiting to be launched is killed externally".
 * Launches a task which will wait for ever to get slots. Kill the
 * task and see if launcher is able to come out of the wait and pickup a
 * another task.
 * @throws IOException
 */
@Test public void testExternalKillForLaunchTask() throws IOException {
  JobConf ttConf=new JobConf();
  ttConf.setInt(TTConfig.TT_MAP_SLOTS,4);
  TaskTracker tt=new MyTaskTracker();
  tt.runningTasks=new LinkedHashMap<TaskAttemptID,TaskInProgress>();
  tt.setConf(ttConf);
  tt.setIndexCache(new IndexCache(ttConf));
  tt.setTaskMemoryManagerEnabledFlag();
  tt.setTaskTrackerInstrumentation(TaskTracker.createInstrumentation(tt,tt.getJobConf()));
  TaskLauncher mapLauncher=tt.new TaskLauncher(TaskType.MAP,4);
  mapLauncher.start();
  String jtId="test";
  TaskAttemptID attemptID=new TaskAttemptID(jtId,1,TaskType.MAP,0,0);
  Task task=new MapTask(null,attemptID,0,null,5);
  mapLauncher.addToTaskQueue(new LaunchTaskAction(task));
  TaskInProgress killTip=tt.runningTasks.get(attemptID);
  assertNotNull(killTip);
  for (int i=0; i < 300; i++) {
    if (mapLauncher.getNumWaitingTasksToLaunch() == 0) {
      break;
    }
    UtilsForTests.waitFor(100);
  }
  assertEquals("Launcher didnt pick up the task " + attemptID + "to launch",0,mapLauncher.getNumWaitingTasksToLaunch());
  tt.processKillTaskAction(new KillTaskAction(attemptID));
  assertEquals(TaskStatus.State.KILLED,killTip.getRunState());
  TaskAttemptID runningAttemptID=new TaskAttemptID(jtId,1,TaskType.MAP,0,expectedLaunchAttemptId);
  mapLauncher.addToTaskQueue(new LaunchTaskAction(new MapTask(null,runningAttemptID,0,null,1)));
  TaskInProgress runningTip=tt.runningTasks.get(runningAttemptID);
  assertNotNull(runningTip);
  for (int i=0; i < 300; i++) {
    if (runningTip.getRunState().equals(TaskStatus.State.RUNNING)) {
      break;
    }
    UtilsForTests.waitFor(100);
  }
  assertEquals(TaskStatus.State.RUNNING,runningTip.getRunState());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.gridmix.TestCompressionEmulationEnableForAllTypesOfJobs </h4><pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Generate compressed input data and verify the compression emulation
 * for all the jobs in the trace irrespective of whether the original
 * job uses the compressed input or not.Also use the custom compression
 * ratios for map input, map output and reduce output.
 * @throws Exception - if an error occurs.
 */
@Test public void testInputCompressionEmualtionEnableForAllJobsWithDefaultRatios() throws Exception {
  final long inputSizeInMB=1024 * 6;
  final String tracePath=getTraceFile("compression_case4_trace");
  Assert.assertNotNull("Trace file has not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",SubmitterUserResolver.class.getName(),"REPLAY",inputSizeInMB + "m",tracePath};
  final String[] otherArgs={"-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=false","-D",GridMixConfig.GRIDMIX_COMPRESSION_ENABLE + "=true","-D",GridMixConfig.GRIDMIX_INPUT_DECOMPRESS_ENABLE + "=true","-D",GridMixConfig.GRIDMIX_INPUT_COMPRESS_RATIO + "=0.46","-D",GridMixConfig.GRIDMIX_INTERMEDIATE_COMPRESSION_RATIO + "=0.35","-D",GridMixConfig.GRIDMIX_OUTPUT_COMPRESSION_RATIO + "=0.36"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.DATA_GENERATION_AND_RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Use existing compressed input data and turn off the compression 
 * emulation. Verify the compression emulation whether it uses 
 * by the jobs or not.
 * @throws Exception - if an error occurs.
 */
@Test public void testInputCompressionEmulationEnableForAllJobsWithCustomRatios() throws Exception {
  final String tracePath=getTraceFile("compression_case4_trace");
  Assert.assertNotNull("Trace file has not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",SubmitterUserResolver.class.getName(),"SERIAL",tracePath};
  final String[] otherArgs={"-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=false","-D",GridMixConfig.GRIDMIX_COMPRESSION_ENABLE + "=false"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.gridmix.TestCompressionEmulationForCompressInAndUncompressOut </h4><pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Use existing compressed input data and verify the compression ratios 
 * of input and intermediate input against custom compression ratios 
 * and also verify the compressed output file output format is enabled or not.
 * @throws Exception -if an error occurs.
 */
@Test public void testCompressionEmulationOfCompressedInputWithCustomRatios() throws Exception {
  final String tracePath=getTraceFile("compression_case2_trace");
  Assert.assertNotNull("Trace file has not found.",tracePath);
  UtilsForGridmix.cleanup(gridmixDir,rtClient.getDaemonConf());
  final String[] runtimeValues={"LOADJOB",SubmitterUserResolver.class.getName(),"STRESS",inputSizeInMB + "m",tracePath};
  final String[] otherArgs={"-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=false","-D",GridMixConfig.GRIDMIX_COMPRESSION_ENABLE + "=true","-D",GridMixConfig.GRIDMIX_INPUT_DECOMPRESS_ENABLE + "=true","-D",GridMixConfig.GRIDMIX_INPUT_COMPRESS_RATIO + "=0.58","-D",GridMixConfig.GRIDMIX_INTERMEDIATE_COMPRESSION_RATIO + "=0.42"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.DATA_GENERATION_AND_RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Generate a compressed input data and verify the compression ratios 
 * of map input and map output against default compression ratios 
 * and also verify the whether the compressed output file output format 
 * is enabled or not.
 * @throws Exception -if an error occurs.
 */
@Test public void testCompressionEmulationOfCompressedInputWithDefaultRatios() throws Exception {
  final String tracePath=getTraceFile("compression_case2_trace");
  Assert.assertNotNull("Trace file has not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",SubmitterUserResolver.class.getName(),"STRESS",inputSizeInMB + "m",tracePath};
  final String[] otherArgs={"-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=false","-D",GridMixConfig.GRIDMIX_COMPRESSION_ENABLE + "=true"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.DATA_GENERATION_AND_RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.gridmix.TestCompressionEmulationForUncompressInAndCompressOut </h4><pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Use existing uncompressed input data and verify the compression ratio 
 * of reduce output against custom output compression ratio and also verify 
 * the compression output file output format.
 * @throws Exception -if an error occurs.
 */
@Test public void testCompressionEmulationOfCompressedOutputWithCustomRatios() throws Exception {
  final String tracePath=getTraceFile("compression_case3_trace");
  Assert.assertNotNull("Trace file has not found.",tracePath);
  UtilsForGridmix.cleanup(gridmixDir,rtClient.getDaemonConf());
  final String[] runtimeValues={"LOADJOB",SubmitterUserResolver.class.getName(),"STRESS",inputSizeInMB + "m",tracePath};
  final String[] otherArgs={"-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=false","-D",GridMixConfig.GRIDMIX_COMPRESSION_ENABLE + "=true","-D",GridMixConfig.GRIDMIX_OUTPUT_COMPRESSION_RATIO + "=0.38"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.DATA_GENERATION_AND_RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Generate a uncompressed input data and verify the compression ratios 
 * of reduce output against default output compression ratio.
 * @throws Exception -if an error occurs.
 */
@Test public void testCompressionEmulationOfCompressedOuputWithDefaultRatios() throws Exception {
  final String tracePath=getTraceFile("compression_case3_trace");
  Assert.assertNotNull("Trace file has not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",RoundRobinUserResolver.class.getName(),"REPLAY",inputSizeInMB + "m","file://" + UtilsForGridmix.getProxyUsersFile(conf),tracePath};
  final String[] otherArgs={"-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=false","-D",GridMixConfig.GRIDMIX_COMPRESSION_ENABLE + "=true"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.DATA_GENERATION_AND_RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.gridmix.TestEmulationOfHDFSAndLocalFSDCFiles </h4><pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Generate the input data and distributed cache files for HDFS and 
 * local FS. Verify the gridmix emulation of HDFS and Local FS 
 * distributed cache files in RoundRobinUserResolver mode with STRESS
 * submission policy.
 * @throws Exception - if an error occurs.
 */
@Test public void testGenerateDataEmulateHDFSAndLocalFSDCFiles() throws Exception {
  final long inputSizeInMB=1024 * 6;
  final String tracePath=getTraceFile("distcache_case8_trace");
  Assert.assertNotNull("Trace file has not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",RoundRobinUserResolver.class.getName(),"STRESS",inputSizeInMB + "m","file://" + UtilsForGridmix.getProxyUsersFile(conf),tracePath};
  final String[] otherArgs={"-D",MRJobConfig.JOB_CANCEL_DELEGATION_TOKEN + "=false","-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=true","-D",GridMixConfig.GRIDMIX_COMPRESSION_ENABLE + "=false"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.DATA_GENERATION_AND_RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Use existing input and distributed cache files for HDFS and
 * local FS. Verify the gridmix emulation of HDFS and Local FS
 * distributed cache files in SubmitterUserResolver mode with REPLAY
 * submission policy.
 * @throws Exception - if an error occurs.
 */
@Test public void testEmulationOfHDFSAndLocalFSDCFiles() throws Exception {
  final String tracePath=getTraceFile("distcache_case8_trace");
  Assert.assertNotNull("Trace file has not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",SubmitterUserResolver.class.getName(),"STRESS",tracePath};
  final String[] otherArgs={"-D",MRJobConfig.JOB_CANCEL_DELEGATION_TOKEN + "=false","-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=true","-D",GridMixConfig.GRIDMIX_COMPRESSION_ENABLE + "=false"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.gridmix.TestEmulationOfHDFSDCFileUsesMultipleJobs </h4><pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Generate the input data and HDFS distributed cache file based 
 * on given input trace. Verify the Gridmix emulation of HDFS
 * distributed cache file in RoundRobinResolver mode with 
 * STRESS submission policy.
 * @throws Exception - if an error occurs.
 */
@Test public void testGenerateAndEmulationOfHDFSDCFile() throws Exception {
  final long inputSizeInMB=1024 * 6;
  final String tracePath=getTraceFile("distcache_case9_trace");
  Assert.assertNotNull("Trace file has not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",RoundRobinUserResolver.class.getName(),"STRESS",inputSizeInMB + "m","file://" + UtilsForGridmix.getProxyUsersFile(conf),tracePath};
  final String[] otherArgs={"-D",MRJobConfig.JOB_CANCEL_DELEGATION_TOKEN + "=false","-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=true"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.DATA_GENERATION_AND_RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Verify the Gridmix emulation of HDFS distributed cache
 * file in SubmitterUserResolver mode with STRESS submission policy 
 * by using the existing input data and HDFS distributed cache file. 
 * @throws Exception - if an error occurs.
 */
@Test public void testGridmixEmulationOfHDFSPublicDCFile() throws Exception {
  final String tracePath=getTraceFile("distcache_case9_trace");
  Assert.assertNotNull("Trace file has not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",SubmitterUserResolver.class.getName(),"STRESS",tracePath};
  final String[] otherArgs={"-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=true"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.gridmix.TestEmulationOfHDFSDCFilesWithDifferentVisibilities </h4><pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Disable the distributed cache emulation and verify the Gridmix jobs
 * whether it emulates or not. 
 * @throws Exception
 */
@Test public void testHDFSDCFilesWithoutEnableDCEmulation() throws Exception {
  final String tracePath=getTraceFile("distcache_case6_trace");
  Assert.assertNotNull("Trace file was not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",SubmitterUserResolver.class.getName(),"REPLAY",tracePath};
  final String[] otherArgs={"-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=false"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Generate input data and HDFS distributed cache files of different
 * visibilities based on given input trace. Verify the Gridmix emulation 
 * of HDFS distributed cache files of different visibilities in 
 * RoundRobinUserResolver mode with SERIAL submission policy.
 * @throws Exception - if an error occurs.
 */
@Test public void testGenerateAndEmulateOfHDFSDCFilesWithDiffVisibilities() throws Exception {
  final long INPUT_SIZE=1024 * 9;
  final String tracePath=getTraceFile("distcache_case5_trace");
  Assert.assertNotNull("Trace file was not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",RoundRobinUserResolver.class.getName(),"STRESS",INPUT_SIZE + "m","file://" + UtilsForGridmix.getProxyUsersFile(conf),tracePath};
  final String[] otherArgs={"-D",MRJobConfig.JOB_CANCEL_DELEGATION_TOKEN + "=false","-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=true"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.DATA_GENERATION_AND_RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.gridmix.TestEmulationOfLocalFSDCFiles </h4><pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Use existing input and local distributed cache files and  verify 
 * the gridmix emulation of local file system distributed cache 
 * files in SubmitterUserResolver mode with STRESS
 * Submission policy.
 * @throws Exception - if an error occurs.
 */
@Test public void testEmulationOfLocalFSDCFile() throws Exception {
  final String tracePath=getTraceFile("distcache_case7_trace");
  Assert.assertNotNull("Trace file has not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",SubmitterUserResolver.class.getName(),"STRESS",tracePath};
  final String[] otherArgs={"-D",MRJobConfig.JOB_CANCEL_DELEGATION_TOKEN + "=false","-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=true","-D",GridMixConfig.GRIDMIX_COMPRESSION_ENABLE + "=false"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Generate the input data and distributer cache files.Verify the 
 * gridmix emulation of local file system distributed cache files 
 * in RoundRobinUserResolver mode with REPLAY submission policy.
 * @throws Exception - if an error occurs.
 */
@Test public void testGenerateInputAndEmulateLocalFSDCFile() throws Exception {
  final long inputSizeInMB=1024 * 6;
  final String tracePath=getTraceFile("distcache_case7_trace");
  Assert.assertNotNull("Trace file has not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",RoundRobinUserResolver.class.getName(),"REPLAY",inputSizeInMB + "m","file://" + UtilsForGridmix.getProxyUsersFile(conf),tracePath};
  final String[] otherArgs={"-D",MRJobConfig.JOB_CANCEL_DELEGATION_TOKEN + "=false","-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=true","-D",GridMixConfig.GRIDMIX_COMPRESSION_ENABLE + "=false"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.DATA_GENERATION_AND_RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.gridmix.TestGridmixCompressionEmulationWithCompressInput </h4><pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Generate compressed input data and verify the map input, 
 * map output and reduce output compression ratios of gridmix jobs 
 * against the default compression ratios. 
 * @throws Exception - if an error occurs.
 */
@Test public void testGridmixCompressionRatiosAgainstDefaultCompressionRatio() throws Exception {
  final String tracePath=getTraceFile("compression_case1_trace");
  Assert.assertNotNull("Trace file has not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",RoundRobinUserResolver.class.getName(),"STRESS",inputSizeInMB + "m","file://" + UtilsForGridmix.getProxyUsersFile(conf),tracePath};
  final String[] otherArgs={"-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=false","-D",GridMixConfig.GRIDMIX_COMPRESSION_ENABLE + "=true"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.DATA_GENERATION_AND_RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Verify map input, map output and  reduce output compression ratios of
 * gridmix jobs against user specified compression ratios. 
 * @throws Exception - if an error occurs.
 */
@Test public void testGridmixOuputCompressionRatiosAgainstCustomRatios() throws Exception {
  final String tracePath=getTraceFile("compression_case1_trace");
  Assert.assertNotNull("Trace file has not found.",tracePath);
  UtilsForGridmix.cleanup(gridmixDir,rtClient.getDaemonConf());
  final String[] runtimeValues={"LOADJOB",RoundRobinUserResolver.class.getName(),"STRESS",inputSizeInMB + "m","file://" + UtilsForGridmix.getProxyUsersFile(conf),tracePath};
  final String[] otherArgs={"-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=false","-D",GridMixConfig.GRIDMIX_COMPRESSION_ENABLE + "=true","-D",GridMixConfig.GRIDMIX_INPUT_DECOMPRESS_ENABLE + "=true","-D",GridMixConfig.GRIDMIX_INPUT_COMPRESS_RATIO + "=0.68","-D",GridMixConfig.GRIDMIX_INTERMEDIATE_COMPRESSION_RATIO + "=0.35","-D",GridMixConfig.GRIDMIX_OUTPUT_COMPRESSION_RATIO + "=0.40"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.DATA_GENERATION_AND_RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.gridmix.TestGridmixEmulationOfHDFSPrivateDCFile </h4><pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Verify the Gridmix emulation of single HDFS private distributed 
 * cache file in SubmitterUserResolver mode with REPLAY submission 
 * policy by using the existing input data and HDFS private 
 * distributed cache file.
 * @throws Exception - if an error occurs.
 */
@Test public void testGridmixEmulationOfHDFSPrivateDCFile() throws Exception {
  final String tracePath=getTraceFile("distcache_case3_trace");
  Assert.assertNotNull("Trace file was not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",SubmitterUserResolver.class.getName(),"REPLAY",tracePath};
  final String[] otherArgs={"-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=true"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Generate input data and single HDFS private distributed cache 
 * file based on given input trace.Verify the Gridmix emulation of 
 * single private HDFS distributed cache file in RoundRobinUserResolver 
 * mode with STRESS submission policy.
 * @throws Exception - if an error occurs.
 */
@Test public void testGenerateAndEmulateOfHDFSPrivateDCFile() throws Exception {
  final long inputSizeInMB=8192;
  final String tracePath=getTraceFile("distcache_case3_trace");
  Assert.assertNotNull("Trace file was not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",RoundRobinUserResolver.class.getName(),"STRESS",inputSizeInMB + "m","file://" + UtilsForGridmix.getProxyUsersFile(conf),tracePath};
  final String[] otherArgs={"-D",MRJobConfig.JOB_CANCEL_DELEGATION_TOKEN + "=false","-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=true"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.DATA_GENERATION_AND_RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.gridmix.TestGridmixEmulationOfHDFSPublicDCFile </h4><pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Verify the Gridmix emulation of Single HDFS public distributed cache
 * file in RoundRobinUserResolver mode with REPLAY submission policy 
 * by using the existing input data and HDFS public distributed cache file. 
 * @throws Exception - if an error occurs.
 */
@Test public void testGridmixEmulationOfSingleHDFSPublicDCFile() throws Exception {
  final String tracePath=getTraceFile("distcache_case1_trace");
  Assert.assertNotNull("Trace file was not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",RoundRobinUserResolver.class.getName(),"REPLAY","file://" + UtilsForGridmix.getProxyUsersFile(conf),tracePath};
  final String[] otherArgs={"-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=true"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Generate the input data and HDFS distributed cache file based 
 * on given input trace. Verify the Gridmix emulation of single HDFS
 * public distributed cache file in SubmitterUserResolver mode with 
 * STRESS submission policy.
 * @throws Exception - if an error occurs.
 */
@Test public void testGenerateAndEmulationOfSingleHDFSDCFile() throws Exception {
  final long inputSizeInMB=7168;
  final String tracePath=getTraceFile("distcache_case1_trace");
  Assert.assertNotNull("Trace file was not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",SubmitterUserResolver.class.getName(),"STRESS",inputSizeInMB + "m",tracePath};
  final String[] otherArgs={"-D",MRJobConfig.JOB_CANCEL_DELEGATION_TOKEN + "=false","-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=true"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.DATA_GENERATION_AND_RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.gridmix.TestGridmixEmulationOfMultipleHDFSPrivateDCFiles </h4><pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Verify the Gridmix emulation of multiple HDFS private distributed 
 * cache files in SubmitterUserResolver mode with STRESS submission 
 * policy by using the existing input data and HDFS private 
 * distributed cache files.
 * @throws Exception - if an error occurs.
 */
@Test public void testGridmixEmulationOfMultipleHDFSPrivateDCFiles() throws Exception {
  final String tracePath=getTraceFile("distcache_case4_trace");
  Assert.assertNotNull("Trace file was not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",SubmitterUserResolver.class.getName(),"STRESS",tracePath};
  final String[] otherArgs={"-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=true"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Generate input data and multiple HDFS private distributed cache 
 * files based on given input trace.Verify the Gridmix emulation of 
 * multiple private HDFS distributed cache files in RoundRobinUserResolver 
 * mode with SERIAL submission policy.
 * @throws Exception - if an error occurs.
 */
@Test public void testGenerateAndEmulationOfMultipleHDFSPrivateDCFiles() throws Exception {
  final long inputSize=6144;
  final String tracePath=getTraceFile("distcache_case4_trace");
  Assert.assertNotNull("Trace file was not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",RoundRobinUserResolver.class.getName(),"SERIAL",inputSize + "m","file://" + UtilsForGridmix.getProxyUsersFile(conf),tracePath};
  final String[] otherArgs={"-D",MRJobConfig.JOB_CANCEL_DELEGATION_TOKEN + "=false","-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=true"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.DATA_GENERATION_AND_RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.gridmix.TestGridmixEmulationOfMultipleHDFSPublicDCFiles </h4><pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Verify the Gridmix emulation of Single HDFS public distributed cache file 
 * by using an existing input compressed data and HDFS dist cache file. 
 * @throws Exception - if an error occurs.
 */
@Test public void testGridmixEmulationOfMulitpleHDFSPublicDCFile() throws Exception {
  final String tracePath=getTraceFile("distcache_case2_trace");
  Assert.assertNotNull("Trace file was not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",SubmitterUserResolver.class.getName(),"SERIAL",tracePath};
  final String[] otherArgs={"-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=true"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Generate the compressed input data and dist cache files based 
 * on input trace. Verify the Gridmix emulation of
 * multiple HDFS public distributed cache file.
 * @throws Exception - if an error occurs.
 */
@Test public void testGenerateAndEmulationOfMultipleHDFSDCFiles() throws Exception {
  final long inputSizeInMB=7168;
  final String tracePath=getTraceFile("distcache_case2_trace");
  Assert.assertNotNull("Trace file was not found.",tracePath);
  final String[] runtimeValues={"LOADJOB",RoundRobinUserResolver.class.getName(),"STRESS",inputSizeInMB + "m","file://" + UtilsForGridmix.getProxyUsersFile(conf),tracePath};
  final String[] otherArgs={"-D",MRJobConfig.JOB_CANCEL_DELEGATION_TOKEN + "=false","-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=true"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath,GridMixRunMode.DATA_GENERATION_AND_RUN_GRIDMIX.getValue());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.gridmix.TestGridmixWith2minStreamingJobTrace </h4><pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Generate input data and run Gridmix by load job with STRESS submission 
 * policy in a SubmitterUserResolver mode against 2 minutes job 
 * trace file of streaming jobs. Verify each Gridmix job history with 
 * a corresponding job story in a trace file after completion of all 
 * the jobs execution.  
 * @throws Exception - if an error occurs.
 */
@Test public void testGridmixWith2minStreamJobTrace() throws Exception {
  final long inputSizeInMB=cSize * 250;
  final long minFileSize=150 * 1024 * 1024;
  String tracePath=getTraceFile("2m_stream");
  Assert.assertNotNull("Trace file has not found.",tracePath);
  String[] runtimeValues={"LOADJOB",SubmitterUserResolver.class.getName(),"STRESS",inputSizeInMB + "m",tracePath};
  String[] otherArgs={"-D",GridMixConfig.GRIDMIX_JOB_SUBMISSION_QUEUE_IN_TRACE + "=true","-D",GridMixConfig.GRIDMIX_MINIMUM_FILE_SIZE + "=" + minFileSize,"-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=false","-D",GridMixConfig.GRIDMIX_COMPRESSION_ENABLE + "=false"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.gridmix.TestGridmixWith3minStreamingJobTrace </h4><pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Generate input data and run gridmix by load job with REPLAY submission 
 * policy in a RoundRobinUserResolver mode against 3 minutes job trace file 
 * of streaming job. Verify each gridmix job history with a corresponding 
 * job story in a trace file after completion of all the jobs execution.
 * @throws Exception - if an error occurs.
 */
@Test public void testGridmixWith3minStreamJobTrace() throws Exception {
  final long inputSizeInMB=cSize * 200;
  final long bytesPerFile=150 * 1024 * 1024;
  String tracePath=getTraceFile("3m_stream");
  Assert.assertNotNull("Trace file has not found.",tracePath);
  String[] runtimeValues={"LOADJOB",RoundRobinUserResolver.class.getName(),"REPLAY",inputSizeInMB + "m","file://" + UtilsForGridmix.getProxyUsersFile(conf),tracePath};
  String[] otherArgs={"-D",GridMixConfig.GRIDMIX_JOB_SUBMISSION_QUEUE_IN_TRACE + "=true","-D",GridMixConfig.GRIDMIX_BYTES_PER_FILE + "=" + bytesPerFile,"-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=false","-D",GridMixConfig.GRIDMIX_COMPRESSION_ENABLE + "=false"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.gridmix.TestGridmixWith5minStreamingJobTrace </h4><pre class="type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies whether objects are null
"></span><br>
/** 
 * Generate input data and run gridmix by load job with SERIAL submission 
 * policy in a SubmitterUserResolver mode against 5 minutes job trace file 
 * of streaming job. Verify each gridmix job history with a corresponding 
 * job story in a trace file after completion of all the jobs execution.
 * @throws Exception - if an error occurs.
 */
@Test public void testGridmixWith5minStreamJobTrace() throws Exception {
  String tracePath=getTraceFile("5m_stream");
  Assert.assertNotNull("Trace file has not found.",tracePath);
  final long inputSizeInMB=cSize * 200;
  final long bytesPerFile=150 * 1024 * 1024;
  String[] runtimeValues={"LOADJOB",SubmitterUserResolver.class.getName(),"SERIAL",inputSizeInMB + "m",tracePath};
  String[] otherArgs={"-D",GridMixConfig.GRIDMIX_KEY_FRC + "=0.5f","-D",GridMixConfig.GRIDMIX_BYTES_PER_FILE + "=" + bytesPerFile,"-D",GridMixConfig.GRIDMIX_DISTCACHE_ENABLE + "=false","-D",GridMixConfig.GRIDMIX_COMPRESSION_ENABLE + "=false"};
  runGridmixAndVerify(runtimeValues,otherArgs,tracePath);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.gridmix.TestHighRamJob </h4><pre class="type-7 type-6 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies boolean conditions
- Verifies whether objects are null
- Contains more than 2 JUnit-based stereotypes
"></span><br>
/** 
 * Tests high ram job properties configuration.
 */
@SuppressWarnings("deprecation") @Test public void testHighRamFeatureEmulation() throws IOException {
  Configuration gridmixConf=new Configuration();
  gridmixConf.setBoolean(GridmixJob.GRIDMIX_HIGHRAM_EMULATION_ENABLE,false);
  testHighRamConfig(10,20,5,10,JobConf.DISABLED_MEMORY_LIMIT,JobConf.DISABLED_MEMORY_LIMIT,JobConf.DISABLED_MEMORY_LIMIT,JobConf.DISABLED_MEMORY_LIMIT,gridmixConf);
  gridmixConf=new Configuration();
  gridmixConf.setLong(JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY,20 * 1024 * 1024);
  testHighRamConfig(10,20,5,10,5,10,10,20,gridmixConf);
  gridmixConf=new Configuration();
  gridmixConf.setLong(JTConfig.JT_MAX_MAPMEMORY_MB,100);
  gridmixConf.setLong(JTConfig.JT_MAX_REDUCEMEMORY_MB,300);
  testHighRamConfig(10,45,5,15,50,100,100,300,gridmixConf);
  gridmixConf=new Configuration();
  gridmixConf.setLong(JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY,70 * 1024 * 1024);
  Boolean failed=null;
  try {
    testHighRamConfig(10,45,5,15,50,100,100,300,gridmixConf);
    failed=false;
  }
 catch (  Exception e) {
    failed=true;
  }
  assertNotNull(failed);
  assertTrue("Exception expected for exceeding map memory limit " + "(deprecation)!",failed);
  gridmixConf=new Configuration();
  gridmixConf.setLong(JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY,150 * 1024 * 1024);
  failed=null;
  try {
    testHighRamConfig(10,45,5,15,50,100,100,300,gridmixConf);
    failed=false;
  }
 catch (  Exception e) {
    failed=true;
  }
  assertNotNull(failed);
  assertTrue("Exception expected for exceeding reduce memory limit " + "(deprecation)!",failed);
  gridmixConf=new Configuration();
  gridmixConf.setLong(JTConfig.JT_MAX_MAPMEMORY_MB,70);
  failed=null;
  try {
    testHighRamConfig(10,45,5,15,50,100,100,300,gridmixConf);
    failed=false;
  }
 catch (  Exception e) {
    failed=true;
  }
  assertNotNull(failed);
  assertTrue("Exception expected for exceeding map memory limit!",failed);
  gridmixConf=new Configuration();
  gridmixConf.setLong(JTConfig.JT_MAX_REDUCEMEMORY_MB,200);
  failed=null;
  try {
    testHighRamConfig(10,45,5,15,50,100,100,300,gridmixConf);
    failed=false;
  }
 catch (  Exception e) {
    failed=true;
  }
  assertNotNull(failed);
  assertTrue("Exception expected for exceeding reduce memory limit!",failed);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapreduce.TestLocalRunner </h4><pre class="type-4 type-7 type-6 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects are null
- Contains more than 2 JUnit-based stereotypes
"></span><br>
/** 
 * Test that the GC counter actually increments when we know that we've
 * spent some time in the GC during the mapper.
 */
@Test public void testGcCounter() throws Exception {
  Path inputPath=getInputPath();
  Path outputPath=getOutputPath();
  Configuration conf=new Configuration();
  FileSystem fs=FileSystem.getLocal(conf);
  if (fs.exists(outputPath)) {
    fs.delete(outputPath,true);
  }
  if (fs.exists(inputPath)) {
    fs.delete(inputPath,true);
  }
  createInputFile(inputPath,0,20);
  Job job=Job.getInstance();
  job.setMapperClass(GCMapper.class);
  job.setNumReduceTasks(0);
  job.getConfiguration().set("io.sort.mb","25");
  FileInputFormat.addInputPath(job,inputPath);
  FileOutputFormat.setOutputPath(job,outputPath);
  boolean ret=job.waitForCompletion(true);
  assertTrue("job failed",ret);
  Counter gcCounter=job.getCounters().findCounter(TaskCounter.GC_TIME_MILLIS);
  assertNotNull(gcCounter);
  assertTrue("No time spent in gc",gcCounter.getValue() > 0);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapreduce.security.TestTokenCache </h4><pre class="type-10 type-4 type-7 type-6 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects are null
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testGetTokensForNamenodes() throws IOException {
  Credentials credentials=new Credentials();
  TokenCache.obtainTokensForNamenodesInternal(credentials,new Path[]{p1,p2},jConf);
  String fs_addr=SecurityUtil.buildDTServiceName(p1.toUri(),NameNode.DEFAULT_PORT);
  Token<DelegationTokenIdentifier> nnt=TokenCache.getDelegationToken(credentials,fs_addr);
  System.out.println("dt for " + p1 + "("+ fs_addr+ ")"+ " = "+ nnt);
  assertNotNull("Token for nn is null",nnt);
  Collection<Token<? extends TokenIdentifier>> tns=credentials.getAllTokens();
  assertEquals("number of tokens is not 1",1,tns.size());
  boolean found=false;
  for (  Token<? extends TokenIdentifier> t : tns) {
    if (t.getKind().equals(DelegationTokenIdentifier.HDFS_DELEGATION_KIND) && t.getService().equals(new Text(fs_addr))) {
      found=true;
    }
    assertTrue("didn't find token for " + p1,found);
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapreduce.util.TestMRAsyncDiskService </h4><pre class="type-4 type-7 type-6 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to AUT calls
- Verifies boolean conditions
- Verifies whether objects are null
- Contains more than 2 JUnit-based stereotypes
"></span><br>
/** 
 * This test creates some directories and then removes them through 
 * MRAsyncDiskService. 
 */
@Test public void testMRAsyncDiskService() throws Throwable {
  FileSystem localFileSystem=FileSystem.getLocal(new Configuration());
  String[] vols=new String[]{TEST_ROOT_DIR + "/0",TEST_ROOT_DIR + "/1"};
  MRAsyncDiskService service=new MRAsyncDiskService(localFileSystem,vols);
  String a="a";
  String b="b";
  String c="b/c";
  String d="d";
  File fa=new File(vols[0],a);
  File fb=new File(vols[1],b);
  File fc=new File(vols[1],c);
  File fd=new File(vols[1],d);
  fa.mkdirs();
  fb.mkdirs();
  fc.mkdirs();
  fd.mkdirs();
  assertTrue(fa.exists());
  assertTrue(fb.exists());
  assertTrue(fc.exists());
  assertTrue(fd.exists());
  service.moveAndDeleteRelativePath(vols[0],a);
  assertFalse(fa.exists());
  service.moveAndDeleteRelativePath(vols[1],b);
  assertFalse(fb.exists());
  assertFalse(fc.exists());
  assertFalse(service.moveAndDeleteRelativePath(vols[1],"not_exists"));
  IOException ee=null;
  try {
    service.moveAndDeleteAbsolutePath(TEST_ROOT_DIR + "/2");
  }
 catch (  IOException e) {
    ee=e;
  }
  assertNotNull("asyncDiskService should not be able to delete files " + "outside all volumes",ee);
  assertTrue(service.moveAndDeleteAbsolutePath(vols[1] + Path.SEPARATOR_CHAR + d));
  makeSureCleanedUp(vols,service);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.streaming.TestClassWithNoPackage </h4><pre class="type-10 type-4 type-6 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies values of objects/variables related to AUT calls
- Verifies whether objects are null
"></span><br>
@Test public void testGoodClassOrNull() throws MalformedURLException {
  Configuration conf=new Configuration();
  conf.setClassLoader(new URLClassLoader(new URL[]{new URL("file",null,JAR)},conf.getClassLoader()));
  String defaultPackage=this.getClass().getPackage().getName();
  Class c=StreamUtil.goodClassOrNull(conf,NAME,defaultPackage);
  assertNotNull("Class " + NAME + " not found!",c);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.tools.rumen.TestRumenJobTraces </h4><pre class="type-10 type-4 type-6 type-5 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to API calls (Java or TPL) ">APIUtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies values of objects/variables related to AUT calls">InternalCallVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects are null">NullVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies whether objects/variable are equal to an expected value ">EqualityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies values of objects/variables related to API calls (Java or TPL) 
- Verifies values of objects/variables related to AUT calls
- Verifies whether objects are null
- Verifies whether objects/variable are equal to an expected value 
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Test public void testHadoop20JHParser() throws Exception {
  final Configuration conf=new Configuration();
  final FileSystem lfs=FileSystem.getLocal(conf);
  final Path rootInputDir=new Path(System.getProperty("test.tools.input.dir","")).makeQualified(lfs.getUri(),lfs.getWorkingDirectory());
  final Path rootInputPath=new Path(rootInputDir,"rumen/small-trace-test");
  final Path inputPath=new Path(rootInputPath,"v20-single-input-log.gz");
  RewindableInputStream ris=getRewindableInputStream(inputPath,conf);
  assertNotNull(ris);
  Hadoop20JHParser parser=null;
  try {
    assertEquals("Hadoop20JHParser can't parse the test file " + inputPath,true,Hadoop20JHParser.canParse(ris));
    ris.rewind();
    parser=new Hadoop20JHParser(ris);
    ArrayList<String> seenEvents=new ArrayList<String>(150);
    getHistoryEvents(parser,seenEvents,null);
    validateSeenHistoryEvents(seenEvents,goldLines);
  }
  finally {
    if (parser != null) {
      parser.close();
    }
    ris.close();
  }
}

</code></pre>

<br>
<script>$(document).ready(function() {
  $('pre code').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});</script>
