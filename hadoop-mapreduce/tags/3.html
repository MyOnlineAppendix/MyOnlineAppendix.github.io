<h3><span class=" glyphicon glyphicon-tag"/>&nbspTestInitializer</h3><kbd>Allocates resources before the execution of the test cases</kbd><br><br><br><h4 style="margin:0px">Class: org.apache.hadoop.cli.TestMRCLI </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
@Before public void setUp() throws Exception {
  super.setUp();
  conf.setClass(PolicyProvider.POLICY_PROVIDER_CONFIG,HadoopPolicyProvider.class,PolicyProvider.class);
  mrConf=new JobConf(conf);
  mrCluster=new MiniMRCluster(1,dfsCluster.getFileSystem().getUri().toString(),1,null,null,mrConf);
  jobtracker=mrCluster.createJobConf().get(JTConfig.JT_IPC_ADDRESS,"local");
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.fs.TestJHLA </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
@Before public void setUp() throws Exception {
  File logFile=new File(historyLog);
  if (!logFile.getParentFile().exists())   if (!logFile.getParentFile().mkdirs())   LOG.error("Cannot create dirs for history log file: " + historyLog);
  if (!logFile.createNewFile())   LOG.error("Cannot create history log file: " + historyLog);
  BufferedWriter writer=new BufferedWriter(new OutputStreamWriter(new FileOutputStream(historyLog)));
  writer.write("$!!FILE=file1.log!!");
  writer.newLine();
  writer.write("Meta VERSION=\"1\" .");
  writer.newLine();
  writer.write("Job JOBID=\"job_200903250600_0004\" JOBNAME=\"streamjob21364.jar\" USER=\"hadoop\" SUBMIT_TIME=\"1237962008012\" JOBCONF=\"hdfs:///job_200903250600_0004/job.xml\" .");
  writer.newLine();
  writer.write("Job JOBID=\"job_200903250600_0004\" JOB_PRIORITY=\"NORMAL\" .");
  writer.newLine();
  writer.write("Job JOBID=\"job_200903250600_0004\" LAUNCH_TIME=\"1237962008712\" TOTAL_MAPS=\"2\" TOTAL_REDUCES=\"0\" JOB_STATUS=\"PREP\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0004_m_000003\" TASK_TYPE=\"SETUP\" START_TIME=\"1237962008736\" SPLITS=\"\" .");
  writer.newLine();
  writer.write("MapAttempt TASK_TYPE=\"SETUP\" TASKID=\"task_200903250600_0004_m_000003\" TASK_ATTEMPT_ID=\"attempt_200903250600_0004_m_000003_0\" START_TIME=\"1237962010929\" TRACKER_NAME=\"tracker_50445\" HTTP_PORT=\"50060\" .");
  writer.newLine();
  writer.write("MapAttempt TASK_TYPE=\"SETUP\" TASKID=\"task_200903250600_0004_m_000003\" TASK_ATTEMPT_ID=\"attempt_200903250600_0004_m_000003_0\" TASK_STATUS=\"SUCCESS\" FINISH_TIME=\"1237962012459\" HOSTNAME=\"host.com\" STATE_STRING=\"setup\" COUNTERS=\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)]}\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0004_m_000003\" TASK_TYPE=\"SETUP\" TASK_STATUS=\"SUCCESS\" FINISH_TIME=\"1237962023824\" COUNTERS=\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)]}\" .");
  writer.newLine();
  writer.write("Job JOBID=\"job_200903250600_0004\" JOB_STATUS=\"RUNNING\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0004_m_000000\" TASK_TYPE=\"MAP\" START_TIME=\"1237962024049\" SPLITS=\"host1.com,host2.com,host3.com\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0004_m_000001\" TASK_TYPE=\"MAP\" START_TIME=\"1237962024065\" SPLITS=\"host1.com,host2.com,host3.com\" .");
  writer.newLine();
  writer.write("MapAttempt TASK_TYPE=\"MAP\" TASKID=\"task_200903250600_0004_m_000000\" TASK_ATTEMPT_ID=\"attempt_200903250600_0004_m_000000_0\" START_TIME=\"1237962026157\" TRACKER_NAME=\"tracker_50524\" HTTP_PORT=\"50060\" .");
  writer.newLine();
  writer.write("MapAttempt TASK_TYPE=\"MAP\" TASKID=\"task_200903250600_0004_m_000000\" TASK_ATTEMPT_ID=\"attempt_200903250600_0004_m_000000_0\" TASK_STATUS=\"SUCCESS\" FINISH_TIME=\"1237962041307\" HOSTNAME=\"host.com\" STATE_STRING=\"Records R/W=2681/1\" COUNTERS=\"{(FileSystemCounters)(FileSystemCounters)[(HDFS_BYTES_READ)(HDFS_BYTES_READ)(56630)][(HDFS_BYTES_WRITTEN)(HDFS_BYTES_WRITTEN)(28327)]}{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(2681)][(SPILLED_RECORDS)(Spilled Records)(0)][(MAP_INPUT_BYTES)(Map input bytes)(28327)][(MAP_OUTPUT_RECORDS)(Map output records)(2681)]}\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0004_m_000000\" TASK_TYPE=\"MAP\" TASK_STATUS=\"SUCCESS\" FINISH_TIME=\"1237962054138\" COUNTERS=\"{(FileSystemCounters)(FileSystemCounters)[(HDFS_BYTES_READ)(HDFS_BYTES_READ)(56630)][(HDFS_BYTES_WRITTEN)(HDFS_BYTES_WRITTEN)(28327)]}{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(2681)][(SPILLED_RECORDS)(Spilled Records)(0)][(MAP_INPUT_BYTES)(Map input bytes)(28327)][(MAP_OUTPUT_RECORDS)(Map output records)(2681)]}\" .");
  writer.newLine();
  writer.write("MapAttempt TASK_TYPE=\"MAP\" TASKID=\"task_200903250600_0004_m_000001\" TASK_ATTEMPT_ID=\"attempt_200903250600_0004_m_000001_0\" START_TIME=\"1237962026077\" TRACKER_NAME=\"tracker_50162\" HTTP_PORT=\"50060\" .");
  writer.newLine();
  writer.write("MapAttempt TASK_TYPE=\"MAP\" TASKID=\"task_200903250600_0004_m_000001\" TASK_ATTEMPT_ID=\"attempt_200903250600_0004_m_000001_0\" TASK_STATUS=\"SUCCESS\" FINISH_TIME=\"1237962041030\" HOSTNAME=\"host.com\" STATE_STRING=\"Records R/W=2634/1\" COUNTERS=\"{(FileSystemCounters)(FileSystemCounters)[(HDFS_BYTES_READ)(HDFS_BYTES_READ)(28316)][(HDFS_BYTES_WRITTEN)(HDFS_BYTES_WRITTEN)(28303)]}{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(2634)][(SPILLED_RECORDS)(Spilled Records)(0)][(MAP_INPUT_BYTES)(Map input bytes)(28303)][(MAP_OUTPUT_RECORDS)(Map output records)(2634)]}\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0004_m_000001\" TASK_TYPE=\"MAP\" TASK_STATUS=\"SUCCESS\" FINISH_TIME=\"1237962054187\" COUNTERS=\"{(FileSystemCounters)(FileSystemCounters)[(HDFS_BYTES_READ)(HDFS_BYTES_READ)(28316)][(HDFS_BYTES_WRITTEN)(HDFS_BYTES_WRITTEN)(28303)]}{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(2634)][(SPILLED_RECORDS)(Spilled Records)(0)][(MAP_INPUT_BYTES)(Map input bytes)(28303)][(MAP_OUTPUT_RECORDS)(Map output records)(2634)]}\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0004_m_000002\" TASK_TYPE=\"CLEANUP\" START_TIME=\"1237962054187\" SPLITS=\"\" .");
  writer.newLine();
  writer.write("MapAttempt TASK_TYPE=\"CLEANUP\" TASKID=\"task_200903250600_0004_m_000002\" TASK_ATTEMPT_ID=\"attempt_200903250600_0004_m_000002_0\" START_TIME=\"1237962055578\" TRACKER_NAME=\"tracker_50162\" HTTP_PORT=\"50060\" .");
  writer.newLine();
  writer.write("MapAttempt TASK_TYPE=\"CLEANUP\" TASKID=\"task_200903250600_0004_m_000002\" TASK_ATTEMPT_ID=\"attempt_200903250600_0004_m_000002_0\" TASK_STATUS=\"SUCCESS\" FINISH_TIME=\"1237962056782\" HOSTNAME=\"host.com\" STATE_STRING=\"cleanup\" COUNTERS=\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)]}\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0004_m_000002\" TASK_TYPE=\"CLEANUP\" TASK_STATUS=\"SUCCESS\" FINISH_TIME=\"1237962069193\" COUNTERS=\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)]}\" .");
  writer.newLine();
  writer.write("Job JOBID=\"job_200903250600_0004\" FINISH_TIME=\"1237962069193\" JOB_STATUS=\"SUCCESS\" FINISHED_MAPS=\"2\" FINISHED_REDUCES=\"0\" FAILED_MAPS=\"0\" FAILED_REDUCES=\"0\" COUNTERS=\"{(org.apache.hadoop.mapred.JobInProgress$Counter)(Job Counters )[(TOTAL_LAUNCHED_MAPS)(Launched map tasks)(2)]}{(FileSystemCounters)(FileSystemCounters)[(HDFS_BYTES_READ)(HDFS_BYTES_READ)(84946)][(HDFS_BYTES_WRITTEN)(HDFS_BYTES_WRITTEN)(56630)]}{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(5315)][(SPILLED_RECORDS)(Spilled Records)(0)][(MAP_INPUT_BYTES)(Map input bytes)(56630)][(MAP_OUTPUT_RECORDS)(Map output records)(5315)]}\" .");
  writer.newLine();
  writer.write("$!!FILE=file2.log!!");
  writer.newLine();
  writer.write("Meta VERSION=\"1\" .");
  writer.newLine();
  writer.write("Job JOBID=\"job_200903250600_0023\" JOBNAME=\"TestJob\" USER=\"hadoop2\" SUBMIT_TIME=\"1237964779799\" JOBCONF=\"hdfs:///job_200903250600_0023/job.xml\" .");
  writer.newLine();
  writer.write("Job JOBID=\"job_200903250600_0023\" JOB_PRIORITY=\"NORMAL\" .");
  writer.newLine();
  writer.write("Job JOBID=\"job_200903250600_0023\" LAUNCH_TIME=\"1237964780928\" TOTAL_MAPS=\"2\" TOTAL_REDUCES=\"0\" JOB_STATUS=\"PREP\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0023_r_000001\" TASK_TYPE=\"SETUP\" START_TIME=\"1237964780940\" SPLITS=\"\" .");
  writer.newLine();
  writer.write("ReduceAttempt TASK_TYPE=\"SETUP\" TASKID=\"task_200903250600_0023_r_000001\" TASK_ATTEMPT_ID=\"attempt_200903250600_0023_r_000001_0\" START_TIME=\"1237964720322\" TRACKER_NAME=\"tracker_3065\" HTTP_PORT=\"50060\" .");
  writer.newLine();
  writer.write("ReduceAttempt TASK_TYPE=\"SETUP\" TASKID=\"task_200903250600_0023_r_000001\" TASK_ATTEMPT_ID=\"attempt_200903250600_0023_r_000001_0\" TASK_STATUS=\"SUCCESS\" SHUFFLE_FINISHED=\"1237964722118\" SORT_FINISHED=\"1237964722118\" FINISH_TIME=\"1237964722118\" HOSTNAME=\"host.com\" STATE_STRING=\"setup\" COUNTERS=\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(REDUCE_INPUT_GROUPS)(Reduce input groups)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(REDUCE_SHUFFLE_BYTES)(Reduce shuffle bytes)(0)][(REDUCE_OUTPUT_RECORDS)(Reduce output records)(0)][(SPILLED_RECORDS)(Spilled Records)(0)][(REDUCE_INPUT_RECORDS)(Reduce input records)(0)]}\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0023_r_000001\" TASK_TYPE=\"SETUP\" TASK_STATUS=\"SUCCESS\" FINISH_TIME=\"1237964796054\" COUNTERS=\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(REDUCE_INPUT_GROUPS)(Reduce input groups)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(REDUCE_SHUFFLE_BYTES)(Reduce shuffle bytes)(0)][(REDUCE_OUTPUT_RECORDS)(Reduce output records)(0)][(SPILLED_RECORDS)(Spilled Records)(0)][(REDUCE_INPUT_RECORDS)(Reduce input records)(0)]}\" .");
  writer.newLine();
  writer.write("Job JOBID=\"job_200903250600_0023\" JOB_STATUS=\"RUNNING\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0023_m_000000\" TASK_TYPE=\"MAP\" START_TIME=\"1237964796176\" SPLITS=\"\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0023_m_000001\" TASK_TYPE=\"MAP\" START_TIME=\"1237964796176\" SPLITS=\"\" .");
  writer.newLine();
  writer.write("MapAttempt TASK_TYPE=\"MAP\" TASKID=\"task_200903250600_0023_m_000000\" TASK_ATTEMPT_ID=\"attempt_200903250600_0023_m_000000_0\" START_TIME=\"1237964809765\" TRACKER_NAME=\"tracker_50459\" HTTP_PORT=\"50060\" .");
  writer.newLine();
  writer.write("MapAttempt TASK_TYPE=\"MAP\" TASKID=\"task_200903250600_0023_m_000000\" TASK_ATTEMPT_ID=\"attempt_200903250600_0023_m_000000_0\" TASK_STATUS=\"SUCCESS\" FINISH_TIME=\"1237964911772\" HOSTNAME=\"host.com\" STATE_STRING=\"\" COUNTERS=\"{(FileSystemCounters)(FileSystemCounters)[(HDFS_BYTES_WRITTEN)(HDFS_BYTES_WRITTEN)(500000000)]}{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(5000000)][(SPILLED_RECORDS)(Spilled Records)(0)][(MAP_INPUT_BYTES)(Map input bytes)(5000000)][(MAP_OUTPUT_RECORDS)(Map output records)(5000000)]}\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0023_m_000000\" TASK_TYPE=\"MAP\" TASK_STATUS=\"SUCCESS\" FINISH_TIME=\"1237964916534\" COUNTERS=\"{(FileSystemCounters)(FileSystemCounters)[(HDFS_BYTES_WRITTEN)(HDFS_BYTES_WRITTEN)(500000000)]}{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(5000000)][(SPILLED_RECORDS)(Spilled Records)(0)][(MAP_INPUT_BYTES)(Map input bytes)(5000000)][(MAP_OUTPUT_RECORDS)(Map output records)(5000000)]}\" .");
  writer.newLine();
  writer.write("MapAttempt TASK_TYPE=\"MAP\" TASKID=\"task_200903250600_0023_m_000001\" TASK_ATTEMPT_ID=\"attempt_200903250600_0023_m_000001_0\" START_TIME=\"1237964798169\" TRACKER_NAME=\"tracker_1524\" HTTP_PORT=\"50060\" .");
  writer.newLine();
  writer.write("MapAttempt TASK_TYPE=\"MAP\" TASKID=\"task_200903250600_0023_m_000001\" TASK_ATTEMPT_ID=\"attempt_200903250600_0023_m_000001_0\" TASK_STATUS=\"SUCCESS\" FINISH_TIME=\"1237964962960\" HOSTNAME=\"host.com\" STATE_STRING=\"\" COUNTERS=\"{(FileSystemCounters)(FileSystemCounters)[(HDFS_BYTES_WRITTEN)(HDFS_BYTES_WRITTEN)(500000000)]}{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(5000000)][(SPILLED_RECORDS)(Spilled Records)(0)][(MAP_INPUT_BYTES)(Map input bytes)(5000000)][(MAP_OUTPUT_RECORDS)(Map output records)(5000000)]}\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0023_m_000001\" TASK_TYPE=\"MAP\" TASK_STATUS=\"SUCCESS\" FINISH_TIME=\"1237964976870\" COUNTERS=\"{(FileSystemCounters)(FileSystemCounters)[(HDFS_BYTES_WRITTEN)(HDFS_BYTES_WRITTEN)(500000000)]}{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(5000000)][(SPILLED_RECORDS)(Spilled Records)(0)][(MAP_INPUT_BYTES)(Map input bytes)(5000000)][(MAP_OUTPUT_RECORDS)(Map output records)(5000000)]}\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0023_r_000000\" TASK_TYPE=\"CLEANUP\" START_TIME=\"1237964976871\" SPLITS=\"\" .");
  writer.newLine();
  writer.write("ReduceAttempt TASK_TYPE=\"CLEANUP\" TASKID=\"task_200903250600_0023_r_000000\" TASK_ATTEMPT_ID=\"attempt_200903250600_0023_r_000000_0\" START_TIME=\"1237964977208\" TRACKER_NAME=\"tracker_1524\" HTTP_PORT=\"50060\" .");
  writer.newLine();
  writer.write("ReduceAttempt TASK_TYPE=\"CLEANUP\" TASKID=\"task_200903250600_0023_r_000000\" TASK_ATTEMPT_ID=\"attempt_200903250600_0023_r_000000_0\" TASK_STATUS=\"SUCCESS\" SHUFFLE_FINISHED=\"1237964979031\" SORT_FINISHED=\"1237964979031\" FINISH_TIME=\"1237964979032\" HOSTNAME=\"host.com\" STATE_STRING=\"cleanup\" COUNTERS=\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(REDUCE_INPUT_GROUPS)(Reduce input groups)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(REDUCE_SHUFFLE_BYTES)(Reduce shuffle bytes)(0)][(REDUCE_OUTPUT_RECORDS)(Reduce output records)(0)][(SPILLED_RECORDS)(Spilled Records)(0)][(REDUCE_INPUT_RECORDS)(Reduce input records)(0)]}\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0023_r_000000\" TASK_TYPE=\"CLEANUP\" TASK_STATUS=\"SUCCESS\" FINISH_TIME=\"1237964991879\" COUNTERS=\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(REDUCE_INPUT_GROUPS)(Reduce input groups)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(REDUCE_SHUFFLE_BYTES)(Reduce shuffle bytes)(0)][(REDUCE_OUTPUT_RECORDS)(Reduce output records)(0)][(SPILLED_RECORDS)(Spilled Records)(0)][(REDUCE_INPUT_RECORDS)(Reduce input records)(0)]}\" .");
  writer.newLine();
  writer.write("Job JOBID=\"job_200903250600_0023\" FINISH_TIME=\"1237964991879\" JOB_STATUS=\"SUCCESS\" FINISHED_MAPS=\"2\" FINISHED_REDUCES=\"0\" FAILED_MAPS=\"0\" FAILED_REDUCES=\"0\" COUNTERS=\"{(org.apache.hadoop.mapred.JobInProgress$Counter)(Job Counters )[(TOTAL_LAUNCHED_MAPS)(Launched map tasks)(2)]}{(FileSystemCounters)(FileSystemCounters)[(HDFS_BYTES_WRITTEN)(HDFS_BYTES_WRITTEN)(1000000000)]}{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(10000000)][(SPILLED_RECORDS)(Spilled Records)(0)][(MAP_INPUT_BYTES)(Map input bytes)(10000000)][(MAP_OUTPUT_RECORDS)(Map output records)(10000000)]}\" .");
  writer.newLine();
  writer.write("$!!FILE=file3.log!!");
  writer.newLine();
  writer.write("Meta VERSION=\"1\" .");
  writer.newLine();
  writer.write("Job JOBID=\"job_200903250600_0034\" JOBNAME=\"TestJob\" USER=\"hadoop3\" SUBMIT_TIME=\"1237966370007\" JOBCONF=\"hdfs:///job_200903250600_0034/job.xml\" .");
  writer.newLine();
  writer.write("Job JOBID=\"job_200903250600_0034\" JOB_PRIORITY=\"NORMAL\" .");
  writer.newLine();
  writer.write("Job JOBID=\"job_200903250600_0034\" LAUNCH_TIME=\"1237966371076\" TOTAL_MAPS=\"2\" TOTAL_REDUCES=\"0\" JOB_STATUS=\"PREP\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0034_m_000003\" TASK_TYPE=\"SETUP\" START_TIME=\"1237966371093\" SPLITS=\"\" .");
  writer.newLine();
  writer.write("MapAttempt TASK_TYPE=\"SETUP\" TASKID=\"task_200903250600_0034_m_000003\" TASK_ATTEMPT_ID=\"attempt_200903250600_0034_m_000003_0\" START_TIME=\"1237966371524\" TRACKER_NAME=\"tracker_50118\" HTTP_PORT=\"50060\" .");
  writer.newLine();
  writer.write("MapAttempt TASK_TYPE=\"SETUP\" TASKID=\"task_200903250600_0034_m_000003\" TASK_ATTEMPT_ID=\"attempt_200903250600_0034_m_000003_0\" TASK_STATUS=\"SUCCESS\" FINISH_TIME=\"1237966373174\" HOSTNAME=\"host.com\" STATE_STRING=\"setup\" COUNTERS=\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)]}\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0034_m_000003\" TASK_TYPE=\"SETUP\" TASK_STATUS=\"SUCCESS\" FINISH_TIME=\"1237966386098\" COUNTERS=\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)]}\" .");
  writer.newLine();
  writer.write("Job JOBID=\"job_200903250600_0034\" JOB_STATUS=\"RUNNING\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0034_m_000000\" TASK_TYPE=\"MAP\" START_TIME=\"1237966386111\" SPLITS=\"\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0034_m_000001\" TASK_TYPE=\"MAP\" START_TIME=\"1237966386124\" SPLITS=\"\" .");
  writer.newLine();
  writer.write("MapAttempt TASK_TYPE=\"MAP\" TASKID=\"task_200903250600_0034_m_000001\" TASK_ATTEMPT_ID=\"attempt_200903250600_0034_m_000001_0\" TASK_STATUS=\"FAILED\" FINISH_TIME=\"1237967174546\" HOSTNAME=\"host.com\" ERROR=\"java.io.IOException: Task process exit with nonzero status of 15.");
  writer.newLine();
  writer.write("  at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:424)");
  writer.newLine();
  writer.write(",java.io.IOException: Task process exit with nonzero status of 15.");
  writer.newLine();
  writer.write("  at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:424)");
  writer.newLine();
  writer.write("\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0034_m_000002\" TASK_TYPE=\"CLEANUP\" START_TIME=\"1237967170815\" SPLITS=\"\" .");
  writer.newLine();
  writer.write("MapAttempt TASK_TYPE=\"CLEANUP\" TASKID=\"task_200903250600_0034_m_000002\" TASK_ATTEMPT_ID=\"attempt_200903250600_0034_m_000002_0\" START_TIME=\"1237967168653\" TRACKER_NAME=\"tracker_3105\" HTTP_PORT=\"50060\" .");
  writer.newLine();
  writer.write("MapAttempt TASK_TYPE=\"CLEANUP\" TASKID=\"task_200903250600_0034_m_000002\" TASK_ATTEMPT_ID=\"attempt_200903250600_0034_m_000002_0\" TASK_STATUS=\"SUCCESS\" FINISH_TIME=\"1237967171301\" HOSTNAME=\"host.com\" STATE_STRING=\"cleanup\" COUNTERS=\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)]}\" .");
  writer.newLine();
  writer.write("Task TASKID=\"task_200903250600_0034_m_000002\" TASK_TYPE=\"CLEANUP\" TASK_STATUS=\"SUCCESS\" FINISH_TIME=\"1237967185818\" COUNTERS=\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)]}\" .");
  writer.newLine();
  writer.write("Job JOBID=\"job_200903250600_0034\" FINISH_TIME=\"1237967185818\" JOB_STATUS=\"KILLED\" FINISHED_MAPS=\"0\" FINISHED_REDUCES=\"0\" .");
  writer.newLine();
  writer.close();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.fs.slive.TestSlive </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
@Before public void ensureDeleted() throws Exception {
  rDelete(getTestFile());
  rDelete(getTestDir());
  rDelete(getTestRenameFile());
  rDelete(getResultFile());
  rDelete(getFlowLocation());
  rDelete(getImaginaryFile());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestControlledJob </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
@Before public void before() throws Exception {
  cluster.setUp();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestDebugScript </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
@Before public void setup() throws Exception {
  setupDebugScriptDirs();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestJvmManager </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
@Before public void setUp(){
  TEST_DIR.mkdirs();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestLinuxTaskController </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
@Before protected void setUp() throws Exception {
  testDir.mkdirs();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestShuffleJobToken </h4><pre class="type-3 type-13 type-7 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies assertions inside branch conditions">BranchVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
- Verifies assertions inside branch conditions
- Verifies boolean conditions
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Before public void setUp() throws Exception {
  dir=new File(System.getProperty("build.webapps","build/webapps") + "/test");
  System.out.println("dir=" + dir.getAbsolutePath());
  if (!dir.exists()) {
    assertTrue(dir.mkdirs());
  }
  server=new HttpServer("test","0.0.0.0",0,true);
  server.addServlet("shuffle","/mapOutput",TaskTracker.MapOutputServlet.class);
  server.start();
  int port=server.getPort();
  baseUrl=new URL("http://localhost:" + port + "/");
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestSimulatorTaskTracker </h4><pre class="type-3 type-14 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies (un)successful execution of the test case by reporting explicitly a failure">UtilityVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
- Verifies (un)successful execution of the test case by reporting explicitly a failure
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Before public void setUp(){
  try {
    jobTracker=new MockSimulatorJobTracker(simulationStartTime,heartbeatInterval,true);
  }
 catch (  Exception e) {
    Assert.fail("Couldn't set up the mock job tracker: " + e);
  }
  Configuration ttConf=new Configuration();
  ttConf.set("mumak.tasktracker.tracker.name",taskTrackerName);
  ttConf.set("mumak.tasktracker.host.name","test_host");
  ttConf.setInt("mapred.tasktracker.map.tasks.maximum",3);
  ttConf.setInt("mapred.tasktracker.reduce.tasks.maximum",3);
  ttConf.setInt("mumak.tasktracker.heartbeat.fuzz",-1);
  taskTracker=new SimulatorTaskTracker(jobTracker,ttConf);
  eventQueue=new CheckedEventQueue(simulationStartTime);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestSortValidate </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
@Before public void setUp() throws java.lang.Exception {
  cluster.setUp();
  client=cluster.getJTClient().getClient();
  dfs=client.getFs();
  dfs.delete(SORT_INPUT_PATH,true);
  dfs.delete(SORT_OUTPUT_PATH,true);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestTaskLogServlet </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
@Before public void setup() throws Exception {
  tester=new ServletTester();
  tester.setContextPath("/");
  tester.addServlet(TaskLogServlet.class,"/tasklog");
  tester.start();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestTaskTrackerDirectories </h4><pre class="type-3 type-7 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
- Verifies boolean conditions
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Before public void deleteTestDir() throws IOException {
  FileUtil.fullyDelete(new File(TEST_DIR));
  assertFalse("Could not delete " + TEST_DIR,new File(TEST_DIR).exists());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.TestTaskTrackerSlotManagement </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
/** 
 * Test-setup. Create the cache-file.
 * @throws Exception
 */
@Before public void setUp() throws Exception {
  new File(TEST_DIR.toString()).mkdirs();
  File myFile=new File(CACHE_FILE_PATH);
  myFile.createNewFile();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapred.tools.TestGetGroups </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
@Before public void setUpJobTracker() throws IOException, InterruptedException {
  cluster=new MiniMRCluster(0,"file:///",1);
  conf=cluster.createJobConf();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapreduce.TestJobACLs </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
/** 
 * Start the cluster before running the actual test.
 * @throws IOException
 */
@Before public void setup() throws Exception {
  startCluster(false);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapreduce.lib.input.TestMultipleInputs </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
@Before public void setUp() throws Exception {
  super.setUp();
  Path rootDir=getDir(ROOT_DIR);
  Path in1Dir=getDir(IN1_DIR);
  Path in2Dir=getDir(IN2_DIR);
  Configuration conf=createJobConf();
  FileSystem fs=FileSystem.get(conf);
  fs.delete(rootDir,true);
  if (!fs.mkdirs(in1Dir)) {
    throw new IOException("Mkdirs failed to create " + in1Dir.toString());
  }
  if (!fs.mkdirs(in2Dir)) {
    throw new IOException("Mkdirs failed to create " + in2Dir.toString());
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.mapreduce.security.token.delegation.TestDelegationToken </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
@Before public void setup() throws Exception {
  user1=UserGroupInformation.createUserForTesting("alice",new String[]{"users"});
  user2=UserGroupInformation.createUserForTesting("bob",new String[]{"users"});
  cluster=new MiniMRCluster(0,0,1,"file:///",1);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.security.TestMapredGroupMappingServiceRefresh </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
@Before public void setUp() throws Exception {
  config=new JobConf(new Configuration());
  config.setClass(CommonConfigurationKeys.HADOOP_SECURITY_GROUP_MAPPING,TestMapredGroupMappingServiceRefresh.MockUnixGroupsMapping.class,GroupMappingServiceProvider.class);
  config.setLong(CommonConfigurationKeys.HADOOP_SECURITY_GROUPS_CACHE_SECS,groupRefreshTimeoutSec);
  LOG.info("GROUP MAPPING class name=" + config.getClass(CommonConfigurationKeys.HADOOP_SECURITY_GROUP_MAPPING,ShellBasedUnixGroupsMapping.class,GroupMappingServiceProvider.class).getName());
  Groups.getUserToGroupsMappingService(config);
  String namenodeUrl="hdfs://localhost:" + "0";
  FileSystem.setDefaultUri(config,namenodeUrl);
  cluster=new MiniDFSCluster(0,config,1,true,true,true,null,null,null,null);
  cluster.waitActive();
  URI uri=cluster.getURI();
  MiniMRCluster miniMRCluster=new MiniMRCluster(0,uri.toString(),3,null,null,config);
  config.set(JTConfig.JT_IPC_ADDRESS,"localhost:" + miniMRCluster.getJobTrackerPort());
  ProxyUsers.refreshSuperUserGroupsConfiguration(config);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.streaming.TestFileArgs </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
@Before @Override public void setUp() throws IOException {
  FileSystem localFs=FileSystem.getLocal(conf);
  DataOutputStream dos=localFs.create(new Path("sidefile"));
  dos.write("hello world\n".getBytes("UTF-8"));
  dos.close();
  input="";
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.streaming.TestStreamXmlMultipleRecords </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
@Override @Before public void setUp() throws IOException {
  super.setUp();
  FileSystem.closeAll();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.streaming.TestStreaming </h4><pre class="type-3 type-7 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
- Verifies boolean conditions
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Before public void setUp() throws IOException {
  UtilTest.recursiveDelete(TEST_DIR);
  assertTrue("Creating " + TEST_DIR,TEST_DIR.mkdirs());
  args.clear();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.streaming.TestStreamingBackground </h4><pre class="type-3 type-7 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
- Verifies boolean conditions
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Before public void setUp() throws IOException {
  UtilTest.recursiveDelete(TEST_DIR);
  assertTrue(TEST_DIR.mkdirs());
  FileOutputStream out=new FileOutputStream(INPUT_FILE.getAbsoluteFile());
  out.write("hello\n".getBytes());
  out.close();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.streaming.TestStreamingExitStatus </h4><pre class="type-3 type-7 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies boolean conditions">BooleanVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
- Verifies boolean conditions
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Before public void setUp() throws IOException {
  UtilTest.recursiveDelete(TEST_DIR);
  assertTrue(TEST_DIR.mkdirs());
  FileOutputStream out=new FileOutputStream(INPUT_FILE.getAbsoluteFile());
  out.write("hello\n".getBytes());
  out.close();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
@Before @Override public void setUp() throws IOException {
  args.clear();
  super.setUp();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.streaming.TestStreamingStatus </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
/** 
 * Start the cluster and create input file before running the actual test.
 * @throws IOException
 */
@Before public void setUp() throws IOException {
  conf=new JobConf();
  conf.setBoolean(JTConfig.JT_RETIREJOBS,false);
  conf.setBoolean(JTConfig.JT_PERSIST_JOBSTATUS,false);
  mr=new MiniMRCluster(1,"file:///",3,null,null,conf);
  Path inFile=new Path(INPUT_FILE);
  fs=inFile.getFileSystem(mr.createJobConf());
  clean(fs);
  buildExpectedJobOutput();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.streaming.TestTypedBytesStreaming </h4><pre class="type-2 type-3 type-8 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Releases resources used by the test cases">TestCleaner</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Contains more than 2 JUnit-based stereotypes">HybridVerifier</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Releases resources used by the test cases
- Allocates resources before the execution of the test cases
- Contains more than 2 JUnit-based stereotypes
"></span><br>
@Before @After public void cleanupOutput() throws Exception {
  FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());
  INPUT_FILE.delete();
  createInput();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.tools.rumen.TestZombieJob </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
@Before public void setUp() throws Exception {
  final Configuration conf=new Configuration();
  final FileSystem lfs=FileSystem.getLocal(conf);
  final Path rootInputDir=new Path(System.getProperty("test.tools.input.dir","")).makeQualified(lfs);
  final Path rootInputFile=new Path(rootInputDir,"rumen/zombie");
  ZombieJobProducer parser=new ZombieJobProducer(new Path(rootInputFile,"input-trace.json"),new ZombieCluster(new Path(rootInputFile,"input-topology.json"),null,conf),conf);
  JobStory job=null;
  for (int i=0; i < 4; i++) {
    job=parser.getNextJob();
    ZombieJob zJob=(ZombieJob)job;
    LoggedJob loggedJob=zJob.getLoggedJob();
    System.out.println(i + ":" + job.getNumberMaps()+ "m, "+ job.getNumberReduces()+ "r");
    System.out.println(loggedJob.getOutcome() + ", " + loggedJob.getJobtype());
    System.out.println("Input Splits -- " + job.getInputSplits().length + ", "+ job.getNumberMaps());
    System.out.println("Successful Map CDF -------");
    for (    LoggedDiscreteCDF cdf : loggedJob.getSuccessfulMapAttemptCDFs()) {
      System.out.println(cdf.getNumberValues() + ": " + cdf.getMinimum()+ "--"+ cdf.getMaximum());
      for (      LoggedSingleRelativeRanking ranking : cdf.getRankings()) {
        System.out.println("   " + ranking.getRelativeRanking() + ":"+ ranking.getDatum());
      }
    }
    System.out.println("Failed Map CDF -----------");
    for (    LoggedDiscreteCDF cdf : loggedJob.getFailedMapAttemptCDFs()) {
      System.out.println(cdf.getNumberValues() + ": " + cdf.getMinimum()+ "--"+ cdf.getMaximum());
      for (      LoggedSingleRelativeRanking ranking : cdf.getRankings()) {
        System.out.println("   " + ranking.getRelativeRanking() + ":"+ ranking.getDatum());
      }
    }
    System.out.println("Successful Reduce CDF ----");
    LoggedDiscreteCDF cdf=loggedJob.getSuccessfulReduceAttemptCDF();
    System.out.println(cdf.getNumberValues() + ": " + cdf.getMinimum()+ "--"+ cdf.getMaximum());
    for (    LoggedSingleRelativeRanking ranking : cdf.getRankings()) {
      System.out.println("   " + ranking.getRelativeRanking() + ":"+ ranking.getDatum());
    }
    System.out.println("Failed Reduce CDF --------");
    cdf=loggedJob.getFailedReduceAttemptCDF();
    System.out.println(cdf.getNumberValues() + ": " + cdf.getMinimum()+ "--"+ cdf.getMaximum());
    for (    LoggedSingleRelativeRanking ranking : cdf.getRankings()) {
      System.out.println("   " + ranking.getRelativeRanking() + ":"+ ranking.getDatum());
    }
    System.out.print("map attempts to success -- ");
    for (    double p : loggedJob.getMapperTriesToSucceed()) {
      System.out.print(p + ", ");
    }
    System.out.println();
    System.out.println("===============");
    loggedJobs.add(loggedJob);
    jobStories.add(job);
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.typedbytes.TestIO </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
@Before public void setUp() throws Exception {
  this.tmpdir=new File(System.getProperty("test.build.data","/tmp"));
  if (this.tmpdir.exists() || this.tmpdir.mkdirs()) {
    this.tmpfile=new File(this.tmpdir,"typedbytes.bin");
  }
 else {
    throw new IOException("Failed to create directory " + tmpdir.getAbsolutePath());
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.util.TestReflectionUtils </h4><pre class="type-3 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Allocates resources before the execution of the test cases">TestInitializer</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Allocates resources before the execution of the test cases
"></span><br>
@Before public void setUp(){
  ReflectionUtils.clearCache();
}

</code></pre>

<br>
<script>$(document).ready(function() {
  $('pre code').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});</script>
