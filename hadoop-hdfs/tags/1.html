<h3><span class=" glyphicon glyphicon-tag"/>&nbspExecutionTester</h3><kbd>Executes methods or other tests from the same test unit</kbd><br><br><br><h4 style="margin:0px">Class: TestFuseDFS </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test basic file creation and writing 
 */
@Test public void testCreate() throws IOException {
  final String contents="hello world";
  File f=new File(mountPoint,"file1");
  createFile(f,contents);
  try {
    Thread.sleep(1000);
  }
 catch (  InterruptedException ie) {
  }
  checkFile(f,contents);
  execAssertSucceeds("cat " + f.getAbsolutePath());
  execAssertSucceeds("stat " + f.getAbsolutePath());
  execAssertSucceeds("rm " + f.getAbsolutePath());
  execAssertFails("ls " + f.getAbsolutePath());
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test creating a file via touch 
 */
@Test public void testTouch() throws IOException {
  File f=new File(mountPoint,"file1");
  execAssertSucceeds("touch " + f.getAbsolutePath());
  execAssertSucceeds("rm " + f.getAbsolutePath());
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test basic directory creation, access, removal 
 */
@Test public void testBasicDir() throws IOException {
  File d=new File(mountPoint,"dir1");
  execAssertSucceeds("mkdir " + d.getAbsolutePath());
  execAssertSucceeds("ls " + d.getAbsolutePath());
  execAssertSucceeds("rmdir " + d.getAbsolutePath());
  execAssertFails("ls " + d.getAbsolutePath());
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.fs.TestFiRename </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Rename test when src exists and dst does not 
 */
@Test public void testFailureNonExistentDst() throws Exception {
  final Path src=getTestPath("testFailureNonExistenSrc/dir/src");
  final Path dst=getTestPath("testFailureNonExistenSrc/newdir/dst");
  createFile(src);
  TestFiRename.exceptionOnRemove(src.toString());
  rename(src,dst,true,true,false,Rename.NONE);
  TestFiRename.exceptionOnAdd(dst.toString());
  rename(src,dst,true,true,false,Rename.NONE);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Rename test when src and dst exist 
 */
@Test public void testFailuresExistingDst() throws Exception {
  final Path src=getTestPath("testFailuresExistingDst/dir/src");
  final Path dst=getTestPath("testFailuresExistingDst/newdir/dst");
  createFile(src);
  createFile(dst);
  TestFiRename.exceptionOnRemove(src.toString());
  rename(src,dst,true,true,true,Rename.OVERWRITE);
  TestFiRename.exceptionOnRemove(dst.toString());
  rename(src,dst,true,true,true,Rename.OVERWRITE);
  TestFiRename.exceptionOnAdd(dst.toString());
  rename(src,dst,true,true,true,Rename.OVERWRITE);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.fs.TestHDFSFileContextMainOperations </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testRenameWithQuota() throws Exception {
  DistributedFileSystem fs=(DistributedFileSystem)cluster.getFileSystem();
  Path src1=getTestRootPath(fc,"test/testRenameWithQuota/srcdir/src1");
  Path src2=getTestRootPath(fc,"test/testRenameWithQuota/srcdir/src2");
  Path dst1=getTestRootPath(fc,"test/testRenameWithQuota/dstdir/dst1");
  Path dst2=getTestRootPath(fc,"test/testRenameWithQuota/dstdir/dst2");
  createFile(src1);
  createFile(src2);
  fs.setQuota(src1.getParent(),FSConstants.QUOTA_DONT_SET,FSConstants.QUOTA_DONT_SET);
  fc.mkdir(dst1.getParent(),FileContext.DEFAULT_PERM,true);
  fs.setQuota(dst1.getParent(),2,FSConstants.QUOTA_DONT_SET);
  rename(src1,dst1,false,true,false,Rename.NONE);
  rename(src2,dst1,true,true,false,Rename.OVERWRITE);
  createFile(src2);
  rename(src2,dst2,false,false,true,Rename.NONE);
  fs.setQuota(src1.getParent(),1,FSConstants.QUOTA_DONT_SET);
  rename(dst1,src1,false,false,true,Rename.NONE);
  fs.setQuota(src1.getParent(),100,FSConstants.QUOTA_DONT_SET);
  createFile(src1);
  fs.setQuota(src1.getParent(),1,FSConstants.QUOTA_DONT_SET);
  rename(dst1,src1,true,true,false,Rename.OVERWRITE);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testOldRenameWithQuota() throws Exception {
  DistributedFileSystem fs=(DistributedFileSystem)cluster.getFileSystem();
  Path src1=getTestRootPath(fc,"test/testOldRenameWithQuota/srcdir/src1");
  Path src2=getTestRootPath(fc,"test/testOldRenameWithQuota/srcdir/src2");
  Path dst1=getTestRootPath(fc,"test/testOldRenameWithQuota/dstdir/dst1");
  Path dst2=getTestRootPath(fc,"test/testOldRenameWithQuota/dstdir/dst2");
  createFile(src1);
  createFile(src2);
  fs.setQuota(src1.getParent(),FSConstants.QUOTA_DONT_SET,FSConstants.QUOTA_DONT_SET);
  fc.mkdir(dst1.getParent(),FileContext.DEFAULT_PERM,true);
  fs.setQuota(dst1.getParent(),2,FSConstants.QUOTA_DONT_SET);
  oldRename(src1,dst1,true,false);
  oldRename(src2,dst2,false,true);
  fs.setQuota(src1.getParent(),1,FSConstants.QUOTA_DONT_SET);
  oldRename(dst1,src1,false,true);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testRenameRoot() throws Exception {
  Path src=getTestRootPath(fc,"test/testRenameRoot/srcdir/src1");
  Path dst=new Path("/");
  createFile(src);
  rename(src,dst,true,false,true,Rename.OVERWRITE);
  rename(dst,src,true,false,true,Rename.OVERWRITE);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.fs.TestResolveHdfsSymlink </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Tests delegation token APIs in FileContext for Hdfs; and renew and cancel
 * APIs in Hdfs.
 * @throws UnsupportedFileSystemException
 * @throws IOException
 * @throws InterruptedException
 */
@SuppressWarnings("unchecked") @Test public void testFcDelegationToken() throws UnsupportedFileSystemException, IOException, InterruptedException {
  FileContext fcHdfs=FileContext.getFileContext(cluster.getFileSystem().getUri());
  final AbstractFileSystem afs=fcHdfs.getDefaultFileSystem();
  final List<Token<?>> tokenList=afs.getDelegationTokens(UserGroupInformation.getCurrentUser().getUserName());
  ((Hdfs)afs).renewDelegationToken((Token<DelegationTokenIdentifier>)tokenList.get(0));
  ((Hdfs)afs).cancelDelegationToken((Token<? extends AbstractDelegationTokenIdentifier>)tokenList.get(0));
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.FileAppendTest4 </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Comprehensive test for append 
 * @throws IOException an exception might be thrown
 */
@Test public void testAppend() throws IOException {
  final int maxOldFileLen=2 * BLOCK_SIZE + 1;
  final int maxFlushedBytes=BLOCK_SIZE;
  byte[] contents=AppendTestUtil.initBuffer(maxOldFileLen + 2 * maxFlushedBytes);
  for (int oldFileLen=0; oldFileLen <= maxOldFileLen; oldFileLen++) {
    for (int flushedBytes1=0; flushedBytes1 <= maxFlushedBytes; flushedBytes1++) {
      for (int flushedBytes2=0; flushedBytes2 <= maxFlushedBytes; flushedBytes2++) {
        final int fileLen=oldFileLen + flushedBytes1 + flushedBytes2;
        final Path p=new Path("foo" + oldFileLen + "_"+ flushedBytes1+ "_"+ flushedBytes2);
        LOG.info("Creating file " + p);
        FSDataOutputStream out=fs.create(p,false,conf.getInt("io.file.buffer.size",4096),REPLICATION,BLOCK_SIZE);
        out.write(contents,0,oldFileLen);
        out.close();
        out=fs.append(p);
        out.write(contents,oldFileLen,flushedBytes1);
        out.hflush();
        out.write(contents,oldFileLen + flushedBytes1,flushedBytes2);
        out.close();
        AppendTestUtil.checkFullFile(fs,p,fileLen,contents,p.toString());
        fs.delete(p,false);
      }
    }
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.TestAbandonBlock </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testAbandonBlock() throws IOException {
  String src=FILE_NAME_PREFIX + "foo";
  FSDataOutputStream fout=fs.create(new Path(src),true,4096,(short)1,512L);
  for (int i=0; i < 1024; i++) {
    fout.write(123);
  }
  fout.hflush();
  DFSClient dfsclient=((DistributedFileSystem)fs).getClient();
  LocatedBlocks blocks=dfsclient.getNamenode().getBlockLocations(src,0,1);
  LocatedBlock b=blocks.getLastLocatedBlock();
  dfsclient.getNamenode().abandonBlock(b.getBlock(),src,dfsclient.clientName);
  fout.close();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.TestClientBlockVerification </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Verify that if we read an entire block, we send CHECKSUM_OK
 */
@Test public void testBlockVerification() throws Exception {
  BlockReader reader=spy(util.getBlockReader(testBlock,0,FILE_SIZE_K * 1024));
  util.readAndCheckEOS(reader,FILE_SIZE_K * 1024,true);
  verify(reader).sendReadResult(reader.dnSock,Status.CHECKSUM_OK);
  reader.close();
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test various unaligned reads to make sure that we properly
 * account even when we don't start or end on a checksum boundary
 */
@Test public void testUnalignedReads() throws Exception {
  int startOffsets[]=new int[]{0,3,129};
  int lengths[]=new int[]{30,300,512,513,1025};
  for (  int startOffset : startOffsets) {
    for (    int length : lengths) {
      DFSClient.LOG.info("Testing startOffset = " + startOffset + " and "+ " len="+ length);
      BlockReader reader=spy(util.getBlockReader(testBlock,startOffset,length));
      util.readAndCheckEOS(reader,length,true);
      verify(reader).sendReadResult(reader.dnSock,Status.CHECKSUM_OK);
      reader.close();
    }
  }
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test that if we ask for a half block, and read it all, we *do
 * send CHECKSUM_OK. The DN takes care of knowing whether it was
 * the whole block or not.
 */
@Test public void testCompletePartialRead() throws Exception {
  BlockReader reader=spy(util.getBlockReader(testBlock,0,FILE_SIZE_K * 1024 / 2));
  util.readAndCheckEOS(reader,FILE_SIZE_K * 1024 / 2,true);
  verify(reader).sendReadResult(reader.dnSock,Status.CHECKSUM_OK);
  reader.close();
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test that if we do an incomplete read, we don't call CHECKSUM_OK
 */
@Test public void testIncompleteRead() throws Exception {
  BlockReader reader=spy(util.getBlockReader(testBlock,0,FILE_SIZE_K * 1024));
  util.readAndCheckEOS(reader,FILE_SIZE_K / 2 * 1024,false);
  verify(reader,never()).sendReadResult(reader.dnSock,Status.CHECKSUM_OK);
  reader.close();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.TestClientReportBadBlock </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testOneBlockReplica() throws Exception {
  final short repl=1;
  final int corruptBlockNumber=1;
  for (int i=0; i < 2; i++) {
    String fileName="/tmp/testClientReportBadBlock/OneBlockReplica" + i;
    Path filePath=new Path(fileName);
    createAFileWithCorruptedBlockReplicas(filePath,repl,corruptBlockNumber);
    if (i == 0) {
      dfsClientReadFile(filePath);
    }
 else {
      dfsClientReadFileFromPosition(filePath);
    }
    int expectedReplicaCount=1;
    verifyCorruptedBlockCount(filePath,expectedReplicaCount);
    verifyFirstBlockCorrupted(filePath,true);
    verifyFsckBlockCorrupted();
    testFsckListCorruptFilesBlocks(filePath,-1);
  }
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * This test creates a file with three block replicas. Corrupt two of the
 * replicas. Make dfs client read the file. The corrupted blocks with their
 * owner data nodes should be reported to the name node. 
 */
@Test public void testCorruptTwoOutOfThreeReplicas() throws Exception {
  final short repl=3;
  final int corruptBlocReplicas=2;
  for (int i=0; i < 2; i++) {
    String fileName="/tmp/testClientReportBadBlock/CorruptTwoOutOfThreeReplicas" + i;
    Path filePath=new Path(fileName);
    createAFileWithCorruptedBlockReplicas(filePath,repl,corruptBlocReplicas);
    int replicaCount=0;
    while (replicaCount != repl - corruptBlocReplicas) {
      if (i == 0) {
        dfsClientReadFile(filePath);
      }
 else {
        dfsClientReadFileFromPosition(filePath);
      }
      LocatedBlocks blocks=dfs.dfs.getNamenode().getBlockLocations(filePath.toString(),0,Long.MAX_VALUE);
      replicaCount=blocks.get(0).getLocations().length;
    }
    verifyFirstBlockCorrupted(filePath,false);
    int expectedReplicaCount=repl - corruptBlocReplicas;
    verifyCorruptedBlockCount(filePath,expectedReplicaCount);
    verifyFsckHealth("Target Replicas is 3 but found 1 replica");
    testFsckListCorruptFilesBlocks(filePath,0);
  }
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * This test creates a file with three block replicas. Corrupt all of the
 * replicas. Make dfs client read the file. No block corruption should be
 * reported.
 */
@Test public void testCorruptAllOfThreeReplicas() throws Exception {
  final short repl=3;
  final int corruptBlockNumber=3;
  for (int i=0; i < 2; i++) {
    String fileName="/tmp/testClientReportBadBlock/testCorruptAllReplicas" + i;
    Path filePath=new Path(fileName);
    createAFileWithCorruptedBlockReplicas(filePath,repl,corruptBlockNumber);
    if (i == 0) {
      dfsClientReadFile(filePath);
    }
 else {
      dfsClientReadFileFromPosition(filePath);
    }
    int expectedReplicasReturned=repl;
    verifyCorruptedBlockCount(filePath,expectedReplicasReturned);
    verifyFirstBlockCorrupted(filePath,false);
    verifyFsckHealth("");
    testFsckListCorruptFilesBlocks(filePath,0);
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.TestConnCache </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Read a file served entirely from one DN. Seek around and read from
 * different offsets. And verify that they all use the same socket.
 * @throws java.io.IOException
 */
@Test @SuppressWarnings("unchecked") public void testReadFromOneDN() throws IOException {
  LOG.info("Starting testReadFromOneDN()");
  DFSClient client=new DFSClient(new InetSocketAddress("localhost",cluster.getNameNodePort()),conf);
  DFSInputStream in=spy(client.open(testFile.toString()));
  LOG.info("opened " + testFile.toString());
  byte[] dataBuf=new byte[BLOCK_SIZE];
  MockGetBlockReader answer=new MockGetBlockReader();
  Mockito.doAnswer(answer).when(in).getBlockReader((InetSocketAddress)Matchers.anyObject(),Matchers.anyString(),(ExtendedBlock)Matchers.anyObject(),(Token<BlockTokenIdentifier>)Matchers.anyObject(),Matchers.anyLong(),Matchers.anyLong(),Matchers.anyInt(),Matchers.anyBoolean(),Matchers.anyString());
  pread(in,0,dataBuf,0,dataBuf.length);
  pread(in,FILE_SIZE - dataBuf.length,dataBuf,0,dataBuf.length);
  pread(in,1024,dataBuf,0,dataBuf.length);
  pread(in,-1,dataBuf,0,dataBuf.length);
  pread(in,64,dataBuf,0,dataBuf.length / 2);
  in.close();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.TestCrcCorruption </h4><pre class="type-14 type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Invokes logging operations">Logger</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Invokes logging operations
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testCrcCorruption() throws Exception {
  System.out.println("TestCrcCorruption with default parameters");
  Configuration conf1=new HdfsConfiguration();
  conf1.setInt(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY,3 * 1000);
  DFSTestUtil util1=new DFSTestUtil("TestCrcCorruption",40,3,8 * 1024);
  thistest(conf1,util1);
  System.out.println("TestCrcCorruption with specific parameters");
  Configuration conf2=new HdfsConfiguration();
  conf2.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,17);
  conf2.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,34);
  DFSTestUtil util2=new DFSTestUtil("TestCrcCorruption",40,3,400);
  thistest(conf2,util2);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Same thing with multiple datanodes - in history, this has
 * behaved differently than the above.
 * This test usually completes in around 15 seconds - if it
 * times out, this suggests that the client is retrying
 * indefinitely.
 */
@Test(timeout=300000) public void testEntirelyCorruptFileThreeNodes() throws Exception {
  doTestEntirelyCorruptFile(3);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Make a single-DN cluster, corrupt a block, and make sure
 * there's no infinite loop, but rather it eventually
 * reports the exception to the client.
 */
@Test(timeout=300000) public void testEntirelyCorruptFileOneNode() throws Exception {
  doTestEntirelyCorruptFile(1);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.TestDFSUpgrade </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * This test attempts to upgrade the NameNode and DataNode under
 * a number of valid and invalid conditions.
 */
@Test public void testUpgrade() throws Exception {
  File[] baseDirs;
  UpgradeUtilities.initialize();
  StorageInfo storageInfo=null;
  for (int numDirs=1; numDirs <= 2; numDirs++) {
    conf=new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_DATANODE_SCAN_PERIOD_HOURS_KEY,-1);
    conf=UpgradeUtilities.initializeStorageStateConf(numDirs,conf);
    String[] nameNodeDirs=conf.getStrings(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY);
    String[] dataNodeDirs=conf.getStrings(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY);
    log("Normal NameNode upgrade",numDirs);
    UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs,"current");
    cluster=createCluster();
    checkNameNode(nameNodeDirs);
    if (numDirs > 1)     TestParallelImageWrite.checkImages(cluster.getNamesystem(),numDirs);
    cluster.shutdown();
    UpgradeUtilities.createEmptyDirs(nameNodeDirs);
    log("Normal DataNode upgrade",numDirs);
    UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs,"current");
    cluster=createCluster();
    UpgradeUtilities.createDataNodeStorageDirs(dataNodeDirs,"current");
    cluster.startDataNodes(conf,1,false,StartupOption.REGULAR,null);
    checkDataNode(dataNodeDirs,UpgradeUtilities.getCurrentBlockPoolID(null));
    cluster.shutdown();
    UpgradeUtilities.createEmptyDirs(nameNodeDirs);
    UpgradeUtilities.createEmptyDirs(dataNodeDirs);
    log("NameNode upgrade with existing previous dir",numDirs);
    UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs,"current");
    UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs,"previous");
    startNameNodeShouldFail(StartupOption.UPGRADE);
    UpgradeUtilities.createEmptyDirs(nameNodeDirs);
    log("DataNode upgrade with existing previous dir",numDirs);
    UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs,"current");
    cluster=createCluster();
    UpgradeUtilities.createDataNodeStorageDirs(dataNodeDirs,"current");
    UpgradeUtilities.createDataNodeStorageDirs(dataNodeDirs,"previous");
    cluster.startDataNodes(conf,1,false,StartupOption.REGULAR,null);
    checkDataNode(dataNodeDirs,UpgradeUtilities.getCurrentBlockPoolID(null));
    cluster.shutdown();
    UpgradeUtilities.createEmptyDirs(nameNodeDirs);
    UpgradeUtilities.createEmptyDirs(dataNodeDirs);
    log("DataNode upgrade with future stored layout version in current",numDirs);
    UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs,"current");
    cluster=createCluster();
    baseDirs=UpgradeUtilities.createDataNodeStorageDirs(dataNodeDirs,"current");
    storageInfo=new StorageInfo(Integer.MIN_VALUE,UpgradeUtilities.getCurrentNamespaceID(cluster),UpgradeUtilities.getCurrentClusterID(cluster),UpgradeUtilities.getCurrentFsscTime(cluster));
    UpgradeUtilities.createDataNodeVersionFile(baseDirs,storageInfo,UpgradeUtilities.getCurrentBlockPoolID(cluster));
    startBlockPoolShouldFail(StartupOption.REGULAR,UpgradeUtilities.getCurrentBlockPoolID(null));
    cluster.shutdown();
    UpgradeUtilities.createEmptyDirs(nameNodeDirs);
    UpgradeUtilities.createEmptyDirs(dataNodeDirs);
    log("DataNode upgrade with newer fsscTime in current",numDirs);
    UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs,"current");
    cluster=createCluster();
    baseDirs=UpgradeUtilities.createDataNodeStorageDirs(dataNodeDirs,"current");
    storageInfo=new StorageInfo(UpgradeUtilities.getCurrentLayoutVersion(),UpgradeUtilities.getCurrentNamespaceID(cluster),UpgradeUtilities.getCurrentClusterID(cluster),Long.MAX_VALUE);
    UpgradeUtilities.createDataNodeVersionFile(baseDirs,storageInfo,UpgradeUtilities.getCurrentBlockPoolID(cluster));
    startBlockPoolShouldFail(StartupOption.REGULAR,UpgradeUtilities.getCurrentBlockPoolID(null));
    cluster.shutdown();
    UpgradeUtilities.createEmptyDirs(nameNodeDirs);
    UpgradeUtilities.createEmptyDirs(dataNodeDirs);
    log("NameNode upgrade with no edits file",numDirs);
    baseDirs=UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs,"current");
    for (    File f : baseDirs) {
      FileUtil.fullyDelete(new File(f,"edits"));
    }
    startNameNodeShouldFail(StartupOption.UPGRADE);
    UpgradeUtilities.createEmptyDirs(nameNodeDirs);
    log("NameNode upgrade with no image file",numDirs);
    baseDirs=UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs,"current");
    for (    File f : baseDirs) {
      FileUtil.fullyDelete(new File(f,"fsimage"));
    }
    startNameNodeShouldFail(StartupOption.UPGRADE);
    UpgradeUtilities.createEmptyDirs(nameNodeDirs);
    log("NameNode upgrade with corrupt version file",numDirs);
    baseDirs=UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs,"current");
    for (    File f : baseDirs) {
      UpgradeUtilities.corruptFile(new File(f,"VERSION"));
    }
    startNameNodeShouldFail(StartupOption.UPGRADE);
    UpgradeUtilities.createEmptyDirs(nameNodeDirs);
    log("NameNode upgrade with old layout version in current",numDirs);
    baseDirs=UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs,"current");
    storageInfo=new StorageInfo(Storage.LAST_UPGRADABLE_LAYOUT_VERSION + 1,UpgradeUtilities.getCurrentNamespaceID(null),UpgradeUtilities.getCurrentClusterID(null),UpgradeUtilities.getCurrentFsscTime(null));
    UpgradeUtilities.createNameNodeVersionFile(conf,baseDirs,storageInfo,UpgradeUtilities.getCurrentBlockPoolID(cluster));
    startNameNodeShouldFail(StartupOption.UPGRADE);
    UpgradeUtilities.createEmptyDirs(nameNodeDirs);
    log("NameNode upgrade with future layout version in current",numDirs);
    baseDirs=UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs,"current");
    storageInfo=new StorageInfo(Integer.MIN_VALUE,UpgradeUtilities.getCurrentNamespaceID(null),UpgradeUtilities.getCurrentClusterID(null),UpgradeUtilities.getCurrentFsscTime(null));
    UpgradeUtilities.createNameNodeVersionFile(conf,baseDirs,storageInfo,UpgradeUtilities.getCurrentBlockPoolID(cluster));
    startNameNodeShouldFail(StartupOption.UPGRADE);
    UpgradeUtilities.createEmptyDirs(nameNodeDirs);
  }
  int numDirs=4;
{
    conf=new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_DATANODE_SCAN_PERIOD_HOURS_KEY,-1);
    conf=UpgradeUtilities.initializeStorageStateConf(numDirs,conf);
    String[] nameNodeDirs=conf.getStrings(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY);
    log("Normal NameNode upgrade",numDirs);
    UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs,"current");
    cluster=createCluster();
    checkNameNode(nameNodeDirs);
    TestParallelImageWrite.checkImages(cluster.getNamesystem(),numDirs);
    cluster.shutdown();
    UpgradeUtilities.createEmptyDirs(nameNodeDirs);
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.TestDataTransferProtocol </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testOpWrite() throws IOException {
  int numDataNodes=1;
  Configuration conf=new HdfsConfiguration();
  conf.setBoolean("dfs.support.append",true);
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();
  try {
    cluster.waitActive();
    String poolId=cluster.getNamesystem().getBlockPoolId();
    datanode=DataNodeTestUtils.getDNRegistrationForBP(cluster.getDataNodes().get(0),poolId);
    dnAddr=NetUtils.createSocketAddr(datanode.getName());
    FileSystem fileSys=cluster.getFileSystem();
    Path file=new Path("dataprotocol.dat");
    DFSTestUtil.createFile(fileSys,file,1L,(short)numDataNodes,0L);
    ExtendedBlock firstBlock=DFSTestUtil.getFirstBlock(fileSys,file);
    testWrite(firstBlock,BlockConstructionStage.PIPELINE_SETUP_CREATE,0L,"Cannot create an existing block",true);
    testWrite(firstBlock,BlockConstructionStage.DATA_STREAMING,0L,"Unexpected stage",true);
    long newGS=firstBlock.getGenerationStamp() + 1;
    testWrite(firstBlock,BlockConstructionStage.PIPELINE_SETUP_STREAMING_RECOVERY,newGS,"Cannot recover data streaming to a finalized replica",true);
    newGS=firstBlock.getGenerationStamp() + 1;
    testWrite(firstBlock,BlockConstructionStage.PIPELINE_SETUP_APPEND,newGS,"Append to a finalized replica",false);
    firstBlock.setGenerationStamp(newGS);
    file=new Path("dataprotocol1.dat");
    DFSTestUtil.createFile(fileSys,file,1L,(short)numDataNodes,0L);
    firstBlock=DFSTestUtil.getFirstBlock(fileSys,file);
    newGS=firstBlock.getGenerationStamp() + 1;
    testWrite(firstBlock,BlockConstructionStage.PIPELINE_SETUP_APPEND_RECOVERY,newGS,"Recover appending to a finalized replica",false);
    file=new Path("dataprotocol2.dat");
    DFSTestUtil.createFile(fileSys,file,1L,(short)numDataNodes,0L);
    firstBlock=DFSTestUtil.getFirstBlock(fileSys,file);
    newGS=firstBlock.getGenerationStamp() + 1;
    testWrite(firstBlock,BlockConstructionStage.PIPELINE_CLOSE_RECOVERY,newGS,"Recover failed close to a finalized replica",false);
    firstBlock.setGenerationStamp(newGS);
    long newBlockId=firstBlock.getBlockId() + 1;
    ExtendedBlock newBlock=new ExtendedBlock(firstBlock.getBlockPoolId(),newBlockId,0,firstBlock.getGenerationStamp());
    testWrite(newBlock,BlockConstructionStage.PIPELINE_SETUP_CREATE,0L,"Create a new block",false);
    newGS=newBlock.getGenerationStamp() + 1;
    newBlock.setBlockId(newBlock.getBlockId() + 1);
    testWrite(newBlock,BlockConstructionStage.PIPELINE_SETUP_STREAMING_RECOVERY,newGS,"Recover a new block",true);
    newGS=newBlock.getGenerationStamp() + 1;
    testWrite(newBlock,BlockConstructionStage.PIPELINE_SETUP_APPEND,newGS,"Cannot append to a new block",true);
    newBlock.setBlockId(newBlock.getBlockId() + 1);
    newGS=newBlock.getGenerationStamp() + 1;
    testWrite(newBlock,BlockConstructionStage.PIPELINE_SETUP_APPEND_RECOVERY,newGS,"Cannot append to a new block",true);
    Path file1=new Path("dataprotocol1.dat");
    DFSTestUtil.createFile(fileSys,file1,1L,(short)numDataNodes,0L);
    DFSOutputStream out=(DFSOutputStream)(fileSys.append(file1).getWrappedStream());
    out.write(1);
    out.hflush();
    FSDataInputStream in=fileSys.open(file1);
    firstBlock=DFSTestUtil.getAllBlocks(in).get(0).getBlock();
    firstBlock.setNumBytes(2L);
    try {
      testWrite(firstBlock,BlockConstructionStage.PIPELINE_SETUP_CREATE,0L,"Cannot create a RBW block",true);
      newGS=newBlock.getGenerationStamp() + 1;
      testWrite(firstBlock,BlockConstructionStage.PIPELINE_SETUP_APPEND,newGS,"Cannot append to a RBW replica",true);
      testWrite(firstBlock,BlockConstructionStage.PIPELINE_SETUP_APPEND_RECOVERY,newGS,"Recover append to a RBW replica",false);
      firstBlock.setGenerationStamp(newGS);
      file=new Path("dataprotocol2.dat");
      DFSTestUtil.createFile(fileSys,file,1L,(short)numDataNodes,0L);
      out=(DFSOutputStream)(fileSys.append(file).getWrappedStream());
      out.write(1);
      out.hflush();
      in=fileSys.open(file);
      firstBlock=DFSTestUtil.getAllBlocks(in).get(0).getBlock();
      firstBlock.setNumBytes(2L);
      newGS=firstBlock.getGenerationStamp() + 1;
      testWrite(firstBlock,BlockConstructionStage.PIPELINE_SETUP_STREAMING_RECOVERY,newGS,"Recover a RBW replica",false);
    }
  finally {
      IOUtils.closeStream(in);
      IOUtils.closeStream(out);
    }
  }
  finally {
    cluster.shutdown();
  }
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testDataTransferProtocol() throws IOException {
  Random random=new Random();
  int oneMil=1024 * 1024;
  Path file=new Path("dataprotocol.dat");
  int numDataNodes=1;
  Configuration conf=new HdfsConfiguration();
  conf.setInt(DFSConfigKeys.DFS_REPLICATION_KEY,numDataNodes);
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();
  try {
    cluster.waitActive();
    DFSClient dfsClient=new DFSClient(new InetSocketAddress("localhost",cluster.getNameNodePort()),conf);
    datanode=dfsClient.datanodeReport(DatanodeReportType.LIVE)[0];
    dnAddr=NetUtils.createSocketAddr(datanode.getName());
    FileSystem fileSys=cluster.getFileSystem();
    int fileLen=Math.min(conf.getInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,4096),4096);
    createFile(fileSys,file,fileLen);
    final ExtendedBlock firstBlock=DFSTestUtil.getFirstBlock(fileSys,file);
    final String poolId=firstBlock.getBlockPoolId();
    long newBlockId=firstBlock.getBlockId() + 1;
    recvBuf.reset();
    sendBuf.reset();
    recvOut.writeShort((short)(DataTransferProtocol.DATA_TRANSFER_VERSION - 1));
    sendOut.writeShort((short)(DataTransferProtocol.DATA_TRANSFER_VERSION - 1));
    sendRecvData("Wrong Version",true);
    sendBuf.reset();
    sendOut.writeShort((short)DataTransferProtocol.DATA_TRANSFER_VERSION);
    sendOut.writeByte(Op.WRITE_BLOCK.code - 1);
    sendRecvData("Wrong Op Code",true);
    sendBuf.reset();
    Sender.opWriteBlock(sendOut,new ExtendedBlock(poolId,newBlockId),0,BlockConstructionStage.PIPELINE_SETUP_CREATE,0L,0L,0L,"cl",null,new DatanodeInfo[1],BlockTokenSecretManager.DUMMY_TOKEN);
    sendOut.writeByte((byte)DataChecksum.CHECKSUM_CRC32);
    sendOut.writeInt(-1 - random.nextInt(oneMil));
    recvBuf.reset();
    sendResponse(Status.ERROR,null,recvOut);
    sendRecvData("wrong bytesPerChecksum while writing",true);
    sendBuf.reset();
    recvBuf.reset();
    Sender.opWriteBlock(sendOut,new ExtendedBlock(poolId,++newBlockId),0,BlockConstructionStage.PIPELINE_SETUP_CREATE,0L,0L,0L,"cl",null,new DatanodeInfo[1],BlockTokenSecretManager.DUMMY_TOKEN);
    sendOut.writeByte((byte)DataChecksum.CHECKSUM_CRC32);
    sendOut.writeInt(512);
    PacketHeader hdr=new PacketHeader(4,0,100,false,-1 - random.nextInt(oneMil));
    hdr.write(sendOut);
    sendResponse(Status.SUCCESS,"",recvOut);
    new PipelineAck(100,new Status[]{Status.ERROR}).write(recvOut);
    sendRecvData("negative DATA_CHUNK len while writing block " + newBlockId,true);
    sendBuf.reset();
    recvBuf.reset();
    Sender.opWriteBlock(sendOut,new ExtendedBlock(poolId,++newBlockId),0,BlockConstructionStage.PIPELINE_SETUP_CREATE,0L,0L,0L,"cl",null,new DatanodeInfo[1],BlockTokenSecretManager.DUMMY_TOKEN);
    sendOut.writeByte((byte)DataChecksum.CHECKSUM_CRC32);
    sendOut.writeInt(512);
    hdr=new PacketHeader(8,0,100,true,0);
    hdr.write(sendOut);
    sendOut.writeInt(0);
    sendOut.flush();
    sendResponse(Status.SUCCESS,"",recvOut);
    new PipelineAck(100,new Status[]{Status.SUCCESS}).write(recvOut);
    sendRecvData("Writing a zero len block blockid " + newBlockId,false);
    String bpid=cluster.getNamesystem().getBlockPoolId();
    ExtendedBlock blk=new ExtendedBlock(bpid,firstBlock.getLocalBlock());
    long blkid=blk.getBlockId();
    sendBuf.reset();
    recvBuf.reset();
    blk.setBlockId(blkid - 1);
    Sender.opReadBlock(sendOut,blk,0L,fileLen,"cl",BlockTokenSecretManager.DUMMY_TOKEN);
    sendRecvData("Wrong block ID " + newBlockId + " for read",false);
    sendBuf.reset();
    blk.setBlockId(blkid);
    Sender.opReadBlock(sendOut,blk,-1L,fileLen,"cl",BlockTokenSecretManager.DUMMY_TOKEN);
    sendRecvData("Negative start-offset for read for block " + firstBlock.getBlockId(),false);
    sendBuf.reset();
    Sender.opReadBlock(sendOut,blk,fileLen,fileLen,"cl",BlockTokenSecretManager.DUMMY_TOKEN);
    sendRecvData("Wrong start-offset for reading block " + firstBlock.getBlockId(),false);
    recvBuf.reset();
    sendResponse(Status.SUCCESS,null,recvOut);
    sendBuf.reset();
    Sender.opReadBlock(sendOut,blk,0L,-1 - random.nextInt(oneMil),"cl",BlockTokenSecretManager.DUMMY_TOKEN);
    sendRecvData("Negative length for reading block " + firstBlock.getBlockId(),false);
    recvBuf.reset();
    sendResponse(Status.ERROR,null,recvOut);
    sendBuf.reset();
    Sender.opReadBlock(sendOut,blk,0L,fileLen + 1,"cl",BlockTokenSecretManager.DUMMY_TOKEN);
    sendRecvData("Wrong length for reading block " + firstBlock.getBlockId(),false);
    sendBuf.reset();
    Sender.opReadBlock(sendOut,blk,0L,fileLen,"cl",BlockTokenSecretManager.DUMMY_TOKEN);
    readFile(fileSys,file,fileLen);
  }
  finally {
    cluster.shutdown();
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.TestDecommission </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Tests cluster storage statistics during decommissioning for non
 * federated cluster
 */
@Test public void testClusterStats() throws Exception {
  testClusterStats(1);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test decommission for federeated cluster
 */
@Test public void testDecommissionFederation() throws IOException {
  testDecommission(2,2);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Tests cluster storage statistics during decommissioning for
 * federated cluster
 */
@Test public void testClusterStatsFederation() throws Exception {
  testClusterStats(3);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test host/include file functionality. Only datanodes
 * in the include file are allowed to connect to the namenode in a non
 * federated cluster.
 */
@Test public void testHostsFile() throws IOException, InterruptedException {
  testHostsFile(1);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test host/include file functionality. Only datanodes
 * in the include file are allowed to connect to the namenode in a 
 * federated cluster.
 */
@Test public void testHostsFileFederation() throws IOException, InterruptedException {
  testHostsFile(3);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Tests decommission for non federated cluster
 */
@Test public void testDecommission() throws IOException {
  testDecommission(1,6);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.TestDistributedFileSystem </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testFileSystemCloseAll() throws Exception {
  Configuration conf=getTestConfiguration();
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).build();
  URI address=FileSystem.getDefaultUri(conf);
  try {
    FileSystem.closeAll();
    conf=getTestConfiguration();
    FileSystem.setDefaultUri(conf,address);
    FileSystem.get(conf);
    FileSystem.get(conf);
    FileSystem.closeAll();
  }
  finally {
    if (cluster != null) {
      cluster.shutdown();
    }
  }
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testAllWithDualPort() throws Exception {
  dualPortTesting=true;
  testFileSystemCloseAll();
  testDFSClose();
  testDFSClient();
  testFileChecksum();
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testStatistics() throws Exception {
  int lsLimit=2;
  final Configuration conf=getTestConfiguration();
  conf.setInt(DFSConfigKeys.DFS_LIST_LIMIT,lsLimit);
  final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).build();
  try {
    final FileSystem fs=cluster.getFileSystem();
    Path dir=new Path("/test");
    Path file=new Path(dir,"file");
    int readOps=DFSTestUtil.getStatistics(fs).getReadOps();
    int writeOps=DFSTestUtil.getStatistics(fs).getWriteOps();
    int largeReadOps=DFSTestUtil.getStatistics(fs).getLargeReadOps();
    fs.mkdirs(dir);
    checkStatistics(fs,readOps,++writeOps,largeReadOps);
    FSDataOutputStream out=fs.create(file,(short)1);
    out.close();
    checkStatistics(fs,readOps,++writeOps,largeReadOps);
    FileStatus status=fs.getFileStatus(file);
    checkStatistics(fs,++readOps,writeOps,largeReadOps);
    fs.getFileBlockLocations(file,0,0);
    checkStatistics(fs,++readOps,writeOps,largeReadOps);
    fs.getFileBlockLocations(status,0,0);
    checkStatistics(fs,++readOps,writeOps,largeReadOps);
    FSDataInputStream in=fs.open(file);
    in.close();
    checkStatistics(fs,++readOps,writeOps,largeReadOps);
    fs.setReplication(file,(short)2);
    checkStatistics(fs,readOps,++writeOps,largeReadOps);
    Path file1=new Path(dir,"file1");
    fs.rename(file,file1);
    checkStatistics(fs,readOps,++writeOps,largeReadOps);
    fs.getContentSummary(file1);
    checkStatistics(fs,++readOps,writeOps,largeReadOps);
    for (int i=0; i < 10; i++) {
      Path p=new Path(dir,Integer.toString(i));
      fs.mkdirs(p);
      FileStatus[] list=fs.listStatus(dir);
      if (list.length > lsLimit) {
        int iterations=(int)Math.ceil((double)list.length / lsLimit);
        largeReadOps+=iterations;
        readOps+=iterations;
      }
 else {
        readOps++;
      }
      checkStatistics(fs,readOps,++writeOps,largeReadOps);
    }
    fs.getStatus(file1);
    checkStatistics(fs,++readOps,writeOps,largeReadOps);
    fs.getFileChecksum(file1);
    checkStatistics(fs,++readOps,writeOps,largeReadOps);
    fs.setPermission(file1,new FsPermission((short)0777));
    checkStatistics(fs,readOps,++writeOps,largeReadOps);
    fs.setTimes(file1,0L,0L);
    checkStatistics(fs,readOps,++writeOps,largeReadOps);
    UserGroupInformation ugi=UserGroupInformation.getCurrentUser();
    fs.setOwner(file1,ugi.getUserName(),ugi.getGroupNames()[0]);
    checkStatistics(fs,readOps,++writeOps,largeReadOps);
    fs.delete(dir,true);
    checkStatistics(fs,readOps,++writeOps,largeReadOps);
  }
  finally {
    if (cluster != null)     cluster.shutdown();
  }
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Tests DFSClient.close throws no ConcurrentModificationException if 
 * multiple files are open.
 */
@Test public void testDFSClose() throws Exception {
  Configuration conf=getTestConfiguration();
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(2).build();
  FileSystem fileSys=cluster.getFileSystem();
  try {
    fileSys.create(new Path("/test/dfsclose/file-0"));
    fileSys.create(new Path("/test/dfsclose/file-1"));
    fileSys.close();
  }
  finally {
    if (cluster != null) {
      cluster.shutdown();
    }
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.TestFiHFlush </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Similar to {@link #hFlushFi01_b()} but for a pipeline's 1st datanode
 */
@Test public void hFlushFi02_b() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  Configuration conf=new HdfsConfiguration();
  int customPerChecksumSize=512;
  int customBlockSize=customPerChecksumSize * 3;
  conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,customPerChecksumSize);
  conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,customBlockSize);
  runDiskErrorTest(conf,methodName,customBlockSize,new DerrAction(methodName,1),1,true);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Similar to {@link #hFlushFi01_a()} but for a pipeline's 1st datanode
 */
@Test public void hFlushFi02_a() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runDiskErrorTest(new HdfsConfiguration(),methodName,AppendTestUtil.BLOCK_SIZE,new DerrAction(methodName,1),1,false);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Similar to {@link #hFlushFi01_b()} but writing happens
 * across block and checksum's boundaries
 */
@Test public void hFlushFi01_c() throws Exception {
  final String methodName=FiTestUtil.getMethodName();
  Configuration conf=new HdfsConfiguration();
  int customPerChecksumSize=400;
  int customBlockSize=customPerChecksumSize * 3;
  conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,customPerChecksumSize);
  conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,customBlockSize);
  runDiskErrorTest(conf,methodName,customBlockSize,new DerrAction(methodName,0),0,true);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Similar to {@link #hFlushFi01_b()} but for a pipeline's 2nd datanode
 */
@Test public void hFlushFi03_b() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  Configuration conf=new HdfsConfiguration();
  int customPerChecksumSize=512;
  int customBlockSize=customPerChecksumSize * 3;
  conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,customPerChecksumSize);
  conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,customBlockSize);
  runDiskErrorTest(conf,methodName,customBlockSize,new DerrAction(methodName,2),2,true);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Similar to {@link #hFlushFi01_c()} but for a pipeline's 1st datanode
 */
@Test public void hFlushFi02_c() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  Configuration conf=new HdfsConfiguration();
  int customPerChecksumSize=400;
  int customBlockSize=customPerChecksumSize * 3;
  conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,customPerChecksumSize);
  conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,customBlockSize);
  runDiskErrorTest(conf,methodName,customBlockSize,new DerrAction(methodName,1),1,true);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Similar to {@link #hFlushFi01_a()} but for a pipeline's 2nd datanode
 */
@Test public void hFlushFi03_a() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runDiskErrorTest(new HdfsConfiguration(),methodName,AppendTestUtil.BLOCK_SIZE,new DerrAction(methodName,2),2,false);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * The tests calls {@link #runDiskErrorTest(Configuration,String,int,DerrAction,int,boolean)}to make a number of writes across a block boundaries.
 * hflush() is called after each write() during a pipeline life time.
 * Thus, injected fault ought to be triggered for 0th datanode
 */
@Test public void hFlushFi01_b() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  Configuration conf=new HdfsConfiguration();
  int customPerChecksumSize=512;
  int customBlockSize=customPerChecksumSize * 3;
  conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,customPerChecksumSize);
  conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,customBlockSize);
  runDiskErrorTest(conf,methodName,customBlockSize,new DerrAction(methodName,0),0,true);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * The tests calls {@link #runDiskErrorTest(Configuration,String,int,DerrAction,int,boolean)}to make a number of writes within a block boundaries.
 * Although hflush() is called the test shouldn't expect an IOException
 * in this case because the invocation is happening after write() call 
 * is complete when pipeline doesn't exist anymore.
 * Thus, injected fault won't be triggered for 0th datanode
 */
@Test public void hFlushFi01_a() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runDiskErrorTest(new HdfsConfiguration(),methodName,AppendTestUtil.BLOCK_SIZE,new DerrAction(methodName,0),0,false);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Similar to {@link #hFlushFi01_c()} but for a pipeline's 2nd datanode
 */
@Test public void hFlushFi03_c() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  Configuration conf=new HdfsConfiguration();
  int customPerChecksumSize=400;
  int customBlockSize=customPerChecksumSize * 3;
  conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,customPerChecksumSize);
  conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,customBlockSize);
  runDiskErrorTest(conf,methodName,customBlockSize,new DerrAction(methodName,2),2,true);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.TestFiHftp </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testHftpOpen() throws IOException {
  final Configuration conf=new Configuration();
  MiniDFSCluster cluster=null;
  try {
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(DATANODE_NUM).build();
    cluster.waitActive();
{
      final long blocksize=1L << 20;
      final long filesize=2 * blocksize + 100;
      runTestHftpOpen(cluster,"/foo",blocksize,filesize);
    }
{
      final long blocksize=1L << 10;
      final long filesize=2 * blocksize + 100;
      runTestHftpOpen(cluster,"/small",blocksize,filesize);
    }
  }
  finally {
    if (cluster != null) {
      cluster.shutdown();
    }
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.TestFiPipelines </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test initiates and sets actions created by injection framework. The actions
 * work with both aspects of sending acknologment packets in a pipeline.
 * Creates and closes a file of certain length < packet size.
 * Injected actions will check if number of visible bytes at datanodes equals
 * to number of acknoleged bytes
 * @throws IOException in case of an error
 */
@Test public void pipeline_04() throws IOException {
  final String METHOD_NAME=GenericTestUtils.getMethodName();
  if (LOG.isDebugEnabled()) {
    LOG.debug("Running " + METHOD_NAME);
  }
  final PipelinesTestUtil.PipelinesTest pipst=(PipelinesTestUtil.PipelinesTest)PipelinesTestUtil.initTest();
  pipst.fiCallSetNumBytes.set(new PipelinesTestUtil.ReceivedCheckAction(METHOD_NAME));
  pipst.fiCallSetBytesAcked.set(new PipelinesTestUtil.AckedCheckAction(METHOD_NAME));
  Path filePath=new Path("/" + METHOD_NAME + ".dat");
  FSDataOutputStream fsOut=fs.create(filePath);
  TestPipelines.writeData(fsOut,2);
  fs.close();
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Similar to pipeline_04 but sends many packets into a pipeline 
 * @throws IOException in case of an error
 */
@Test public void pipeline_05() throws IOException {
  final String METHOD_NAME=GenericTestUtils.getMethodName();
  if (LOG.isDebugEnabled()) {
    LOG.debug("Running " + METHOD_NAME);
  }
  final PipelinesTestUtil.PipelinesTest pipst=(PipelinesTestUtil.PipelinesTest)PipelinesTestUtil.initTest();
  pipst.fiCallSetNumBytes.set(new PipelinesTestUtil.ReceivedCheckAction(METHOD_NAME));
  pipst.fiCallSetBytesAcked.set(new PipelinesTestUtil.AckedCheckAction(METHOD_NAME));
  Path filePath=new Path("/" + METHOD_NAME + ".dat");
  FSDataOutputStream fsOut=fs.create(filePath);
  for (int i=0; i < 17; i++) {
    TestPipelines.writeData(fsOut,23);
  }
  fs.close();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.TestFileAppend </h4><pre class="type-15 type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies that exceptions are thrown during the test case execution">ExceptionVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies that exceptions are thrown during the test case execution
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * FileNotFoundException is expected for appending to a non-exisiting file
 * @throws FileNotFoundException as the result
 */
@Test(expected=FileNotFoundException.class) public void testFileNotFound() throws IOException {
  Configuration conf=new HdfsConfiguration();
  if (simulatedStorage) {
    conf.setBoolean(SimulatedFSDataset.CONFIG_PROPERTY_SIMULATED,true);
  }
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).build();
  FileSystem fs=cluster.getFileSystem();
  try {
    Path file1=new Path("/nonexistingfile.dat");
    fs.append(file1);
  }
  finally {
    fs.close();
    cluster.shutdown();
  }
}

</code></pre>

<br>
<pre class="type-14 type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Invokes logging operations">Logger</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Invokes logging operations
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test a simple flush on a simple HDFS file.
 * @throws IOException an exception might be thrown
 */
@Test public void testSimpleFlush() throws IOException {
  Configuration conf=new HdfsConfiguration();
  if (simulatedStorage) {
    conf.setBoolean(SimulatedFSDataset.CONFIG_PROPERTY_SIMULATED,true);
  }
  fileContents=AppendTestUtil.initBuffer(AppendTestUtil.FILE_SIZE);
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).build();
  FileSystem fs=cluster.getFileSystem();
  try {
    Path file1=new Path("/simpleFlush.dat");
    FSDataOutputStream stm=AppendTestUtil.createFile(fs,file1,1);
    System.out.println("Created file simpleFlush.dat");
    int mid=AppendTestUtil.FILE_SIZE / 2;
    stm.write(fileContents,0,mid);
    stm.hflush();
    System.out.println("Wrote and Flushed first part of file.");
    stm.write(fileContents,mid,AppendTestUtil.FILE_SIZE - mid);
    System.out.println("Written second part of file");
    stm.hflush();
    stm.hflush();
    System.out.println("Wrote and Flushed second part of file.");
    checkFile(fs,file1,1);
    stm.close();
    System.out.println("Closed file.");
    AppendTestUtil.checkFullFile(fs,file1,AppendTestUtil.FILE_SIZE,fileContents,"Read 2");
  }
 catch (  IOException e) {
    System.out.println("Exception :" + e);
    throw e;
  }
catch (  Throwable e) {
    System.out.println("Throwable :" + e);
    e.printStackTrace();
    throw new IOException("Throwable : " + e);
  }
 finally {
    fs.close();
    cluster.shutdown();
  }
}

</code></pre>

<br>
<pre class="type-14 type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Invokes logging operations">Logger</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Invokes logging operations
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test that file data can be flushed.
 * @throws IOException an exception might be thrown
 */
@Test public void testComplexFlush() throws IOException {
  Configuration conf=new HdfsConfiguration();
  if (simulatedStorage) {
    conf.setBoolean(SimulatedFSDataset.CONFIG_PROPERTY_SIMULATED,true);
  }
  fileContents=AppendTestUtil.initBuffer(AppendTestUtil.FILE_SIZE);
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).build();
  FileSystem fs=cluster.getFileSystem();
  try {
    Path file1=new Path("/complexFlush.dat");
    FSDataOutputStream stm=AppendTestUtil.createFile(fs,file1,1);
    System.out.println("Created file complexFlush.dat");
    int start=0;
    for (start=0; (start + 29) < AppendTestUtil.FILE_SIZE; ) {
      stm.write(fileContents,start,29);
      stm.hflush();
      start+=29;
    }
    stm.write(fileContents,start,AppendTestUtil.FILE_SIZE - start);
    checkFile(fs,file1,1);
    stm.close();
    AppendTestUtil.checkFullFile(fs,file1,AppendTestUtil.FILE_SIZE,fileContents,"Read 2");
  }
 catch (  IOException e) {
    System.out.println("Exception :" + e);
    throw e;
  }
catch (  Throwable e) {
    System.out.println("Throwable :" + e);
    e.printStackTrace();
    throw new IOException("Throwable : " + e);
  }
 finally {
    fs.close();
    cluster.shutdown();
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.TestHFlush </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * The test uses {@link #doTheJob(Configuration,String,long,short)to write a file with a standard block size}
 */
@Test public void hFlush_01() throws IOException {
  doTheJob(new HdfsConfiguration(),fName,AppendTestUtil.BLOCK_SIZE,(short)2);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * The test uses {@link #doTheJob(Configuration,String,long,short)to write a file with a custom block size so the writes will be
 * happening across block' boundaries}
 */
@Test public void hFlush_02() throws IOException {
  Configuration conf=new HdfsConfiguration();
  int customPerChecksumSize=512;
  int customBlockSize=customPerChecksumSize * 3;
  conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,customPerChecksumSize);
  conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,customBlockSize);
  doTheJob(conf,fName,customBlockSize,(short)2);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * The test uses {@link #doTheJob(Configuration,String,long,short)to write a file with a custom block size so the writes will be
 * happening across block's and checksum' boundaries}
 */
@Test public void hFlush_03() throws IOException {
  Configuration conf=new HdfsConfiguration();
  int customPerChecksumSize=400;
  int customBlockSize=customPerChecksumSize * 3;
  conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,customPerChecksumSize);
  conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,customBlockSize);
  doTheJob(conf,fName,customBlockSize,(short)2);
}

</code></pre>

<br>
<pre class="type-14 type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Invokes logging operations">Logger</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Invokes logging operations
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * This creates a slow writer and check to see 
 * if pipeline heartbeats work fine
 */
@Test public void testPipelineHeartbeat() throws Exception {
  final int DATANODE_NUM=2;
  final int fileLen=6;
  Configuration conf=new HdfsConfiguration();
  final int timeout=2000;
  conf.setInt(DFSConfigKeys.DFS_CLIENT_SOCKET_TIMEOUT_KEY,timeout);
  final Path p=new Path("/pipelineHeartbeat/foo");
  System.out.println("p=" + p);
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(DATANODE_NUM).build();
  try {
    DistributedFileSystem fs=(DistributedFileSystem)cluster.getFileSystem();
    byte[] fileContents=AppendTestUtil.initBuffer(fileLen);
    FSDataOutputStream stm=AppendTestUtil.createFile(fs,p,DATANODE_NUM);
    stm.write(fileContents,0,1);
    Thread.sleep(timeout);
    stm.hflush();
    System.out.println("Wrote 1 byte and hflush " + p);
    Thread.sleep(timeout);
    stm.write(fileContents,1,1);
    stm.hflush();
    stm.write(fileContents,2,1);
    Thread.sleep(timeout);
    stm.hflush();
    stm.write(fileContents,3,1);
    Thread.sleep(timeout);
    stm.write(fileContents,4,1);
    stm.hflush();
    stm.write(fileContents,5,1);
    Thread.sleep(timeout);
    stm.close();
    AppendTestUtil.checkFullFile(fs,p,fileLen,fileContents,"Failed to slowly write to a file");
  }
  finally {
    cluster.shutdown();
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.TestLargeBlock </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test for block size of 2GB + 512B
 * @throws IOException in case of errors
 */
@Test public void testLargeBlockSize() throws IOException {
  final long blockSize=2L * 1024L * 1024L* 1024L + 512L;
  runTest(blockSize);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.TestLeaseRecovery2 </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test the NameNode's revoke lease on current lease holder function.
 * @throws Exception
 */
@Test public void testImmediateRecoveryOfLease() throws Exception {
  byte[] actual=new byte[FILE_SIZE];
  int size=AppendTestUtil.nextInt(FILE_SIZE);
  Path filepath=createFile("/immediateRecoverLease-shortlease",size,true);
  cluster.setLeasePeriod(SHORT_LEASE_PERIOD,LONG_LEASE_PERIOD);
  recoverLeaseUsingCreate(filepath);
  verifyFile(dfs,filepath,actual,size);
  cluster.setLeasePeriod(LONG_LEASE_PERIOD,LONG_LEASE_PERIOD);
  size=AppendTestUtil.nextInt(FILE_SIZE);
  filepath=createFile("/immediateRecoverLease-longlease",size,false);
  recoverLease(filepath,null);
  verifyFile(dfs,filepath,actual,size);
  size=AppendTestUtil.nextInt(FILE_SIZE);
  filepath=createFile("/immediateRecoverLease-sameclient",size,false);
  Path filepath1=new Path(filepath.toString() + AppendTestUtil.nextInt());
  FSDataOutputStream stm=dfs.create(filepath1,true,BUF_SIZE,REPLICATION_NUM,BLOCK_SIZE);
  recoverLease(filepath,dfs);
  verifyFile(dfs,filepath,actual,size);
  stm.write(buffer,0,size);
  stm.close();
  verifyFile(dfs,filepath1,actual,size);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testHardLeaseRecoveryWithRenameAfterNameNodeRestart() throws Exception {
  hardLeaseRecoveryRestartHelper(true);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * This test makes it so the client does not renew its lease and also
 * set the hard lease expiration period to be short, thus triggering
 * lease expiration to happen while the client is still alive. The test
 * also causes the NN to restart after lease recovery has begun, but before
 * the DNs have completed the blocks. This test verifies that when the NN
 * comes back up, the client no longer holds the lease.
 * The test makes sure that the lease recovery completes and the client
 * fails if it continues to write to the file, even after NN restart.
 * @throws Exception
 */
@Test public void testHardLeaseRecoveryAfterNameNodeRestart() throws Exception {
  hardLeaseRecoveryRestartHelper(false);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.TestMultiThreadedHflush </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test case where a bunch of threads are both appending and flushing.
 * They all finish before the file is closed.
 */
@Test public void testMultipleHflushers() throws Exception {
  Configuration conf=new Configuration();
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).build();
  FileSystem fs=cluster.getFileSystem();
  Path p=new Path("/multiple-hflushers.dat");
  try {
    doMultithreadedWrites(conf,p,NUM_THREADS,WRITE_SIZE,NUM_WRITES_PER_THREAD);
  }
  finally {
    fs.close();
    cluster.shutdown();
  }
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test case where a bunch of threads are continuously calling hflush() while another
 * thread appends some data and then closes the file.
 * The hflushing threads should eventually catch an IOException stating that the stream
 * was closed -- and not an NPE or anything like that.
 */
@Test public void testHflushWhileClosing() throws Throwable {
  Configuration conf=new Configuration();
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).build();
  FileSystem fs=cluster.getFileSystem();
  Path p=new Path("/hflush-and-close.dat");
  final FSDataOutputStream stm=createFile(fs,p,1);
  ArrayList<Thread> flushers=new ArrayList<Thread>();
  final AtomicReference<Throwable> thrown=new AtomicReference<Throwable>();
  try {
    for (int i=0; i < 10; i++) {
      Thread flusher=new Thread(){
        public void run(){
          try {
            while (true) {
              try {
                stm.hflush();
              }
 catch (              IOException ioe) {
                if (!ioe.toString().contains("DFSOutputStream is closed")) {
                  throw ioe;
                }
 else {
                  return;
                }
              }
            }
          }
 catch (          Throwable t) {
            thrown.set(t);
          }
        }
      }
;
      flusher.start();
      flushers.add(flusher);
    }
    for (int i=0; i < 10000; i++) {
      stm.write(1);
    }
    stm.close();
    for (    Thread t : flushers) {
      t.join();
    }
    if (thrown.get() != null) {
      throw thrown.get();
    }
  }
  finally {
    fs.close();
    cluster.shutdown();
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.TestReadWhileWriting </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test reading while writing. 
 */
@Test public void pipeline_02_03() throws Exception {
  final Configuration conf=new HdfsConfiguration();
  conf.setBoolean(DFSConfigKeys.DFS_SUPPORT_APPEND_KEY,true);
  conf.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1);
  final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(4).build();
  try {
    cluster.setLeasePeriod(SOFT_LEASE_LIMIT,HARD_LEASE_LIMIT);
    cluster.waitActive();
    final FileSystem fs=cluster.getFileSystem();
    final Path p=new Path(DIR,"file1");
    final int half=BLOCK_SIZE / 2;
{
      final FSDataOutputStream out=fs.create(p,true,fs.getConf().getInt("io.file.buffer.size",4096),(short)3,BLOCK_SIZE);
      write(out,0,half);
      ((DFSOutputStream)out.getWrappedStream()).hflush();
    }
    checkFile(p,half,conf);
    AppendTestUtil.LOG.info("leasechecker.interruptAndJoin()");
    ((DistributedFileSystem)fs).dfs.leaserenewer.interruptAndJoin();
{
      Thread.sleep(2 * SOFT_LEASE_LIMIT);
      final UserGroupInformation current=UserGroupInformation.getCurrentUser();
      final UserGroupInformation ugi=UserGroupInformation.createUserForTesting(current.getShortUserName() + "x",new String[]{"supergroup"});
      final DistributedFileSystem dfs=ugi.doAs(new PrivilegedExceptionAction<DistributedFileSystem>(){
        @Override public DistributedFileSystem run() throws Exception {
          return (DistributedFileSystem)FileSystem.newInstance(conf);
        }
      }
);
      final FSDataOutputStream out=append(dfs,p);
      write(out,0,half);
      out.close();
    }
    checkFile(p,2 * half,conf);
  }
  finally {
    cluster.shutdown();
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.TestWriteConfigurationToDFS </h4><pre class="type-14 type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Invokes logging operations">Logger</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Invokes logging operations
- Executes methods or other tests from the same test unit
"></span><br>
@Test(timeout=60000) public void testWriteConf() throws Exception {
  Configuration conf=new HdfsConfiguration();
  conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,4096);
  System.out.println("Setting conf in: " + System.identityHashCode(conf));
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).build();
  FileSystem fs=cluster.getFileSystem();
  Path filePath=new Path("/testWriteConf.xml");
  OutputStream os=fs.create(filePath);
  StringBuilder longString=new StringBuilder();
  for (int i=0; i < 100000; i++) {
    longString.append("hello");
  }
  conf.set("foobar",longString.toString());
  conf.writeXml(os);
  os.close();
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.protocol.TestLayoutVersion </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Tests to make sure a given layout version supports all the
 * features from the ancestor
 */
@Test public void testFeaturesFromAncestorSupported(){
  for (  Feature f : Feature.values()) {
    validateFeatureList(f);
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.security.TestClientProtocolWithDelegationToken </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testDelegationTokenRpc() throws Exception {
  ClientProtocol mockNN=mock(ClientProtocol.class);
  FSNamesystem mockNameSys=mock(FSNamesystem.class);
  when(mockNN.getProtocolVersion(anyString(),anyLong())).thenReturn(ClientProtocol.versionID);
  doReturn(ProtocolSignature.getProtocolSignature(mockNN,ClientProtocol.class.getName(),ClientProtocol.versionID,0)).when(mockNN).getProtocolSignature(anyString(),anyLong(),anyInt());
  DelegationTokenSecretManager sm=new DelegationTokenSecretManager(DFSConfigKeys.DFS_NAMENODE_DELEGATION_KEY_UPDATE_INTERVAL_DEFAULT,DFSConfigKeys.DFS_NAMENODE_DELEGATION_KEY_UPDATE_INTERVAL_DEFAULT,DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_MAX_LIFETIME_DEFAULT,3600000,mockNameSys);
  sm.startThreads();
  final Server server=RPC.getServer(ClientProtocol.class,mockNN,ADDRESS,0,5,true,conf,sm);
  server.start();
  final UserGroupInformation current=UserGroupInformation.getCurrentUser();
  final InetSocketAddress addr=NetUtils.getConnectAddress(server);
  String user=current.getUserName();
  Text owner=new Text(user);
  DelegationTokenIdentifier dtId=new DelegationTokenIdentifier(owner,owner,null);
  Token<DelegationTokenIdentifier> token=new Token<DelegationTokenIdentifier>(dtId,sm);
  Text host=new Text(addr.getAddress().getHostAddress() + ":" + addr.getPort());
  token.setService(host);
  LOG.info("Service IP address for token is " + host);
  current.addToken(token);
  current.doAs(new PrivilegedExceptionAction(){
    @Override public Object run() throws Exception {
      ClientProtocol proxy=null;
      try {
        proxy=(ClientProtocol)RPC.getProxy(ClientProtocol.class,ClientProtocol.versionID,addr,conf);
        proxy.getServerDefaults();
      }
  finally {
        server.stop();
        if (proxy != null) {
          RPC.stopProxy(proxy);
        }
      }
      return null;
    }
  }
);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.security.token.block.TestBlockToken </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * test block key and token handling 
 */
@Test public void testBlockTokenSecretManager() throws Exception {
  BlockTokenSecretManager masterHandler=new BlockTokenSecretManager(true,blockKeyUpdateInterval,blockTokenLifetime);
  BlockTokenSecretManager slaveHandler=new BlockTokenSecretManager(false,blockKeyUpdateInterval,blockTokenLifetime);
  ExportedBlockKeys keys=masterHandler.exportKeys();
  slaveHandler.setKeys(keys);
  tokenGenerationAndVerification(masterHandler,slaveHandler);
  masterHandler.updateKeys();
  tokenGenerationAndVerification(masterHandler,slaveHandler);
  keys=masterHandler.exportKeys();
  slaveHandler.setKeys(keys);
  tokenGenerationAndVerification(masterHandler,slaveHandler);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testWritable() throws Exception {
  TestWritable.testWritable(new BlockTokenIdentifier());
  BlockTokenSecretManager sm=new BlockTokenSecretManager(true,blockKeyUpdateInterval,blockTokenLifetime);
  TestWritable.testWritable(generateTokenId(sm,block1,EnumSet.allOf(BlockTokenSecretManager.AccessMode.class)));
  TestWritable.testWritable(generateTokenId(sm,block2,EnumSet.of(BlockTokenSecretManager.AccessMode.WRITE)));
  TestWritable.testWritable(generateTokenId(sm,block3,EnumSet.noneOf(BlockTokenSecretManager.AccessMode.class)));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test {@link BlockPoolTokenSecretManager}
 */
@Test public void testBlockPoolTokenSecretManager() throws Exception {
  BlockPoolTokenSecretManager bpMgr=new BlockPoolTokenSecretManager();
  for (int i=0; i < 10; i++) {
    String bpid=Integer.toString(i);
    BlockTokenSecretManager masterHandler=new BlockTokenSecretManager(true,blockKeyUpdateInterval,blockTokenLifetime);
    BlockTokenSecretManager slaveHandler=new BlockTokenSecretManager(false,blockKeyUpdateInterval,blockTokenLifetime);
    bpMgr.addBlockPool(bpid,slaveHandler);
    ExportedBlockKeys keys=masterHandler.exportKeys();
    bpMgr.setKeys(bpid,keys);
    tokenGenerationAndVerification(masterHandler,bpMgr.get(bpid));
    masterHandler.updateKeys();
    tokenGenerationAndVerification(masterHandler,bpMgr.get(bpid));
    keys=masterHandler.exportKeys();
    bpMgr.setKeys(bpid,keys);
    tokenGenerationAndVerification(masterHandler,bpMgr.get(bpid));
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.balancer.TestBalancerWithMultipleNameNodes </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test a cluster with even distribution, 
 * then a new empty node is added to the cluster
 */
@Test public void testBalancer() throws Exception {
  final Configuration conf=createConf();
  runTest(2,new long[]{CAPACITY},new String[]{RACK0},CAPACITY / 2,RACK0,conf);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test unevenly distributed cluster 
 */
@Test public void testUnevenDistribution() throws Exception {
  final Configuration conf=createConf();
  unevenDistribution(2,new long[]{30 * CAPACITY / 100,5 * CAPACITY / 100},new long[]{CAPACITY,CAPACITY},new String[]{RACK0,RACK1},conf);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.common.TestJspHelper </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testGetUgi() throws IOException {
  conf.set(DFSConfigKeys.FS_DEFAULT_NAME_KEY,"hdfs://localhost:4321/");
  HttpServletRequest request=mock(HttpServletRequest.class);
  ServletContext context=mock(ServletContext.class);
  String user="TheDoctor";
  Text userText=new Text(user);
  DelegationTokenIdentifier dtId=new DelegationTokenIdentifier(userText,userText,null);
  Token<DelegationTokenIdentifier> token=new Token<DelegationTokenIdentifier>(dtId,new DummySecretManager(0,0,0,0));
  String tokenString=token.encodeToUrlString();
  when(request.getParameter(JspHelper.DELEGATION_PARAMETER_NAME)).thenReturn(tokenString);
  when(request.getRemoteUser()).thenReturn(user);
  when(request.getParameter(JspHelper.NAMENODE_ADDRESS)).thenReturn("1.1.1.1:1111");
  conf.set(DFSConfigKeys.HADOOP_SECURITY_AUTHENTICATION,"kerberos");
  UserGroupInformation.setConfiguration(conf);
  verifyServiceInToken(context,request,"1.1.1.1:1111");
  when(request.getParameter(JspHelper.NAMENODE_ADDRESS)).thenReturn(null);
  InetSocketAddress addr=new InetSocketAddress("localhost",2222);
  when(context.getAttribute(NameNode.NAMENODE_ADDRESS_ATTRIBUTE_KEY)).thenReturn(addr);
  verifyServiceInToken(context,request,addr.getAddress().getHostAddress() + ":2222");
  token.setService(new Text("3.3.3.3:3333"));
  tokenString=token.encodeToUrlString();
  when(context.getAttribute(NameNode.NAMENODE_ADDRESS_ATTRIBUTE_KEY)).thenReturn(null);
  when(request.getParameter(JspHelper.DELEGATION_PARAMETER_NAME)).thenReturn(tokenString);
  verifyServiceInToken(context,request,"3.3.3.3:3333");
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * BlockRecovery_02.11.
 * Two replicas are RBW.
 * @throws IOException in case of an error
 */
@Test public void testRBWReplicas() throws IOException {
  if (LOG.isDebugEnabled()) {
    LOG.debug("Running " + GenericTestUtils.getMethodName());
  }
  ReplicaRecoveryInfo replica1=new ReplicaRecoveryInfo(BLOCK_ID,REPLICA_LEN1,GEN_STAMP - 1,ReplicaState.RBW);
  ReplicaRecoveryInfo replica2=new ReplicaRecoveryInfo(BLOCK_ID,REPLICA_LEN2,GEN_STAMP - 2,ReplicaState.RBW);
  InterDatanodeProtocol dn1=mock(InterDatanodeProtocol.class);
  InterDatanodeProtocol dn2=mock(InterDatanodeProtocol.class);
  long minLen=Math.min(REPLICA_LEN1,REPLICA_LEN2);
  testSyncReplicas(replica1,replica2,dn1,dn2,minLen);
  verify(dn1).updateReplicaUnderRecovery(block,RECOVERY_ID,minLen);
  verify(dn2).updateReplicaUnderRecovery(block,RECOVERY_ID,minLen);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * BlockRecovery_02.12.
 * One replica is RBW and another is RWR. 
 * @throws IOException in case of an error
 */
@Test public void testRBW_RWRReplicas() throws IOException {
  if (LOG.isDebugEnabled()) {
    LOG.debug("Running " + GenericTestUtils.getMethodName());
  }
  ReplicaRecoveryInfo replica1=new ReplicaRecoveryInfo(BLOCK_ID,REPLICA_LEN1,GEN_STAMP - 1,ReplicaState.RBW);
  ReplicaRecoveryInfo replica2=new ReplicaRecoveryInfo(BLOCK_ID,REPLICA_LEN1,GEN_STAMP - 2,ReplicaState.RWR);
  InterDatanodeProtocol dn1=mock(InterDatanodeProtocol.class);
  InterDatanodeProtocol dn2=mock(InterDatanodeProtocol.class);
  testSyncReplicas(replica1,replica2,dn1,dn2,REPLICA_LEN1);
  verify(dn1).updateReplicaUnderRecovery(block,RECOVERY_ID,REPLICA_LEN1);
  verify(dn2,never()).updateReplicaUnderRecovery(block,RECOVERY_ID,REPLICA_LEN1);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * BlockRecovery_02.10.
 * One replica is Finalized and another is RWR. 
 * @throws IOException in case of an error
 */
@Test public void testFinalizedRwrReplicas() throws IOException {
  if (LOG.isDebugEnabled()) {
    LOG.debug("Running " + GenericTestUtils.getMethodName());
  }
  ReplicaRecoveryInfo replica1=new ReplicaRecoveryInfo(BLOCK_ID,REPLICA_LEN1,GEN_STAMP - 1,ReplicaState.FINALIZED);
  ReplicaRecoveryInfo replica2=new ReplicaRecoveryInfo(BLOCK_ID,REPLICA_LEN1,GEN_STAMP - 2,ReplicaState.RWR);
  InterDatanodeProtocol dn1=mock(InterDatanodeProtocol.class);
  InterDatanodeProtocol dn2=mock(InterDatanodeProtocol.class);
  testSyncReplicas(replica1,replica2,dn1,dn2,REPLICA_LEN1);
  verify(dn1).updateReplicaUnderRecovery(block,RECOVERY_ID,REPLICA_LEN1);
  verify(dn2,never()).updateReplicaUnderRecovery(block,RECOVERY_ID,REPLICA_LEN1);
  replica1=new ReplicaRecoveryInfo(BLOCK_ID,REPLICA_LEN1,GEN_STAMP - 1,ReplicaState.FINALIZED);
  replica2=new ReplicaRecoveryInfo(BLOCK_ID,REPLICA_LEN2,GEN_STAMP - 2,ReplicaState.RBW);
  dn1=mock(InterDatanodeProtocol.class);
  dn2=mock(InterDatanodeProtocol.class);
  testSyncReplicas(replica1,replica2,dn1,dn2,REPLICA_LEN1);
  verify(dn1).updateReplicaUnderRecovery(block,RECOVERY_ID,REPLICA_LEN1);
  verify(dn2,never()).updateReplicaUnderRecovery(block,RECOVERY_ID,REPLICA_LEN1);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * BlockRecoveryFI_07. max replica length from all DNs is zero.
 * @throws IOException in case of an error
 */
@Test public void testZeroLenReplicas() throws IOException, InterruptedException {
  if (LOG.isDebugEnabled()) {
    LOG.debug("Running " + GenericTestUtils.getMethodName());
  }
  DataNode spyDN=spy(dn);
  doReturn(new ReplicaRecoveryInfo(block.getBlockId(),0,block.getGenerationStamp(),ReplicaState.FINALIZED)).when(spyDN).initReplicaRecovery(any(RecoveringBlock.class));
  Daemon d=spyDN.recoverBlocks(initRecoveringBlocks());
  d.join();
  DatanodeProtocol dnP=dn.getBPNamenode(POOL_ID);
  verify(dnP).commitBlockSynchronization(block,RECOVERY_ID,0,true,true,DatanodeID.EMPTY_ARRAY);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * BlockRecovery_02.9.
 * One replica is Finalized and another is RBW. 
 * @throws IOException in case of an error
 */
@Test public void testFinalizedRbwReplicas() throws IOException {
  if (LOG.isDebugEnabled()) {
    LOG.debug("Running " + GenericTestUtils.getMethodName());
  }
  ReplicaRecoveryInfo replica1=new ReplicaRecoveryInfo(BLOCK_ID,REPLICA_LEN1,GEN_STAMP - 1,ReplicaState.FINALIZED);
  ReplicaRecoveryInfo replica2=new ReplicaRecoveryInfo(BLOCK_ID,REPLICA_LEN1,GEN_STAMP - 2,ReplicaState.RBW);
  InterDatanodeProtocol dn1=mock(InterDatanodeProtocol.class);
  InterDatanodeProtocol dn2=mock(InterDatanodeProtocol.class);
  testSyncReplicas(replica1,replica2,dn1,dn2,REPLICA_LEN1);
  verify(dn1).updateReplicaUnderRecovery(block,RECOVERY_ID,REPLICA_LEN1);
  verify(dn2).updateReplicaUnderRecovery(block,RECOVERY_ID,REPLICA_LEN1);
  replica1=new ReplicaRecoveryInfo(BLOCK_ID,REPLICA_LEN1,GEN_STAMP - 1,ReplicaState.FINALIZED);
  replica2=new ReplicaRecoveryInfo(BLOCK_ID,REPLICA_LEN2,GEN_STAMP - 2,ReplicaState.RBW);
  dn1=mock(InterDatanodeProtocol.class);
  dn2=mock(InterDatanodeProtocol.class);
  testSyncReplicas(replica1,replica2,dn1,dn2,REPLICA_LEN1);
  verify(dn1).updateReplicaUnderRecovery(block,RECOVERY_ID,REPLICA_LEN1);
  verify(dn2,never()).updateReplicaUnderRecovery(block,RECOVERY_ID,REPLICA_LEN1);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * BlockRecoveryFI_06. all datanodes throws an exception.
 * @throws IOExceptionin case of an error
 */
@Test public void testErrorReplicas() throws IOException, InterruptedException {
  if (LOG.isDebugEnabled()) {
    LOG.debug("Running " + GenericTestUtils.getMethodName());
  }
  DataNode spyDN=spy(dn);
  doThrow(new IOException()).when(spyDN).initReplicaRecovery(any(RecoveringBlock.class));
  Daemon d=spyDN.recoverBlocks(initRecoveringBlocks());
  d.join();
  verify(spyDN,never()).syncBlock(any(RecoveringBlock.class),anyListOf(BlockRecord.class));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * BlockRecovery_02.13. 
 * Two replicas are RWR.
 * @throws IOException in case of an error
 */
@Test public void testRWRReplicas() throws IOException {
  if (LOG.isDebugEnabled()) {
    LOG.debug("Running " + GenericTestUtils.getMethodName());
  }
  ReplicaRecoveryInfo replica1=new ReplicaRecoveryInfo(BLOCK_ID,REPLICA_LEN1,GEN_STAMP - 1,ReplicaState.RWR);
  ReplicaRecoveryInfo replica2=new ReplicaRecoveryInfo(BLOCK_ID,REPLICA_LEN2,GEN_STAMP - 2,ReplicaState.RWR);
  InterDatanodeProtocol dn1=mock(InterDatanodeProtocol.class);
  InterDatanodeProtocol dn2=mock(InterDatanodeProtocol.class);
  long minLen=Math.min(REPLICA_LEN1,REPLICA_LEN2);
  testSyncReplicas(replica1,replica2,dn1,dn2,minLen);
  verify(dn1).updateReplicaUnderRecovery(block,RECOVERY_ID,minLen);
  verify(dn2).updateReplicaUnderRecovery(block,RECOVERY_ID,minLen);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * BlockRecoveryFI_05. One DN throws RecoveryInProgressException.
 * @throws IOExceptionin case of an error
 */
@Test public void testRecoveryInProgressException() throws IOException, InterruptedException {
  if (LOG.isDebugEnabled()) {
    LOG.debug("Running " + GenericTestUtils.getMethodName());
  }
  DataNode spyDN=spy(dn);
  doThrow(new RecoveryInProgressException("Replica recovery is in progress")).when(spyDN).initReplicaRecovery(any(RecoveringBlock.class));
  Daemon d=spyDN.recoverBlocks(initRecoveringBlocks());
  d.join();
  verify(spyDN,never()).syncBlock(any(RecoveringBlock.class),anyListOf(BlockRecord.class));
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure </h4><pre class="type-14 type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Invokes logging operations">Logger</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Invokes logging operations
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testVolumeFailure() throws IOException {
  FileSystem fs=cluster.getFileSystem();
  dataDir=new File(cluster.getDataDirectory());
  System.out.println("Data dir: is " + dataDir.getPath());
  String filename="/test.txt";
  Path filePath=new Path(filename);
  int filesize=block_size * blocks_num;
  DFSTestUtil.createFile(fs,filePath,filesize,repl,1L);
  DFSTestUtil.waitReplication(fs,filePath,repl);
  System.out.println("file " + filename + "(size "+ filesize+ ") is created and replicated");
  data_fail=new File(dataDir,"data3");
  failedDir=MiniDFSCluster.getFinalizedDir(dataDir,cluster.getNamesystem().getBlockPoolId());
  if (failedDir.exists() && !deteteBlocks(failedDir)) {
    throw new IOException("Could not delete hdfs directory '" + failedDir + "'");
  }
  data_fail.setReadOnly();
  failedDir.setReadOnly();
  System.out.println("Deleteing " + failedDir.getPath() + "; exist="+ failedDir.exists());
  triggerFailure(filename,filesize);
  DataNode dn=cluster.getDataNodes().get(1);
  String bpid=cluster.getNamesystem().getBlockPoolId();
  DatanodeRegistration dnR=dn.getDNRegistrationForBP(bpid);
  long[] bReport=dn.getFSDataset().getBlockReport(bpid).getBlockListAsLongs();
  cluster.getNameNode().blockReport(dnR,bpid,bReport);
  verify(filename,filesize);
  System.out.println("creating file test1.txt");
  Path fileName1=new Path("/test1.txt");
  DFSTestUtil.createFile(fs,fileName1,filesize,repl,1L);
  DFSTestUtil.waitReplication(fs,fileName1,repl);
  System.out.println("file " + fileName1.getName() + " is created and replicated");
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test for different combination of volume configs and volumes tolerated 
 * values.
 */
@Test public void testVolumeAndTolerableConfiguration() throws Exception {
  testVolumeConfig(-1,0,false,true);
  testVolumeConfig(100,0,false,true);
  testVolumeConfig(0,1,false,false);
  testVolumeConfig(1,1,true,false);
  testVolumeConfig(0,0,true,false);
  testVolumeConfig(0,2,false,false);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.datanode.TestDatanodeJsp </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testViewFileJsp() throws IOException {
  MiniDFSCluster cluster=null;
  try {
    Configuration conf=new HdfsConfiguration();
    cluster=new MiniDFSCluster.Builder(conf).build();
    cluster.waitActive();
    testViewingFile(cluster,"/test-file",false);
    testViewingFile(cluster,"/tmp/test-file",false);
    testViewingFile(cluster,"/tmp/test-file%with goofy&characters",false);
    testViewingFile(cluster,"/test-file",true);
    testViewingFile(cluster,"/tmp/test-file",true);
    testViewingFile(cluster,"/tmp/test-file%with goofy&characters",true);
  }
  finally {
    if (cluster != null) {
      cluster.shutdown();
    }
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.datanode.TestDatanodeRestart </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testFinalizedReplicas() throws Exception {
  Configuration conf=new HdfsConfiguration();
  conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,1024L);
  conf.setInt(DFSConfigKeys.DFS_CLIENT_WRITE_PACKET_SIZE_KEY,512);
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(3).build();
  cluster.waitActive();
  FileSystem fs=cluster.getFileSystem();
  try {
    final String TopDir="/test";
    DFSTestUtil util=new DFSTestUtil("TestCrcCorruption",2,3,8 * 1024);
    util.createFiles(fs,TopDir,(short)3);
    util.waitReplication(fs,TopDir,(short)3);
    util.checkFiles(fs,TopDir);
    cluster.restartDataNodes();
    cluster.waitActive();
    util.checkFiles(fs,TopDir);
  }
  finally {
    cluster.shutdown();
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.datanode.TestFiDataTransferProtocol </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline setup, DN1 never responses after received setup ack from DN2.
 * Client gets an IOException and determine DN1 bad.
 */
@Test public void pipeline_Fi_04() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runStatusReadTest(methodName,1,new SleepAction(methodName,1,0));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Streaming: Write a packet, DN1 throws a DiskOutOfSpaceError
 * when it writes the data to disk.
 * Client gets an IOException and determine DN1 bad.
 */
@Test public void pipeline_Fi_15() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runCallWritePacketToDisk(methodName,1,new DoosAction(methodName,1));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline setup, DN0 never responses after received setup ack from DN1.
 * Client gets an IOException and determine DN0 bad.
 */
@Test public void pipeline_Fi_05() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runStatusReadTest(methodName,0,new SleepAction(methodName,0,0));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Streaming: Write a packet, DN2 throws a DiskOutOfSpaceError
 * when it writes the data to disk.
 * Client gets an IOException and determine DN2 bad.
 */
@Test public void pipeline_Fi_16() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runCallWritePacketToDisk(methodName,2,new DoosAction(methodName,2));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline setup with DN0 very slow but it won't lead to timeout.
 * Client finishes setup successfully.
 */
@Test public void pipeline_Fi_06() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runSlowDatanodeTest(methodName,new SleepAction(methodName,0,3000));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline setup with DN1 very slow but it won't lead to timeout.
 * Client finishes setup successfully.
 */
@Test public void pipeline_Fi_07() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runSlowDatanodeTest(methodName,new SleepAction(methodName,1,3000));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline setup with DN2 very slow but it won't lead to timeout.
 * Client finishes setup successfully.
 */
@Test public void pipeline_Fi_08() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runSlowDatanodeTest(methodName,new SleepAction(methodName,2,3000));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline setup, DN0 throws an OutOfMemoryException right after it
 * received a setup request from client.
 * Client gets an IOException and determine DN0 bad.
 */
@Test public void pipeline_Fi_09() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runReceiverOpWriteBlockTest(methodName,0,new OomAction(methodName,0));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline setup, DN1 throws an OutOfMemoryException right after it
 * received a setup request from DN0.
 * Client gets an IOException and determine DN1 bad.
 */
@Test public void pipeline_Fi_10() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runReceiverOpWriteBlockTest(methodName,1,new OomAction(methodName,1));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline setup, DN2 throws an OutOfMemoryException right after it
 * received a setup request from DN1.
 * Client gets an IOException and determine DN2 bad.
 */
@Test public void pipeline_Fi_11() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runReceiverOpWriteBlockTest(methodName,2,new OomAction(methodName,2));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline setup:
 * DN0 never responses after received setup request from client.
 * Client gets an IOException and determine DN0 bad.
 */
@Test public void pipeline_Fi_01() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runReceiverOpWriteBlockTest(methodName,0,new SleepAction(methodName,0,0));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline setup, DN1 throws an OutOfMemoryException right after it
 * received a setup ack from DN2.
 * Client gets an IOException and determine DN1 bad.
 */
@Test public void pipeline_Fi_12() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runStatusReadTest(methodName,1,new OomAction(methodName,1));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline setup:
 * DN1 never responses after received setup request from client.
 * Client gets an IOException and determine DN1 bad.
 */
@Test public void pipeline_Fi_02() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runReceiverOpWriteBlockTest(methodName,1,new SleepAction(methodName,1,0));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline setup, DN0 throws an OutOfMemoryException right after it
 * received a setup ack from DN1.
 * Client gets an IOException and determine DN0 bad.
 */
@Test public void pipeline_Fi_13() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runStatusReadTest(methodName,0,new OomAction(methodName,0));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline setup:
 * DN2 never responses after received setup request from client.
 * Client gets an IOException and determine DN2 bad.
 */
@Test public void pipeline_Fi_03() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runReceiverOpWriteBlockTest(methodName,2,new SleepAction(methodName,2,0));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Streaming: Write a packet, DN0 throws a DiskOutOfSpaceError
 * when it writes the data to disk.
 * Client gets an IOException and determine DN0 bad.
 */
@Test public void pipeline_Fi_14() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runCallWritePacketToDisk(methodName,0,new DoosAction(methodName,0));
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.datanode.TestFiDataTransferProtocol2 </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Streaming: Randomize datanode speed, write several packets, DN1 throws a
 * OutOfMemoryException when it receives the ack of the third packet from DN2.
 * Client gets an IOException and determines DN1 bad.
 */
@Test public void pipeline_Fi_29() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runTest29_30(methodName,1);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Streaming:
 * Randomize datanode speed, write several packets,
 * DN0 throws a DiskOutOfSpaceError when it writes the third packet to disk.
 * Client gets an IOException and determines DN0 bad.
 */
@Test public void pipeline_Fi_17() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runTest17_19(methodName,0);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Streaming:
 * Randomize datanode speed, write several packets,
 * DN1 throws a DiskOutOfSpaceError when it writes the third packet to disk.
 * Client gets an IOException and determines DN1 bad.
 */
@Test public void pipeline_Fi_18() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runTest17_19(methodName,1);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Streaming:
 * Randomize datanode speed, write several packets,
 * DN2 throws a DiskOutOfSpaceError when it writes the third packet to disk.
 * Client gets an IOException and determines DN2 bad.
 */
@Test public void pipeline_Fi_19() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runTest17_19(methodName,2);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Streaming: Randomize datanode speed, write several packets, DN0 throws a
 * OutOfMemoryException when it receives the ack of the third packet from DN1.
 * Client gets an IOException and determines DN0 bad.
 */
@Test public void pipeline_Fi_30() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runTest29_30(methodName,0);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Streaming: Client writes several packets with DN0 very slow. Client
 * finishes write successfully.
 */
@Test public void pipeline_Fi_20() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  FiTestUtil.LOG.info("Running " + methodName + " ...");
  final DataTransferTest t=(DataTransferTest)DataTransferTestUtil.initTest();
  initSlowDatanodeTest(t,new SleepAction(methodName,0,MAX_SLEEP));
  writeSeveralPackets(methodName);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Streaming: Client writes several packets with DN1 very slow. Client
 * finishes write successfully.
 */
@Test public void pipeline_Fi_21() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  FiTestUtil.LOG.info("Running " + methodName + " ...");
  final DataTransferTest t=(DataTransferTest)DataTransferTestUtil.initTest();
  initSlowDatanodeTest(t,new SleepAction(methodName,1,MAX_SLEEP));
  writeSeveralPackets(methodName);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Streaming: Client writes several packets with DN2 very slow. Client
 * finishes write successfully.
 */
@Test public void pipeline_Fi_22() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  FiTestUtil.LOG.info("Running " + methodName + " ...");
  final DataTransferTest t=(DataTransferTest)DataTransferTestUtil.initTest();
  initSlowDatanodeTest(t,new SleepAction(methodName,2,MAX_SLEEP));
  writeSeveralPackets(methodName);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Streaming: Write several packets, DN1 never responses when it receives the
 * ack of the third packet from DN2. Client gets an IOException and determines
 * DN1 bad.
 */
@Test public void pipeline_Fi_34() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runTest34_35(methodName,1);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Streaming: Write several packets, DN0 never responses when it receives the
 * ack of the third packet from DN1. Client gets an IOException and determines
 * DN0 bad.
 */
@Test public void pipeline_Fi_35() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runTest34_35(methodName,0);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.datanode.TestFiPipelineClose </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline close:
 * DN1 never responses after received close ack DN2.
 * Client gets an IOException and determine DN1 bad.
 */
@Test public void pipeline_Fi_39() throws IOException {
  run39_40(FiTestUtil.getMethodName(),1);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline close:
 * DN2 never responses after received close request from client.
 * Client gets an IOException and determine DN2 bad.
 */
@Test public void pipeline_Fi_38() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runPipelineCloseTest(methodName,new SleepAction(methodName,2,0));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline close with DN0 very slow but it won't lead to timeout.
 * Client finishes close successfully.
 */
@Test public void pipeline_Fi_41() throws IOException {
  run41_43(FiTestUtil.getMethodName(),0);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline close:
 * DN0 never responses after received close ack DN1.
 * Client gets an IOException and determine DN0 bad.
 */
@Test public void pipeline_Fi_40() throws IOException {
  run39_40(FiTestUtil.getMethodName(),0);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline close:
 * DN2 throws a disk error exception when it is closing the block file.
 * Client gets an IOException and determine DN2 bad.
 */
@Test public void pipeline_Fi_51() throws IOException {
  run49_51(FiTestUtil.getMethodName(),2);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline close:
 * DN1 throws a disk error exception when it is closing the block file.
 * Client gets an IOException and determine DN1 bad.
 */
@Test public void pipeline_Fi_50() throws IOException {
  run49_51(FiTestUtil.getMethodName(),1);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline close:
 * DN1 throws an OutOfMemoryException
 * right after it received a close request from client.
 * Client gets an IOException and determine DN1 bad.
 */
@Test public void pipeline_Fi_45() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runPipelineCloseTest(methodName,new OomAction(methodName,1));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline close:
 * DN0 throws an OutOfMemoryException
 * right after it received a close request from client.
 * Client gets an IOException and determine DN0 bad.
 */
@Test public void pipeline_Fi_44() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runPipelineCloseTest(methodName,new OomAction(methodName,0));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline close with DN2 very slow but it won't lead to timeout.
 * Client finishes close successfully.
 */
@Test public void pipeline_Fi_43() throws IOException {
  run41_43(FiTestUtil.getMethodName(),2);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline close with DN1 very slow but it won't lead to timeout.
 * Client finishes close successfully.
 */
@Test public void pipeline_Fi_42() throws IOException {
  run41_43(FiTestUtil.getMethodName(),1);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline close:
 * DN0 throws a disk error exception when it is closing the block file.
 * Client gets an IOException and determine DN0 bad.
 */
@Test public void pipeline_Fi_49() throws IOException {
  run49_51(FiTestUtil.getMethodName(),0);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline close:
 * DN1 never responses after received close request from client.
 * Client gets an IOException and determine DN1 bad.
 */
@Test public void pipeline_Fi_37() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runPipelineCloseTest(methodName,new SleepAction(methodName,1,0));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline close:
 * DN0 throws an OutOfMemoryException right after
 * it received a close ack from DN1.
 * Client gets an IOException and determine DN0 bad.
 */
@Test public void pipeline_Fi_48() throws IOException {
  run47_48(FiTestUtil.getMethodName(),0);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline close:
 * DN0 never responses after received close request from client.
 * Client gets an IOException and determine DN0 bad.
 */
@Test public void pipeline_Fi_36() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runPipelineCloseTest(methodName,new SleepAction(methodName,0,0));
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline close:
 * DN1 throws an OutOfMemoryException right after
 * it received a close ack from DN2.
 * Client gets an IOException and determine DN1 bad.
 */
@Test public void pipeline_Fi_47() throws IOException {
  run47_48(FiTestUtil.getMethodName(),1);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Pipeline close:
 * DN2 throws an OutOfMemoryException
 * right after it received a close request from client.
 * Client gets an IOException and determine DN2 bad.
 */
@Test public void pipeline_Fi_46() throws IOException {
  final String methodName=FiTestUtil.getMethodName();
  runPipelineCloseTest(methodName,new OomAction(methodName,2));
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.datanode.TestMulitipleNNDataBlockScanner </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testBlockScannerAfterRefresh() throws IOException, InterruptedException {
  setUp(9933);
  try {
    Configuration conf=new HdfsConfiguration(cluster.getConfiguration(0));
    StringBuilder namenodesBuilder=new StringBuilder();
    String bpidToShutdown=cluster.getNamesystem(2).getBlockPoolId();
    for (int i=0; i < 2; i++) {
      String nsId=DFSUtil.getNameServiceId(cluster.getConfiguration(i));
      namenodesBuilder.append(nsId);
      namenodesBuilder.append(",");
    }
    conf.set(DFSConfigKeys.DFS_FEDERATION_NAMESERVICES,namenodesBuilder.toString());
    DataNode dn=cluster.getDataNodes().get(0);
    dn.refreshNamenodes(conf);
    try {
      while (true) {
        dn.blockScanner.getBlocksScannedInLastRun(bpidToShutdown);
        Thread.sleep(1000);
      }
    }
 catch (    IOException ex) {
      LOG.info(ex.getMessage());
    }
    namenodesBuilder.append(DFSUtil.getNameServiceId(cluster.getConfiguration(2)));
    conf.set(DFSConfigKeys.DFS_FEDERATION_NAMESERVICES,namenodesBuilder.toString());
    dn.refreshNamenodes(conf);
    for (int i=0; i < 3; i++) {
      long blocksScanned=0;
      while (blocksScanned != 20) {
        blocksScanned=dn.blockScanner.getBlocksScannedInLastRun(bpids[i]);
        LOG.info("Waiting for all blocks to be scanned for bpid=" + bpids[i] + "; Scanned so far="+ blocksScanned);
        Thread.sleep(5000);
      }
    }
  }
  finally {
    cluster.shutdown();
  }
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testBlockScannerAfterRestart() throws IOException, InterruptedException {
  setUp(9943);
  try {
    cluster.restartDataNode(0);
    cluster.waitActive();
    DataNode dn=cluster.getDataNodes().get(0);
    for (int i=0; i < 3; i++) {
      long blocksScanned=0;
      while (blocksScanned != 20) {
        if (dn.blockScanner != null) {
          blocksScanned=dn.blockScanner.getBlocksScannedInLastRun(bpids[i]);
          LOG.info("Waiting for all blocks to be scanned for bpid=" + bpids[i] + "; Scanned so far="+ blocksScanned);
        }
        Thread.sleep(5000);
      }
    }
  }
  finally {
    cluster.shutdown();
  }
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testDataBlockScanner() throws IOException, InterruptedException {
  setUp(9923);
  try {
    DataNode dn=cluster.getDataNodes().get(0);
    for (int i=0; i < 3; i++) {
      long blocksScanned=0;
      while (blocksScanned != 20) {
        blocksScanned=dn.blockScanner.getBlocksScannedInLastRun(bpids[i]);
        LOG.info("Waiting for all blocks to be scanned for bpid=" + bpids[i] + "; Scanned so far="+ blocksScanned);
        Thread.sleep(5000);
      }
    }
    StringBuilder buffer=new StringBuilder();
    dn.blockScanner.printBlockReport(buffer,false);
    LOG.info("Block Report\n" + buffer.toString());
  }
  finally {
    cluster.shutdown();
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.datanode.TestWriteToReplica </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testWriteToRbw() throws Exception {
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(new HdfsConfiguration()).build();
  try {
    cluster.waitActive();
    DataNode dn=cluster.getDataNodes().get(0);
    FSDataset dataSet=(FSDataset)dn.data;
    String bpid=cluster.getNamesystem().getBlockPoolId();
    ExtendedBlock[] blocks=setup(bpid,dataSet);
    testWriteToRbw(dataSet,blocks);
  }
  finally {
    cluster.shutdown();
  }
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testWriteToTempoary() throws Exception {
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(new HdfsConfiguration()).build();
  try {
    cluster.waitActive();
    DataNode dn=cluster.getDataNodes().get(0);
    FSDataset dataSet=(FSDataset)dn.data;
    String bpid=cluster.getNamesystem().getBlockPoolId();
    ExtendedBlock[] blocks=setup(bpid,dataSet);
    testWriteToTemporary(dataSet,blocks);
  }
  finally {
    cluster.shutdown();
  }
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testClose() throws Exception {
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(new HdfsConfiguration()).build();
  try {
    cluster.waitActive();
    DataNode dn=cluster.getDataNodes().get(0);
    FSDataset dataSet=(FSDataset)dn.data;
    String bpid=cluster.getNamesystem().getBlockPoolId();
    ExtendedBlock[] blocks=setup(bpid,dataSet);
    testClose(dataSet,blocks);
  }
  finally {
    cluster.shutdown();
  }
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testAppend() throws Exception {
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(new HdfsConfiguration()).build();
  try {
    cluster.waitActive();
    DataNode dn=cluster.getDataNodes().get(0);
    FSDataset dataSet=(FSDataset)dn.data;
    String bpid=cluster.getNamesystem().getBlockPoolId();
    ExtendedBlock[] blocks=setup(bpid,dataSet);
    testAppend(bpid,dataSet,blocks);
  }
  finally {
    cluster.shutdown();
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.namenode.TestBlockUnderConstruction </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testBlockCreation() throws IOException {
  Path file1=new Path(BASE_DIR,"file1.dat");
  FSDataOutputStream out=TestFileCreation.createFile(hdfs,file1,3);
  for (int idx=0; idx < NUM_BLOCKS; idx++) {
    writeFile(file1,out,BLOCK_SIZE);
    verifyFileBlocks(file1.toString(),true);
  }
  out.close();
  verifyFileBlocks(file1.toString(),false);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.namenode.TestBlocksWithNotEnoughRacks </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testUnderReplicatedUsesNewRacks() throws Exception {
  Configuration conf=getConf();
  short REPLICATION_FACTOR=3;
  final Path filePath=new Path("/testFile");
  String racks[]={"/rack1","/rack1","/rack1","/rack1","/rack1"};
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(racks.length).racks(racks).build();
  final FSNamesystem ns=cluster.getNameNode().getNamesystem();
  try {
    final FileSystem fs=cluster.getFileSystem();
    DFSTestUtil.createFile(fs,filePath,1L,REPLICATION_FACTOR,1L);
    ExtendedBlock b=DFSTestUtil.getFirstBlock(fs,filePath);
    DFSTestUtil.waitForReplication(cluster,b,1,REPLICATION_FACTOR,1);
    String newRacks[]={"/rack2","/rack2"};
    cluster.startDataNodes(conf,2,true,null,newRacks);
    REPLICATION_FACTOR=5;
    ns.setReplication("/testFile",REPLICATION_FACTOR);
    DFSTestUtil.waitForReplication(cluster,b,2,REPLICATION_FACTOR,0);
  }
  finally {
    cluster.shutdown();
  }
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testReplDueToNodeFailRespectsRackPolicy() throws Exception {
  Configuration conf=getConf();
  short REPLICATION_FACTOR=3;
  final Path filePath=new Path("/testFile");
  String racks[]={"/rack1","/rack1","/rack1","/rack2","/rack2"};
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(racks.length).racks(racks).build();
  final FSNamesystem ns=cluster.getNameNode().getNamesystem();
  try {
    final FileSystem fs=cluster.getFileSystem();
    DFSTestUtil.createFile(fs,filePath,1L,REPLICATION_FACTOR,1L);
    ExtendedBlock b=DFSTestUtil.getFirstBlock(fs,filePath);
    DFSTestUtil.waitForReplication(cluster,b,2,REPLICATION_FACTOR,0);
    ArrayList<DataNode> datanodes=cluster.getDataNodes();
    int idx=datanodes.size() - 1;
    DataNode dataNode=datanodes.get(idx);
    DatanodeID dnId=dataNode.getDatanodeId();
    cluster.stopDataNode(idx);
    ns.removeDatanode(dnId);
    DFSTestUtil.waitForReplication(cluster,b,2,REPLICATION_FACTOR,0);
    datanodes=cluster.getDataNodes();
    idx=datanodes.size() - 1;
    dataNode=datanodes.get(idx);
    dnId=dataNode.getDatanodeId();
    cluster.stopDataNode(idx);
    ns.removeDatanode(dnId);
    DFSTestUtil.waitForReplication(cluster,b,1,REPLICATION_FACTOR,1);
  }
  finally {
    cluster.shutdown();
  }
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testSufficientlySingleReplBlockUsesNewRack() throws Exception {
  Configuration conf=getConf();
  short REPLICATION_FACTOR=1;
  final Path filePath=new Path("/testFile");
  String racks[]={"/rack1","/rack1","/rack1","/rack2"};
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(racks.length).racks(racks).build();
  final FSNamesystem ns=cluster.getNameNode().getNamesystem();
  try {
    final FileSystem fs=cluster.getFileSystem();
    DFSTestUtil.createFile(fs,filePath,1L,REPLICATION_FACTOR,1L);
    ExtendedBlock b=DFSTestUtil.getFirstBlock(fs,filePath);
    DFSTestUtil.waitForReplication(cluster,b,1,REPLICATION_FACTOR,0);
    REPLICATION_FACTOR=2;
    ns.setReplication("/testFile",REPLICATION_FACTOR);
    DFSTestUtil.waitForReplication(cluster,b,2,REPLICATION_FACTOR,0);
  }
  finally {
    cluster.shutdown();
  }
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testReduceReplFactorRespectsRackPolicy() throws Exception {
  Configuration conf=getConf();
  short REPLICATION_FACTOR=3;
  final Path filePath=new Path("/testFile");
  String racks[]={"/rack1","/rack1","/rack2","/rack2"};
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(racks.length).racks(racks).build();
  final FSNamesystem ns=cluster.getNameNode().getNamesystem();
  try {
    final FileSystem fs=cluster.getFileSystem();
    DFSTestUtil.createFile(fs,filePath,1L,REPLICATION_FACTOR,1L);
    ExtendedBlock b=DFSTestUtil.getFirstBlock(fs,filePath);
    DFSTestUtil.waitForReplication(cluster,b,2,REPLICATION_FACTOR,0);
    REPLICATION_FACTOR=2;
    ns.setReplication("/testFile",REPLICATION_FACTOR);
    DFSTestUtil.waitForReplication(cluster,b,2,REPLICATION_FACTOR,0);
  }
  finally {
    cluster.shutdown();
  }
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testSufficientlyReplBlocksUsesNewRack() throws Exception {
  Configuration conf=getConf();
  final short REPLICATION_FACTOR=3;
  final Path filePath=new Path("/testFile");
  String racks[]={"/rack1","/rack1","/rack1"};
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(racks.length).racks(racks).build();
  try {
    final FileSystem fs=cluster.getFileSystem();
    DFSTestUtil.createFile(fs,filePath,1L,REPLICATION_FACTOR,1L);
    ExtendedBlock b=DFSTestUtil.getFirstBlock(fs,filePath);
    DFSTestUtil.waitForReplication(cluster,b,1,REPLICATION_FACTOR,1);
    String newRacks[]={"/rack2"};
    cluster.startDataNodes(conf,1,true,null,newRacks);
    cluster.waitActive();
    DFSTestUtil.waitForReplication(cluster,b,2,REPLICATION_FACTOR,0);
  }
  finally {
    cluster.shutdown();
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.namenode.TestEditLogRace </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Tests saving fs image while transactions are ongoing.
 */
@Test public void testSaveNamespace() throws Exception {
  Configuration conf=new HdfsConfiguration();
  MiniDFSCluster cluster=null;
  FileSystem fileSys=null;
  AtomicReference<Throwable> caughtErr=new AtomicReference<Throwable>();
  try {
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DATA_NODES).build();
    cluster.waitActive();
    fileSys=cluster.getFileSystem();
    final FSNamesystem namesystem=cluster.getNamesystem();
    FSImage fsimage=namesystem.getFSImage();
    FSEditLog editLog=fsimage.getEditLog();
    editLog.setBufferCapacity(2048);
    editLog.close();
    editLog.open();
    startTransactionWorkers(namesystem,caughtErr);
    for (int i=0; i < NUM_SAVE_IMAGE && caughtErr.get() == null; i++) {
      try {
        Thread.sleep(20);
      }
 catch (      InterruptedException e) {
      }
      LOG.info("Save " + i + ": entering safe mode");
      namesystem.enterSafeMode(false);
      verifyEditLogs(namesystem,fsimage);
      LOG.info("Save " + i + ": saving namespace");
      namesystem.saveNamespace();
      LOG.info("Save " + i + ": leaving safemode");
      verifyEditLogs(namesystem,fsimage);
      namesystem.leaveSafeMode(false);
      LOG.info("Save " + i + ": complete");
    }
  }
  finally {
    stopTransactionWorkers();
    if (caughtErr.get() != null) {
      throw new RuntimeException(caughtErr.get());
    }
    if (fileSys != null)     fileSys.close();
    if (cluster != null)     cluster.shutdown();
  }
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Tests rolling edit logs while transactions are ongoing.
 */
@Test public void testEditLogRolling() throws Exception {
  Configuration conf=new HdfsConfiguration();
  MiniDFSCluster cluster=null;
  FileSystem fileSys=null;
  AtomicReference<Throwable> caughtErr=new AtomicReference<Throwable>();
  try {
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DATA_NODES).build();
    cluster.waitActive();
    fileSys=cluster.getFileSystem();
    final FSNamesystem namesystem=cluster.getNamesystem();
    FSImage fsimage=namesystem.getFSImage();
    FSEditLog editLog=fsimage.getEditLog();
    editLog.setBufferCapacity(2048);
    editLog.close();
    editLog.open();
    startTransactionWorkers(namesystem,caughtErr);
    for (int i=0; i < NUM_ROLLS && caughtErr.get() == null; i++) {
      try {
        Thread.sleep(20);
      }
 catch (      InterruptedException e) {
      }
      LOG.info("Starting roll " + i + ".");
      editLog.rollEditLog();
      LOG.info("Roll complete " + i + ".");
      verifyEditLogs(namesystem,fsimage);
      LOG.info("Starting purge " + i + ".");
      editLog.purgeEditLog();
      LOG.info("Complete purge " + i + ".");
    }
  }
  finally {
    stopTransactionWorkers();
    if (caughtErr.get() != null) {
      throw new RuntimeException(caughtErr.get());
    }
    if (fileSys != null)     fileSys.close();
    if (cluster != null)     cluster.shutdown();
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.namenode.TestFSEditLogLoader </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test that, if the NN restarts with a new minimum replication,
 * any files created with the old replication count will get
 * automatically bumped up to the new minimum upon restart.
 */
@Test public void testReplicationAdjusted() throws IOException {
  Configuration conf=new HdfsConfiguration();
  conf.setInt(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY,1);
  conf.setInt(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1);
  MiniDFSCluster cluster=null;
  try {
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(2).build();
    cluster.waitActive();
    FileSystem fs=cluster.getFileSystem();
    Path p=new Path("/testfile");
    DFSTestUtil.createFile(fs,p,10,(short)1,1);
    DFSTestUtil.waitReplication(fs,p,(short)1);
    cluster.shutdown();
    cluster=null;
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_REPLICATION_MIN_KEY,2);
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(2).format(false).build();
    cluster.waitActive();
    fs=cluster.getFileSystem();
    DFSTestUtil.waitReplication(fs,p,(short)2);
  }
  finally {
    if (cluster != null) {
      cluster.shutdown();
    }
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.namenode.TestFsLimits </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testDuringEditLogs() throws Exception {
  conf.setInt(DFSConfigKeys.DFS_NAMENODE_MAX_COMPONENT_LENGTH_KEY,3);
  conf.setInt(DFSConfigKeys.DFS_NAMENODE_MAX_DIRECTORY_ITEMS_KEY,2);
  fsIsReady=false;
  addChildWithName("1",null);
  addChildWithName("22",null);
  addChildWithName("333",null);
  addChildWithName("4444",null);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testNoLimits() throws Exception {
  addChildWithName("1",null);
  addChildWithName("22",null);
  addChildWithName("333",null);
  addChildWithName("4444",null);
  addChildWithName("55555",null);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testMaxDirItems() throws Exception {
  conf.setInt(DFSConfigKeys.DFS_NAMENODE_MAX_DIRECTORY_ITEMS_KEY,2);
  addChildWithName("1",null);
  addChildWithName("22",null);
  addChildWithName("333",MaxDirectoryItemsExceededException.class);
  addChildWithName("4444",MaxDirectoryItemsExceededException.class);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testMaxComponentLength() throws Exception {
  conf.setInt(DFSConfigKeys.DFS_NAMENODE_MAX_COMPONENT_LENGTH_KEY,2);
  addChildWithName("1",null);
  addChildWithName("22",null);
  addChildWithName("333",PathComponentTooLongException.class);
  addChildWithName("4444",PathComponentTooLongException.class);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testMaxComponentsAndMaxDirItems() throws Exception {
  conf.setInt(DFSConfigKeys.DFS_NAMENODE_MAX_COMPONENT_LENGTH_KEY,3);
  conf.setInt(DFSConfigKeys.DFS_NAMENODE_MAX_DIRECTORY_ITEMS_KEY,2);
  addChildWithName("1",null);
  addChildWithName("22",null);
  addChildWithName("333",MaxDirectoryItemsExceededException.class);
  addChildWithName("4444",PathComponentTooLongException.class);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.namenode.TestGetImageServlet </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void IsValidRequestorAcceptsCorrectly() throws IOException {
  GetImageServlet gim=new GetImageServlet();
  verifyIsValidReqBehavior(gim,true,"isValidRequestor has rejected a valid requestor: ");
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.namenode.TestINodeFile </h4><pre class="type-15 type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies that exceptions are thrown during the test case execution">ExceptionVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies that exceptions are thrown during the test case execution
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * IllegalArgumentException is expected for setting below lower bound
 * for PreferredBlockSize.
 * @throws IllegalArgumentException as the result
 */
@Test(expected=IllegalArgumentException.class) public void testPreferredBlockSizeBelowLowerBound() throws IllegalArgumentException {
  replication=3;
  preferredBlockSize=-1;
  INodeFile inf=new INodeFile(new PermissionStatus(userName,null,FsPermission.getDefault()),null,replication,0L,0L,preferredBlockSize);
}

</code></pre>

<br>
<pre class="type-15 type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies that exceptions are thrown during the test case execution">ExceptionVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies that exceptions are thrown during the test case execution
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * IllegalArgumentException is expected for setting above upper bound
 * for PreferredBlockSize.
 * @throws IllegalArgumentException as the result
 */
@Test(expected=IllegalArgumentException.class) public void testPreferredBlockSizeAboveUpperBound() throws IllegalArgumentException {
  replication=3;
  preferredBlockSize=BLKSIZE_MAXVALUE + 1;
  INodeFile inf=new INodeFile(new PermissionStatus(userName,null,FsPermission.getDefault()),null,replication,0L,0L,preferredBlockSize);
}

</code></pre>

<br>
<pre class="type-15 type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Verifies that exceptions are thrown during the test case execution">ExceptionVerifier</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Verifies that exceptions are thrown during the test case execution
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * IllegalArgumentException is expected for setting below lower bound
 * for Replication.
 * @throws IllegalArgumentException as the result
 */
@Test(expected=IllegalArgumentException.class) public void testReplicationBelowLowerBound() throws IllegalArgumentException {
  replication=-1;
  preferredBlockSize=128 * 1024 * 1024;
  INodeFile inf=new INodeFile(new PermissionStatus(userName,null,FsPermission.getDefault()),null,replication,0L,0L,preferredBlockSize);
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.namenode.TestNNThroughputBenchmark </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * This test runs all benchmarks defined in {@link NNThroughputBenchmark}.
 */
@Test public void testNNThroughput() throws Exception {
  Configuration conf=new HdfsConfiguration();
  FileSystem.setDefaultUri(conf,"hdfs://localhost:" + 0);
  conf.set(DFSConfigKeys.DFS_NAMENODE_HTTP_ADDRESS_KEY,"0.0.0.0:0");
  DFSTestUtil.formatNameNode(conf);
  String[] args=new String[]{"-op","all"};
  NNThroughputBenchmark.runBenchmark(conf,Arrays.asList(args));
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.namenode.TestPathComponents </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testBytes2ByteArray() throws Exception {
  testString("/");
  testString("/file");
  testString("/directory/");
  testString("//");
  testString("/dir//file");
  testString("/dir/dir1//");
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test case where savenamespace fails in all directories
 * and then the NN shuts down. Here we should recover from the
 * failed checkpoint by moving the directories back on next
 * NN start. This is a regression test for HDFS-1921.
 */
@Test public void testFailedSaveNamespace() throws Exception {
  doTestFailedSaveNamespace(false);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test case where saveNamespace fails in all directories, but then
 * the operator restores the directories and calls it again.
 * This should leave the NN in a clean state for next start.
 */
@Test public void testFailedSaveNamespaceWithRecovery() throws Exception {
  doTestFailedSaveNamespace(true);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testCrashWhileMoveLastCheckpoint() throws Exception {
  saveNamespaceWithInjectedFault(Fault.MOVE_LAST_CHECKPOINT);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testCrashWhileMoveCurrent() throws Exception {
  saveNamespaceWithInjectedFault(Fault.MOVE_CURRENT);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testCrashWhileSavingSecondImage() throws Exception {
  saveNamespaceWithInjectedFault(Fault.SAVE_FSIMAGE);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testSaveWhileEditsRolled() throws Exception {
  Configuration conf=getConf();
  NameNode.initMetrics(conf,NamenodeRole.ACTIVE);
  DFSTestUtil.formatNameNode(conf);
  FSNamesystem fsn=new FSNamesystem(conf);
  final FSImage originalImage=fsn.dir.fsImage;
  originalImage.getStorage().close();
  FSImage spyImage=spy(originalImage);
  spyImage.getStorage().setStorageDirectories(FSNamesystem.getNamespaceDirs(conf),FSNamesystem.getNamespaceEditsDirs(conf));
  fsn.dir.fsImage=spyImage;
  try {
    doAnEdit(fsn,1);
    CheckpointSignature sig=fsn.rollEditLog();
    LOG.warn("Checkpoint signature: " + sig);
    doAnEdit(fsn,2);
    fsn.setSafeMode(SafeModeAction.SAFEMODE_ENTER);
    fsn.saveNamespace();
    originalImage.close();
    fsn.close();
    fsn=null;
    fsn=new FSNamesystem(conf);
    checkEditExists(fsn,1);
    checkEditExists(fsn,2);
  }
  finally {
    if (fsn != null) {
      fsn.close();
    }
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.tools.TestGetConf </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Tests to make sure the returned addresses are correct in case of federation
 * of setup.
 */
@Test public void testFederation() throws Exception {
  final int nsCount=10;
  HdfsConfiguration conf=new HdfsConfiguration(false);
  setupNameServices(conf,nsCount);
  String[] nnAddresses=setupAddress(conf,DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY,nsCount,1000);
  setupAddress(conf,DFS_NAMENODE_RPC_ADDRESS_KEY,nsCount,1500);
  String[] backupAddresses=setupAddress(conf,DFS_NAMENODE_BACKUP_ADDRESS_KEY,nsCount,2000);
  String[] secondaryAddresses=setupAddress(conf,DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY,nsCount,3000);
  verifyAddresses(conf,TestType.NAMENODE,false,nnAddresses);
  verifyAddresses(conf,TestType.BACKUP,false,backupAddresses);
  verifyAddresses(conf,TestType.SECONDARY,false,secondaryAddresses);
  verifyAddresses(conf,TestType.NNRPCADDRESSES,true,nnAddresses);
  conf=new HdfsConfiguration(false);
  setupNameServices(conf,nsCount);
  nnAddresses=setupAddress(conf,DFS_NAMENODE_RPC_ADDRESS_KEY,nsCount,1000);
  backupAddresses=setupAddress(conf,DFS_NAMENODE_BACKUP_ADDRESS_KEY,nsCount,2000);
  secondaryAddresses=setupAddress(conf,DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY,nsCount,3000);
  verifyAddresses(conf,TestType.NAMENODE,false,nnAddresses);
  verifyAddresses(conf,TestType.BACKUP,false,backupAddresses);
  verifyAddresses(conf,TestType.SECONDARY,false,secondaryAddresses);
  verifyAddresses(conf,TestType.NNRPCADDRESSES,true,nnAddresses);
}

</code></pre>

<br>
<pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Tests to make sure the returned addresses are correct in case of default
 * configuration with no federation
 */
@Test public void testNonFederation() throws Exception {
  HdfsConfiguration conf=new HdfsConfiguration(false);
  conf.set(FS_DEFAULT_NAME_KEY,"hdfs://localhost:1000");
  verifyAddresses(conf,TestType.NAMENODE,false,"localhost:1000");
  verifyAddresses(conf,TestType.NNRPCADDRESSES,true,"localhost:1000");
  conf.set(DFS_NAMENODE_BACKUP_ADDRESS_KEY,"localhost:1001");
  verifyAddresses(conf,TestType.BACKUP,false,"localhost:1001");
  conf.set(DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY,"localhost:1002");
  verifyAddresses(conf,TestType.SECONDARY,false,"localhost:1002");
  conf=new HdfsConfiguration();
  conf.set(DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY,"localhost:1000");
  conf.set(DFS_NAMENODE_RPC_ADDRESS_KEY,"localhost:1001");
  verifyAddresses(conf,TestType.NAMENODE,false,"localhost:1000");
  verifyAddresses(conf,TestType.NNRPCADDRESSES,true,"localhost:1000");
  conf=new HdfsConfiguration();
  conf.set(DFS_NAMENODE_RPC_ADDRESS_KEY,"localhost:1001");
  verifyAddresses(conf,TestType.NAMENODE,false,"localhost:1001");
  verifyAddresses(conf,TestType.NNRPCADDRESSES,true,"localhost:1001");
}

</code></pre>

<br>
<pre class="type-14 type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Invokes logging operations">Logger</span>&nbsp;<span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Invokes logging operations
- Executes methods or other tests from the same test unit
"></span><br>
/** 
 * Test empty configuration
 */
@Test public void testEmptyConf() throws Exception {
  HdfsConfiguration conf=new HdfsConfiguration(false);
  getAddressListFromTool(TestType.NAMENODE,conf,false);
  System.out.println(getAddressListFromTool(TestType.BACKUP,conf,false));
  getAddressListFromTool(TestType.SECONDARY,conf,false);
  getAddressListFromTool(TestType.NNRPCADDRESSES,conf,false);
  for (  Command cmd : Command.values()) {
    CommandHandler handler=Command.getHandler(cmd.getName());
    if (handler.key != null) {
      String[] args={handler.key};
      runTool(conf,args,false);
    }
  }
}

</code></pre>

<br>
<h4 style="margin:0px">Class: org.apache.hadoop.hdfs.util.TestGSet </h4><pre class="type-1 "><code><span class="label label-info" style="display: inline-block;" data-toggle="tooltip" title="Executes methods or other tests from the same test unit">ExecutionTester</span>&nbsp;<span class=" glyphicon glyphicon-comment" aria-hidden="true" label-info" data-toggle="tooltip" title="This method/test case: 
- Executes methods or other tests from the same test unit
"></span><br>
@Test public void testGSet(){
  check(new GSetTestCase(1,1 << 4,65537));
  check(new GSetTestCase(17,1 << 16,17));
  check(new GSetTestCase(255,1 << 10,65537));
}

</code></pre>

<br>
<script>$(document).ready(function() {
  $('pre code').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});</script>
